# Final PDF Collection


## Content


### **Objective**


To validate and consolidate the collected PDFs by removing duplicates, ensuring the dataset is reliable for downstream analysis.







---


### **Process**

- Developed a script to calculate the **checksum** of each PDF.
- Identified duplicate files, some occurring two or three times.
- Removed duplicates to prevent over-representation that could distort semantic search outputs.
- Results:
    - Initial total: **8,528** PDFs
    - Located: **7,773** PDFs (~755 missing)
    - Duplicates removed: **932**
    - Final unique set: **6,841** PDFs

    



---


### **Reflection**


Deduplication proved essential: 452 files were present in multiple copies, which could have introduced noise and biased analyses. By validating against checksums rather than filenames, the process ensured accuracy and reproducibility. Although ~9% of publications remain inaccessible, the final dataset is sufficiently large and reliable for meaningful downstream analysis.







---


### **Next Step**


Proceed with metadata verification and preparation of the cleaned dataset for semantic search and feature extraction.


