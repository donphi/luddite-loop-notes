# Topic Reclassification Report


## Content




~~~{=html}
<pre class="notion-ascii-diagram"><code># Topic Reclassification Report

This report explains the output logs from the BERTopic model during the topic extraction and section classification process. 
The goal is to show how clusters of text (`topics`) were reclassified into journal paper sections (e.g., abstract, methods, references).

---

## Example Log Extracts

Here are **three raw examples** from the system logs, exactly as they appeared during processing:

```
INFO:__main__:  Topic 692 -&gt; references
INFO:__main__:    Score: 0.638 (emb: 0.911, kw: 0.000)
INFO:__main__:    Words: info, article, , , 
```

```
INFO:__main__:  Topic 695 -&gt; tables_and_figures
INFO:__main__:    Score: 0.741 (emb: 0.898, kw: 0.375)
INFO:__main__:    Words: lv, rv, chamber, indexes, es
```

```
INFO:__main__:  Topic 697 -&gt; acknowledgments
INFO:__main__:    Score: 0.700 (emb: 0.928, kw: 0.167)
INFO:__main__:    Words: acknowledgements, funding, advancements, acknowledgement, acknowledged
```

These extracts represent how each topic (a cluster of related text sections) is scored and then mapped to a likely canonical section.

---

## How to Interpret the Output

- **Topic ID** – the cluster number assigned by BERTopic.
- **Section** – the reclassified journal section name (e.g., references, discussion, abstract).
- **Score** – combined metric from embeddings and keyword matching.
  - **emb**: similarity to canonical embeddings.
  - **kw**: keyword overlap with predefined section keywords.
- **Top Words** – the most representative words for that topic cluster.

The **progress bars** (e.g., `Batches: 100%|█████| 1/1 [...]`) reflect the process of extracting these top words. They run quickly because each topic-word extraction is short and CPU-based.

---

## Reclassified Topics (Sample)

| Topic ID | Section                  | Score | Embedding Sim | Keyword Score | Top Words                                      |
|----------|--------------------------|-------|---------------|---------------|------------------------------------------------|
| 692      | references               | 0.638 | 0.911         | 0.000         | info, article, , ,                             |
| 693      | discussion               | 0.688 | 0.921         | 0.143         | limitations, limitation, study, ,              |
| 694      | abstract                 | 0.669 | 0.902         | 0.125         | author, summary, , ,                           |
| 695      | tables_and_figures       | 0.741 | 0.898         | 0.375         | lv, rv, chamber, indexes, es                   |
| 696      | data_and_code_availability | 0.679 | 0.909         | 0.143         | biobank, uk, data, mellitus, diabetes          |
| 697      | acknowledgments          | 0.700 | 0.928         | 0.167         | acknowledgements, funding, advancements, acknowledgement, acknowledged |
| 698      | funding                  | 0.628 | 0.897         | 0.000         | gla, publications, glasgow, members, ac        |
| 699      | methods                  | 0.735 | 0.942         | 0.250         | legend, description, biologists, repeatedly, monte |
| 700      | abstract                 | 0.671 | 0.904         | 0.125         | setup, experimental, experiment, experiments, setups |
| 701      | supplementary_materials  | 0.639 | 0.913         | 0.000         | software, code, , ,                            |
| 702      | tables_and_figures       | 0.708 | 0.905         | 0.250         | mediation, modelling, annu, vanderweele, fow   |
| 703      | results                  | 0.648 | 0.878         | 0.111         | ahead, print, publish, revised, editorial      |
| 704      | supplementary_materials  | 0.771 | 0.917         | 0.429         | jaha, 1161, suppl, ahajournals, athttps        |
| 705      | ethics_approval          | 0.803 | 0.902         | 0.571         | ethics, approval, consent, committee, standards |

---

## Summary

- **GPU handled the clustering (UMAP + HDBSCAN).**
- **CPU handled the topic representation (vectorizer + c-TF-IDF).**
- The progress bars you see are from **topic word extraction**, not embeddings.
- Scores show how confident the model is when mapping clusters back to canonical sections.

This output confirms the pipeline is functioning: clusters are being created, scored, and labeled with interpretable sections and top words.</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
# Topic Reclassification Report

This report explains the output logs from the BERTopic model during the topic extraction and section classification process. 
The goal is to show how clusters of text (`topics`) were reclassified into journal paper sections (e.g., abstract, methods, references).

---

## Example Log Extracts

Here are **three raw examples** from the system logs, exactly as they appeared during processing:

```
INFO:__main__:  Topic 692 -> references
INFO:__main__:    Score: 0.638 (emb: 0.911, kw: 0.000)
INFO:__main__:    Words: info, article, , , 
```

```
INFO:__main__:  Topic 695 -> tables_and_figures
INFO:__main__:    Score: 0.741 (emb: 0.898, kw: 0.375)
INFO:__main__:    Words: lv, rv, chamber, indexes, es
```

```
INFO:__main__:  Topic 697 -> acknowledgments
INFO:__main__:    Score: 0.700 (emb: 0.928, kw: 0.167)
INFO:__main__:    Words: acknowledgements, funding, advancements, acknowledgement, acknowledged
```

These extracts represent how each topic (a cluster of related text sections) is scored and then mapped to a likely canonical section.

---

## How to Interpret the Output

- **Topic ID** – the cluster number assigned by BERTopic.
- **Section** – the reclassified journal section name (e.g., references, discussion, abstract).
- **Score** – combined metric from embeddings and keyword matching.
  - **emb**: similarity to canonical embeddings.
  - **kw**: keyword overlap with predefined section keywords.
- **Top Words** – the most representative words for that topic cluster.

The **progress bars** (e.g., `Batches: 100%|█████| 1/1 [...]`) reflect the process of extracting these top words. They run quickly because each topic-word extraction is short and CPU-based.

---

## Reclassified Topics (Sample)

| Topic ID | Section                  | Score | Embedding Sim | Keyword Score | Top Words                                      |
|----------|--------------------------|-------|---------------|---------------|------------------------------------------------|
| 692      | references               | 0.638 | 0.911         | 0.000         | info, article, , ,                             |
| 693      | discussion               | 0.688 | 0.921         | 0.143         | limitations, limitation, study, ,              |
| 694      | abstract                 | 0.669 | 0.902         | 0.125         | author, summary, , ,                           |
| 695      | tables_and_figures       | 0.741 | 0.898         | 0.375         | lv, rv, chamber, indexes, es                   |
| 696      | data_and_code_availability | 0.679 | 0.909         | 0.143         | biobank, uk, data, mellitus, diabetes          |
| 697      | acknowledgments          | 0.700 | 0.928         | 0.167         | acknowledgements, funding, advancements, acknowledgement, acknowledged |
| 698      | funding                  | 0.628 | 0.897         | 0.000         | gla, publications, glasgow, members, ac        |
| 699      | methods                  | 0.735 | 0.942         | 0.250         | legend, description, biologists, repeatedly, monte |
| 700      | abstract                 | 0.671 | 0.904         | 0.125         | setup, experimental, experiment, experiments, setups |
| 701      | supplementary_materials  | 0.639 | 0.913         | 0.000         | software, code, , ,                            |
| 702      | tables_and_figures       | 0.708 | 0.905         | 0.250         | mediation, modelling, annu, vanderweele, fow   |
| 703      | results                  | 0.648 | 0.878         | 0.111         | ahead, print, publish, revised, editorial      |
| 704      | supplementary_materials  | 0.771 | 0.917         | 0.429         | jaha, 1161, suppl, ahajournals, athttps        |
| 705      | ethics_approval          | 0.803 | 0.902         | 0.571         | ethics, approval, consent, committee, standards |

---

## Summary

- **GPU handled the clustering (UMAP + HDBSCAN).**
- **CPU handled the topic representation (vectorizer + c-TF-IDF).**
- The progress bars you see are from **topic word extraction**, not embeddings.
- Scores show how confident the model is when mapping clusters back to canonical sections.

This output confirms the pipeline is functioning: clusters are being created, scored, and labeled with interpretable sections and top words.
\end{Verbatim}
~~~






~~~{=html}
<pre class="notion-ascii-diagram"><code>
# RAG + Topic Reclassification: How Vectors, Retrieval, and BERTopic Scores Fit Together

This report connects the dots between your **vector database**, **vector space embeddings**, **retrieval**, and the **BERTopic scores** you’re seeing in the logs. It also clarifies **which progress bars** correspond to which stage so you can read runtime output with confidence.

---

## 1) End-to-End Overview (Plain English)

**Goal:** take many journal chunks and (a) make them searchable with vectors, and (b) reclassify “other/unknown” chunks into real sections (abstract, methods, results, etc.).

**Pipeline (two intertwined tracks):**

```
          ┌───────────┐
          │  Chunks   │  (text bodies + metadata)
          └─────┬─────┘
                │
                │ 1. Embedding  f(text) → R^d   (GPU)
                ▼
        ┌───────────────┐
        │ Vector Space  │  (one vector per chunk)
        └─────┬─────────┘
              │
              │ 2. Index in Vector DB (e.g., Milvus) for ANN search
              ▼
        ┌───────────────┐           ┌──────────────────────────┐
        │  Retrieval    │◄── query ─┤ embed(query) → vector q  │
        └───────────────┘           └──────────────────────────┘
              │  returns top-k by cosine/dot-product similarity
              ▼
      (RAG consumer: downstream ranking/reading/QA, etc.)


In parallel for section reclassification:

Chunks ──embeddings (GPU)──► UMAP (GPU) ─► HDBSCAN (GPU) ─► BERTopic topic words (CPU c‑TF‑IDF)
                                            │
                                            ▼
                                     Section mapping (your Score = 0.7*emb + 0.3*kw)
```

- **Vector DB track:** makes your chunks searchable with **similarity scores** (cosine or dot-product).
- **BERTopic track:** groups similar chunks into topics and **names** those groups with top words; you then **map topics → canonical sections** using your combined score.

**Important:** the “Score” values in your logs are **classification scores** (for section mapping), **not** retrieval scores from the vector DB.

---

## 2) Vector Database &amp; “Vector Space” (Technical)

- Each chunk \(t_i\) is embedded via \( \mathbf{x}_i = f(t_i) \in \mathbb{R}^d \).
- Store \((\text{id}, \mathbf{x}_i, \text{metadata})\) in a **vector database** (e.g., Milvus).
- Build an **ANN (Approximate Nearest Neighbor)** index:
  - Common choices: **HNSW**, **IVF-FLAT/IVF-PQ**, **DiskANN** (vendor-dependent).
  - Trade‑off: recall vs speed vs memory.
- Metadata (e.g., `paper_id`, `section_type`, `doi`) is kept for **filters** and downstream joins (often in the same vector DB or in a side store like DuckDB).

**Schema sketch (conceptual):**
```text
chunk_id: str PRIMARY KEY
embedding: float[d]  # dense vector
text: str
section_type: str
paper_id: str
source: str
... (other attributes for filters)
```

---

## 3) Embeddings &amp; Similarity (Math)

Let \( f(\cdot) \) be the encoder (e.g., SentenceTransformer). For any text:

- **Embedding:** \( \mathbf{x} = f(\text{text}) \in \mathbb{R}^d \).
- Often we **L2-normalize** embeddings so cosine and dot-product are aligned.

**Cosine similarity:**
\[
\operatorname{cos\_sim}(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \; \|\mathbf{y}\|}
\]

**Retrieval:** Given a query \(q\) with embedding \(\mathbf{q}\), return top‑\(k\) chunks by similarity \(s_i = \operatorname{cos\_sim}(\mathbf{q}, \mathbf{x}_i)\) (or by dot‑product if that’s how your embeddings are trained).

**Mini numeric example (illustrative):**
- \(\mathbf{q}=(1,0,0)\), \(\mathbf{x}_1=(0.9,0.1,0)\), \(\mathbf{x}_2=(0.3,0.95,0)\)
- \(\cos(\mathbf{q},\mathbf{x}_1)\approx 0.99\), \(\cos(\mathbf{q},\mathbf{x}_2)\approx 0.30\)
- Top‑1 = \(x_1\). The **retrieval score** is the similarity value returned by the vector DB.

---

## 4) BERTopic: Topics, Words, and Your Section “Score”

**Clustering path (you enabled GPU here):**
1. **UMAP** reduces dimensionality of embeddings (still preserves local structure).
2. **HDBSCAN** finds dense clusters; points with no clear cluster → noise.

**Topic representation (CPU):**
3. **Vectorizer (CountVectorizer)** builds a vocabulary for each topic’s documents.
4. **c‑TF‑IDF** weights words to identify the **most representative words** per topic.

**Your section mapping:** you compute a **combined score** for each topic to select a canonical section (abstract, methods, …):

\[
\text{Score} = 0.7 \times \text{EmbeddingSimilarity} \;+\; 0.3 \times \text{KeywordScore}
\]

- **EmbeddingSimilarity** = cosine similarity between the topic’s pooled text embedding and a canonical section embedding.
- **KeywordScore** = fraction of section‑specific keywords found in the pooled text.

That’s the **Score** in your logs (e.g., `0.803 (emb: 0.902, kw: 0.571)`).

---

## 5) Progress Bars: What You’re Seeing

- **Embeddings:** `Batches: xx% | ... | n/N [.. it/s]` with many batches. *GPU-heavy* (your SentenceTransformer on CUDA).
- **Clustering:** brief logs from cuML/UMAP/HDBSCAN. *GPU-heavy but short*.
- **BERTopic “Fitting model” &amp; `Batches: 100%| 1/1 ...` lines:** fast single‑batch updates while **topic words** are computed with **CountVectorizer + c‑TF‑IDF**. *CPU‑heavy; GPU idle is normal here.*

---

## 6) Sample Log Extracts (Verbatim)

```
INFO:__main__:  Topic 692 -&gt; references
INFO:__main__:    Score: 0.638 (emb: 0.911, kw: 0.000)
INFO:__main__:    Words: info, article, , , 
```

```
INFO:__main__:  Topic 695 -&gt; tables_and_figures
INFO:__main__:    Score: 0.741 (emb: 0.898, kw: 0.375)
INFO:__main__:    Words: lv, rv, chamber, indexes, es
```

```
INFO:__main__:  Topic 697 -&gt; acknowledgments
INFO:__main__:    Score: 0.700 (emb: 0.928, kw: 0.167)
INFO:__main__:    Words: acknowledgements, funding, advancements, acknowledgement, acknowledged
```

---

## 7) Reclassified Topics (Your Provided Slice)

| Topic ID | Section                     | Score | Embedding Sim | Keyword Score | Top Words                                      |
|----------|-----------------------------|-------|---------------|---------------|------------------------------------------------|
| 692      | references                  | 0.638 | 0.911         | 0.000         | info, article, , ,                             |
| 693      | discussion                  | 0.688 | 0.921         | 0.143         | limitations, limitation, study, ,              |
| 694      | abstract                    | 0.669 | 0.902         | 0.125         | author, summary, , ,                           |
| 695      | tables_and_figures          | 0.741 | 0.898         | 0.375         | lv, rv, chamber, indexes, es                   |
| 696      | data_and_code_availability  | 0.679 | 0.909         | 0.143         | biobank, uk, data, mellitus, diabetes          |
| 697      | acknowledgments             | 0.700 | 0.928         | 0.167         | acknowledgements, funding, advancements, acknowledgement, acknowledged |
| 698      | funding                     | 0.628 | 0.897         | 0.000         | gla, publications, glasgow, members, ac        |
| 699      | methods                     | 0.735 | 0.942         | 0.250         | legend, description, biologists, repeatedly, monte |
| 700      | abstract                    | 0.671 | 0.904         | 0.125         | setup, experimental, experiment, experiments, setups |
| 701      | supplementary_materials     | 0.639 | 0.913         | 0.000         | software, code, , ,                            |
| 702      | tables_and_figures          | 0.708 | 0.905         | 0.250         | mediation, modelling, annu, vanderweele, fow   |
| 703      | results                     | 0.648 | 0.878         | 0.111         | ahead, print, publish, revised, editorial      |
| 704      | supplementary_materials     | 0.771 | 0.917         | 0.429         | jaha, 1161, suppl, ahajournals, athttps        |
| 705      | ethics_approval             | 0.803 | 0.902         | 0.571         | ethics, approval, consent, committee, standards |

---

## 8) How Retrieval Scores Relate to This

- **Retrieval score** (from the vector DB) = similarity between **query embedding** and **chunk embedding** (cosine or dot-product). Used to **rank** chunks for a question/need.
- **Your “Score” in logs** = **section-classification** confidence combining embedding similarity to a canonical section and keyword overlap. Used to **label** topics.

These are different scores for different purposes. It’s valid (and common) to use both: retrieval to **find** the right chunks, BERTopic+mapping to **understand** and **organize** them.

---

## 9) Practical Knobs

- **Retrieval quality:**
  - Ensure embeddings are **normalized** if using cosine similarity.
  - Pick an ANN index (e.g., **HNSW**) with enough `ef_construction`/`M`; at query time tune `ef` (HNSW) for recall.
  - Filter by metadata (e.g., section) to reduce noise before ranking.

- **Topic quality (BERTopic):**
  - Keep `ngram_range=(1,1)`, `min_df≥5`, `max_df≤0.9` for speed + cleaner vocab.
  - Improve tokenization to drop punctuation-only tokens: `token_pattern=r&#39;(?u)\b[A-Za-z][A-Za-z]+\b&#39;`.
  - Adjust your weight mix if needed: `Score = α*emb + (1-α)*kw` (you currently use α=0.7).

---

## 10) Pseudocode: Insert &amp; Search (Milvus-style)

```python
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType

# 1) Connect
connections.connect(&quot;default&quot;, host=&quot;milvus&quot;, port=&quot;19530&quot;)

# 2) Define schema
fields = [
    FieldSchema(name=&quot;chunk_id&quot;, dtype=DataType.VARCHAR, is_primary=True, max_length=64),
    FieldSchema(name=&quot;embedding&quot;, dtype=DataType.FLOAT_VECTOR, dim=d),  # d per your encoder
    FieldSchema(name=&quot;paper_id&quot;, dtype=DataType.VARCHAR, max_length=64),
    FieldSchema(name=&quot;section_type&quot;, dtype=DataType.VARCHAR, max_length=64),
]
schema = CollectionSchema(fields, description=&quot;UKB chunks&quot;)
col = Collection(&quot;ukb_chunks&quot;, schema=schema)

# 3) Create index (HNSW example)
index_params = {&quot;index_type&quot;: &quot;HNSW&quot;, &quot;metric_type&quot;: &quot;IP&quot;, &quot;params&quot;: {&quot;M&quot;: 32, &quot;efConstruction&quot;: 200}}
col.create_index(field_name=&quot;embedding&quot;, index_params=index_params)

# 4) Insert data (ids, vectors, metadata)
col.insert([chunk_ids, embeddings, paper_ids, section_types])
col.flush()

# 5) Search with a query vector q
search_params = {&quot;metric_type&quot;: &quot;IP&quot;, &quot;params&quot;: {&quot;ef&quot;: 256}}
res = col.search([q], &quot;embedding&quot;, param=search_params, limit=20, output_fields=[&quot;paper_id&quot;, &quot;section_type&quot;])
# res returns top-k with similarity scores per hit
```

---

### TL;DR

- **Vector DB + embeddings** → fast **retrieval scores** for search (cosine/dot-product).
- **BERTopic + mapping** → interpretable **section scores** (your 0.7*emb + 0.3*kw) for labeling clusters.
- Different scores, different roles; both are essential to your stack.</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]

# RAG + Topic Reclassification: How Vectors, Retrieval, and BERTopic Scores Fit Together

This report connects the dots between your **vector database**, **vector space embeddings**, **retrieval**, and the **BERTopic scores** you’re seeing in the logs. It also clarifies **which progress bars** correspond to which stage so you can read runtime output with confidence.

---

## 1) End-to-End Overview (Plain English)

**Goal:** take many journal chunks and (a) make them searchable with vectors, and (b) reclassify “other/unknown” chunks into real sections (abstract, methods, results, etc.).

**Pipeline (two intertwined tracks):**

```
          ┌───────────┐
          │  Chunks   │  (text bodies + metadata)
          └─────┬─────┘
                │
                │ 1. Embedding  f(text) → R^d   (GPU)
                ▼
        ┌───────────────┐
        │ Vector Space  │  (one vector per chunk)
        └─────┬─────────┘
              │
              │ 2. Index in Vector DB (e.g., Milvus) for ANN search
              ▼
        ┌───────────────┐           ┌──────────────────────────┐
        │  Retrieval    │◄── query ─┤ embed(query) → vector q  │
        └───────────────┘           └──────────────────────────┘
              │  returns top-k by cosine/dot-product similarity
              ▼
      (RAG consumer: downstream ranking/reading/QA, etc.)


In parallel for section reclassification:

Chunks ──embeddings (GPU)──► UMAP (GPU) ─► HDBSCAN (GPU) ─► BERTopic topic words (CPU c‑TF‑IDF)
                                            │
                                            ▼
                                     Section mapping (your Score = 0.7*emb + 0.3*kw)
```

- **Vector DB track:** makes your chunks searchable with **similarity scores** (cosine or dot-product).
- **BERTopic track:** groups similar chunks into topics and **names** those groups with top words; you then **map topics → canonical sections** using your combined score.

**Important:** the “Score” values in your logs are **classification scores** (for section mapping), **not** retrieval scores from the vector DB.

---

## 2) Vector Database & “Vector Space” (Technical)

- Each chunk \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}(t_i\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}) is embedded via \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}( \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}x\textbraceright{}_i = f(t_i) \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}in \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbb\textbraceleft{\textbraceright{}R\textbraceright{}^d \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}).
- Store \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}((\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}text\textbraceleft{\textbraceright{}id\textbraceright{}, \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}x\textbraceright{}_i, \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}text\textbraceleft{\textbraceright{}metadata\textbraceright{})\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}) in a **vector database** (e.g., Milvus).
- Build an **ANN (Approximate Nearest Neighbor)** index:
  - Common choices: **HNSW**, **IVF-FLAT/IVF-PQ**, **DiskANN** (vendor-dependent).
  - Trade‑off: recall vs speed vs memory.
- Metadata (e.g., `paper_id`, `section_type`, `doi`) is kept for **filters** and downstream joins (often in the same vector DB or in a side store like DuckDB).

**Schema sketch (conceptual):**
```text
chunk_id: str PRIMARY KEY
embedding: float[d]  # dense vector
text: str
section_type: str
paper_id: str
source: str
... (other attributes for filters)
```

---

## 3) Embeddings & Similarity (Math)

Let \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}( f(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}cdot) \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}) be the encoder (e.g., SentenceTransformer). For any text:

- **Embedding:** \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}( \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}x\textbraceright{} = f(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}text\textbraceleft{\textbraceright{}text\textbraceright{}) \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}in \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbb\textbraceleft{\textbraceright{}R\textbraceright{}^d \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}).
- Often we **L2-normalize** embeddings so cosine and dot-product are aligned.

**Cosine similarity:**
\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}[
\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}operatorname\textbraceleft{\textbraceright{}cos\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}_sim\textbraceright{}(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}x\textbraceright{}, \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}y\textbraceright{}) = \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}frac\textbraceleft{\textbraceright{}\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}x\textbraceright{} \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}cdot \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}y\textbraceright{}\textbraceright{}\textbraceleft{\textbraceright{}\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}|\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}x\textbraceright{}\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}| \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}; \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}|\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}y\textbraceright{}\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}|\textbraceright{}
\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}]

**Retrieval:** Given a query \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}(q\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}) with embedding \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}q\textbraceright{}\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}), return top‑\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}(k\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}) chunks by similarity \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}(s_i = \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}operatorname\textbraceleft{\textbraceright{}cos\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}_sim\textbraceright{}(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}q\textbraceright{}, \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}x\textbraceright{}_i)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}) (or by dot‑product if that’s how your embeddings are trained).

**Mini numeric example (illustrative):**
- \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}q\textbraceright{}=(1,0,0)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}), \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}x\textbraceright{}_1=(0.9,0.1,0)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}), \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}x\textbraceright{}_2=(0.3,0.95,0)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{})
- \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}cos(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}q\textbraceright{},\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}x\textbraceright{}_1)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}approx 0.99\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}), \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}cos(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}q\textbraceright{},\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}mathbf\textbraceleft{\textbraceright{}x\textbraceright{}_2)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}approx 0.30\textbackslash\textbraceleft{\textbraceright{}\textbraceright{})
- Top‑1 = \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}(x_1\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}). The **retrieval score** is the similarity value returned by the vector DB.

---

## 4) BERTopic: Topics, Words, and Your Section “Score”

**Clustering path (you enabled GPU here):**
1. **UMAP** reduces dimensionality of embeddings (still preserves local structure).
2. **HDBSCAN** finds dense clusters; points with no clear cluster → noise.

**Topic representation (CPU):**
3. **Vectorizer (CountVectorizer)** builds a vocabulary for each topic’s documents.
4. **c‑TF‑IDF** weights words to identify the **most representative words** per topic.

**Your section mapping:** you compute a **combined score** for each topic to select a canonical section (abstract, methods, …):

\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}[
\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}text\textbraceleft{\textbraceright{}Score\textbraceright{} = 0.7 \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}times \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}text\textbraceleft{\textbraceright{}EmbeddingSimilarity\textbraceright{} \textbackslash\textbraceleft{\textbraceright{}\textbraceright{};+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}; 0.3 \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}times \textbackslash\textbraceleft{\textbraceright{}\textbraceright{}text\textbraceleft{\textbraceright{}KeywordScore\textbraceright{}
\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}]

- **EmbeddingSimilarity** = cosine similarity between the topic’s pooled text embedding and a canonical section embedding.
- **KeywordScore** = fraction of section‑specific keywords found in the pooled text.

That’s the **Score** in your logs (e.g., `0.803 (emb: 0.902, kw: 0.571)`).

---

## 5) Progress Bars: What You’re Seeing

- **Embeddings:** `Batches: xx% | ... | n/N [.. it/s]` with many batches. *GPU-heavy* (your SentenceTransformer on CUDA).
- **Clustering:** brief logs from cuML/UMAP/HDBSCAN. *GPU-heavy but short*.
- **BERTopic “Fitting model” & `Batches: 100%| 1/1 ...` lines:** fast single‑batch updates while **topic words** are computed with **CountVectorizer + c‑TF‑IDF**. *CPU‑heavy; GPU idle is normal here.*

---

## 6) Sample Log Extracts (Verbatim)

```
INFO:__main__:  Topic 692 -> references
INFO:__main__:    Score: 0.638 (emb: 0.911, kw: 0.000)
INFO:__main__:    Words: info, article, , , 
```

```
INFO:__main__:  Topic 695 -> tables_and_figures
INFO:__main__:    Score: 0.741 (emb: 0.898, kw: 0.375)
INFO:__main__:    Words: lv, rv, chamber, indexes, es
```

```
INFO:__main__:  Topic 697 -> acknowledgments
INFO:__main__:    Score: 0.700 (emb: 0.928, kw: 0.167)
INFO:__main__:    Words: acknowledgements, funding, advancements, acknowledgement, acknowledged
```

---

## 7) Reclassified Topics (Your Provided Slice)

| Topic ID | Section                     | Score | Embedding Sim | Keyword Score | Top Words                                      |
|----------|-----------------------------|-------|---------------|---------------|------------------------------------------------|
| 692      | references                  | 0.638 | 0.911         | 0.000         | info, article, , ,                             |
| 693      | discussion                  | 0.688 | 0.921         | 0.143         | limitations, limitation, study, ,              |
| 694      | abstract                    | 0.669 | 0.902         | 0.125         | author, summary, , ,                           |
| 695      | tables_and_figures          | 0.741 | 0.898         | 0.375         | lv, rv, chamber, indexes, es                   |
| 696      | data_and_code_availability  | 0.679 | 0.909         | 0.143         | biobank, uk, data, mellitus, diabetes          |
| 697      | acknowledgments             | 0.700 | 0.928         | 0.167         | acknowledgements, funding, advancements, acknowledgement, acknowledged |
| 698      | funding                     | 0.628 | 0.897         | 0.000         | gla, publications, glasgow, members, ac        |
| 699      | methods                     | 0.735 | 0.942         | 0.250         | legend, description, biologists, repeatedly, monte |
| 700      | abstract                    | 0.671 | 0.904         | 0.125         | setup, experimental, experiment, experiments, setups |
| 701      | supplementary_materials     | 0.639 | 0.913         | 0.000         | software, code, , ,                            |
| 702      | tables_and_figures          | 0.708 | 0.905         | 0.250         | mediation, modelling, annu, vanderweele, fow   |
| 703      | results                     | 0.648 | 0.878         | 0.111         | ahead, print, publish, revised, editorial      |
| 704      | supplementary_materials     | 0.771 | 0.917         | 0.429         | jaha, 1161, suppl, ahajournals, athttps        |
| 705      | ethics_approval             | 0.803 | 0.902         | 0.571         | ethics, approval, consent, committee, standards |

---

## 8) How Retrieval Scores Relate to This

- **Retrieval score** (from the vector DB) = similarity between **query embedding** and **chunk embedding** (cosine or dot-product). Used to **rank** chunks for a question/need.
- **Your “Score” in logs** = **section-classification** confidence combining embedding similarity to a canonical section and keyword overlap. Used to **label** topics.

These are different scores for different purposes. It’s valid (and common) to use both: retrieval to **find** the right chunks, BERTopic+mapping to **understand** and **organize** them.

---

## 9) Practical Knobs

- **Retrieval quality:**
  - Ensure embeddings are **normalized** if using cosine similarity.
  - Pick an ANN index (e.g., **HNSW**) with enough `ef_construction`/`M`; at query time tune `ef` (HNSW) for recall.
  - Filter by metadata (e.g., section) to reduce noise before ranking.

- **Topic quality (BERTopic):**
  - Keep `ngram_range=(1,1)`, `min_df≥5`, `max_df≤0.9` for speed + cleaner vocab.
  - Improve tokenization to drop punctuation-only tokens: `token_pattern=r'(?u)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}b[A-Za-z][A-Za-z]+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}b'`.
  - Adjust your weight mix if needed: `Score = α*emb + (1-α)*kw` (you currently use α=0.7).

---

## 10) Pseudocode: Insert & Search (Milvus-style)

```python
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType

# 1) Connect
connections.connect("default", host="milvus", port="19530")

# 2) Define schema
fields = [
    FieldSchema(name="chunk_id", dtype=DataType.VARCHAR, is_primary=True, max_length=64),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=d),  # d per your encoder
    FieldSchema(name="paper_id", dtype=DataType.VARCHAR, max_length=64),
    FieldSchema(name="section_type", dtype=DataType.VARCHAR, max_length=64),
]
schema = CollectionSchema(fields, description="UKB chunks")
col = Collection("ukb_chunks", schema=schema)

# 3) Create index (HNSW example)
index_params = \textbraceleft{\textbraceright{}"index_type": "HNSW", "metric_type": "IP", "params": \textbraceleft{\textbraceright{}"M": 32, "efConstruction": 200\textbraceright{}\textbraceright{}
col.create_index(field_name="embedding", index_params=index_params)

# 4) Insert data (ids, vectors, metadata)
col.insert([chunk_ids, embeddings, paper_ids, section_types])
col.flush()

# 5) Search with a query vector q
search_params = \textbraceleft{\textbraceright{}"metric_type": "IP", "params": \textbraceleft{\textbraceright{}"ef": 256\textbraceright{}\textbraceright{}
res = col.search([q], "embedding", param=search_params, limit=20, output_fields=["paper_id", "section_type"])
# res returns top-k with similarity scores per hit
```

---

### TL;DR

- **Vector DB + embeddings** → fast **retrieval scores** for search (cosine/dot-product).
- **BERTopic + mapping** → interpretable **section scores** (your 0.7*emb + 0.3*kw) for labeling clusters.
- Different scores, different roles; both are essential to your stack.
\end{Verbatim}
~~~










~~~markdown
# Adjusted Reclassification Comparison

This table shows the section counts and percentages before and after reclassification, where 'other' has been reallocated.

| Section                    |   Before Count | Before %   |   After Count | After %   |
|:---------------------------|---------------:|:-----------|--------------:|:----------|
| abstract                   |          14511 | 2.00%      |         45282 | 5.09%     |
| acknowledgments            |           7420 | 1.02%      |         14669 | 1.65%     |
| author_info                |          11447 | 1.58%      |         29982 | 3.37%     |
| conclusion                 |          11679 | 1.61%      |         20296 | 2.28%     |
| conflict_of_interest       |           4202 | 0.58%      |         10867 | 1.22%     |
| data_and_code_availability |          20450 | 2.81%      |         46079 | 5.18%     |
| discussion                 |          53475 | 7.36%      |         65007 | 7.30%     |
| ethics_approval            |           1845 | 0.25%      |          8435 | 0.95%     |
| funding                    |           3425 | 0.47%      |         18152 | 2.04%     |
| introduction               |          24792 | 3.41%      |         34990 | 3.93%     |
| keywords                   |           1211 | 0.17%      |          3093 | 0.35%     |
| methods                    |          56796 | 7.82%      |         73392 | 8.24%     |
| notes                      |           1702 | 0.23%      |          8457 | 0.95%     |
| other                      |         214293 | 29.49%     |             0 | 0.00%     |
| raw_headings               |         163664 | 22.53%     |        163707 | 18.39%    |
| references                 |          42103 | 5.79%      |         67909 | 7.63%     |
| results                    |          36051 | 4.96%      |        140161 | 15.74%    |
| study_participants         |          17569 | 2.42%      |         49684 | 5.58%     |
| supplementary_materials    |          15556 | 2.14%      |         45056 | 5.06%     |
| tables_and_figures         |          20782 | 2.86%      |         45015 | 5.06%     |
| unknown                    |           3596 | 0.49%      |             0 | 0.00%     |
| TOTAL                      |         726569 | 100.00%    |        890233 | 100.00%   |
~~~








![](https://prod-files-secure.s3.us-west-2.amazonaws.com/7a8a68e8-5855-41f2-ae17-565ecdbd6218/9c7b3b06-a401-48f6-a63d-ccefd9025a5c/before_distribution.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466V6DJYT4J%2F20260112%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20260112T014054Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEBgaCXVzLXdlc3QtMiJIMEYCIQCsyF2a2NWr3I8TZFSFkxG88jEJKBLasnvaDDTvgM9J3QIhAN8KrgcQ6Be1An%2Bx3GKhd3UIaBpU%2BO3Q5v4vsG6g4P7UKogECOH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgxZSj6BGcqiXPUb2Acq3ANHK8R6bN3fxURQG23s4hwdPFiuBFpy7vaOWGsbzBouc3zeTknxW7zkBpU3paksREW%2FjsG8lmHpAEL74IfFRuLr3kSMA1und6V7%2FQRWes8zXsoMCv13t6EWFi4pqT0HeYSZhRyJ84kVe08H218KbY%2BF47BM7P5KY%2F%2FKbbyuRuXIxKRpr3ZUw%2BvU%2FyvAX2VL3qwRrPC6kpKNU1dX%2Fg19OCyG9bP2BleMRaZVzO9G8oR1afJVNUd1a%2BcNFfvOBO18ZAd%2BjHl%2FqHCmfSI6ZQiLiudL8eZ7K89gHRRM5HffJGqVi690sGDpfHq4NGOd6NbJ3aeyxai3jPD3cfc%2FCn51DPZSZe%2BOVuQWo8dY24FG6D%2FcwAd6dryHtsJA%2BbDEE8r8IooZ9BbLeyHE9yysQoVsw%2B1Z33Rww5WPxlaEigbuWtM0WZDejTmzdxecspbz%2F02sTHSdNs2ZvYIi8KWzRI9gFly%2FUrrco9RCG4jxK1IVbtTzR6yPLv3y35OHH4NmghyjUDP706l8h7phLOJrG1Lqsf1ifG%2Fe5rEye%2BTCHxGGejCrxGVzqchuG%2F9s%2FmOVECpWPtr9bEOIqfyRcZxn7jB5fNsafYJ6xawn8C%2FOgJvRnJlVqcBvlGTjm%2BLSF3EBGzDk5JDLBjqkATkg9KrjrbSpIUxXN7c8wOQ5ueTrTam5zxD4rnMNcDxvr2PiaQxDK9yHrp0TGqF1x9ga6GIaHxjVR%2FWW885QuEKd%2FJ3t8aqy%2F26DvZzB6AEr%2BH6yiP2J%2F%2BgOigFjFzEC1eLzZi4ZrAXkhoXcr0LiXlE0Tt%2FIsi43bwF7hemFQyYpXIw9cwIphJHuRrpOVaKOQtm3Me0PlOznk7pHJlemptJ0xeoZ&X-Amz-Signature=cbb418a6075202fe3447358cfb9498caf197d2faadca3eb8be90bf6f81799361&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)






![](https://prod-files-secure.s3.us-west-2.amazonaws.com/7a8a68e8-5855-41f2-ae17-565ecdbd6218/b4f46d8b-ff72-4583-957a-9c4ef93bf427/after_distribution.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466V6DJYT4J%2F20260112%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20260112T014054Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEBgaCXVzLXdlc3QtMiJIMEYCIQCsyF2a2NWr3I8TZFSFkxG88jEJKBLasnvaDDTvgM9J3QIhAN8KrgcQ6Be1An%2Bx3GKhd3UIaBpU%2BO3Q5v4vsG6g4P7UKogECOH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgxZSj6BGcqiXPUb2Acq3ANHK8R6bN3fxURQG23s4hwdPFiuBFpy7vaOWGsbzBouc3zeTknxW7zkBpU3paksREW%2FjsG8lmHpAEL74IfFRuLr3kSMA1und6V7%2FQRWes8zXsoMCv13t6EWFi4pqT0HeYSZhRyJ84kVe08H218KbY%2BF47BM7P5KY%2F%2FKbbyuRuXIxKRpr3ZUw%2BvU%2FyvAX2VL3qwRrPC6kpKNU1dX%2Fg19OCyG9bP2BleMRaZVzO9G8oR1afJVNUd1a%2BcNFfvOBO18ZAd%2BjHl%2FqHCmfSI6ZQiLiudL8eZ7K89gHRRM5HffJGqVi690sGDpfHq4NGOd6NbJ3aeyxai3jPD3cfc%2FCn51DPZSZe%2BOVuQWo8dY24FG6D%2FcwAd6dryHtsJA%2BbDEE8r8IooZ9BbLeyHE9yysQoVsw%2B1Z33Rww5WPxlaEigbuWtM0WZDejTmzdxecspbz%2F02sTHSdNs2ZvYIi8KWzRI9gFly%2FUrrco9RCG4jxK1IVbtTzR6yPLv3y35OHH4NmghyjUDP706l8h7phLOJrG1Lqsf1ifG%2Fe5rEye%2BTCHxGGejCrxGVzqchuG%2F9s%2FmOVECpWPtr9bEOIqfyRcZxn7jB5fNsafYJ6xawn8C%2FOgJvRnJlVqcBvlGTjm%2BLSF3EBGzDk5JDLBjqkATkg9KrjrbSpIUxXN7c8wOQ5ueTrTam5zxD4rnMNcDxvr2PiaQxDK9yHrp0TGqF1x9ga6GIaHxjVR%2FWW885QuEKd%2FJ3t8aqy%2F26DvZzB6AEr%2BH6yiP2J%2F%2BgOigFjFzEC1eLzZi4ZrAXkhoXcr0LiXlE0Tt%2FIsi43bwF7hemFQyYpXIw9cwIphJHuRrpOVaKOQtm3Me0PlOznk7pHJlemptJ0xeoZ&X-Amz-Signature=54bd493bb3fe15ba6353abe849faa6b1dfc37ff8ab872941f56f86c545ba30a1&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)






MASSIVE PROBLEM, check if it took the test out of circulation or tested all topic options against other, … might not be as thats not the purpose of having the model create the cateogires and hten narrowing it down


