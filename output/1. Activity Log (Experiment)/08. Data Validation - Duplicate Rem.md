# Data Validation - Duplicate Rem


## Content


### Objective


Identify and remove duplicate JSON/Markdown file pairs using DOI matching and metadata quality scoring. This step ensures data integrity by eliminating redundant entries while keeping only the highest-quality version of each document.







---


### Process

1. **Duplicate Detection**
    - All JSON files (main + missing directories) analyzed with `metadata_analyser.py`.
    - DOIs normalized to ISO 26324 standards using `robust_doi_matcher.py`.
    - Found **303 duplicate DOI groups** with multiple files pointing to the same DOI.
2. **Quality-Based Selection**
    - Each duplicate group scored on:
        - `extraction_quality_score` (0–1).
        - `extraction_completeness` (0–1).
        - `doi_exact_match` (boolean).
    - Scores combined into a composite metadata score.
    - Highest scoring file kept; others flagged for removal.
3. **File Management**
    - Duplicate files moved to a backup directory at `/data/ukb/json_validation_issue/`.
    - Scripts generated directory structures, validated moves, and logged all actions.
    - Safety checks prevented accidental overwrites.

    



---


### Validation


Comparison of quality distribution between kept vs deleted files:




<table>
  <thead>
    <tr>
      <th>Quality Level</th>
      <th>Files Kept</th>
      <th>% of Kept</th>
      <th>Files Deleted</th>
      <th>% of Deleted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Perfect (EQ=1.0, EC=1.0, DOI=True)</td>
      <td>28</td>
      <td>12.8%</td>
      <td>13</td>
      <td>4.3%</td>
    </tr>
    <tr>
      <td>High Quality (EQ ≥ 0.9)</td>
      <td>119</td>
      <td>54.6%</td>
      <td>106</td>
      <td>35.0%</td>
    </tr>
    <tr>
      <td>Medium Quality (EQ ≥ 0.7)</td>
      <td>66</td>
      <td>30.3%</td>
      <td>137</td>
      <td>45.2%</td>
    </tr>
    <tr>
      <td>Low Quality (EQ &lt; 0.7)</td>
      <td>5</td>
      <td>2.3%</td>
      <td>47</td>
      <td>15.5%</td>
    </tr>
  </tbody>
</table>



Result: 67.4% of kept files scored High or Perfect, confirming the scoring algorithm prioritized quality.







---


### Reflection

- 303 duplicate groups confirmed — a significant redundancy issue fixed before feature extraction.
- The scoring system worked well, though in edge cases with similar scores, tie-breakers (metadata richness, content length) were needed.
- ISO 26324 DOI normalization was essential — DOIs appear in multiple inconsistent formats in the literature.
- Running in Docker containers provided reproducibility and isolated environments.
- Trade-off: some high-quality duplicates were discarded when scores tied closely.





---


### Next Step


Proceed to 3_extract_syn_ukb_features: extract UK Biobank-specific features from the cleaned, deduplicated dataset.







---


### Technical Implementation


Libraries


- **Python Standard**: `json`, `os`, `re`, `pathlib`, `collections`.
- **Fast JSON**: `orjson`.
- **DOI Handling**: `idutils` (CERN standard DOI normalization/validation).
- **String Comparison**: `rapidfuzz` (Levenshtein distance for fuzzy matching).
- **HTTP/Retry**: `httpx`, `tenacity` (for optional DOI verification).
- **Progress**: `tqdm`.





Docker



Two container builds:


1. **Dockerfile** – Standards-compliant DOI analysis:

    

    ~~~docker
    FROM python:3.11-slim
    RUN pip install --no-cache-dir orjson polars idutils rapidfuzz httpx tenacity tqdm
    ~~~
    
    

2. **Dockerfile.meta** – Metadata quality analysis:
FROM python:3.11-slim
