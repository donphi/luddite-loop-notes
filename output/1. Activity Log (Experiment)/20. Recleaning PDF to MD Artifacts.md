# Recleaning PDF to MD Artifacts


## Content




~~~{=html}
<pre class="notion-ascii-diagram"><code># üìä UK Biobank Data Cleaning Pipeline Report: PDF-to-Markdown Post-Processing for SLATE

## Executive Summary

**Status**: Pipeline development paused for architectural revision  
**Reason**: Discovered critical sentence boundary corruption requiring spaCy NLP intervention  
**Files Processed**: 6,411 UK Biobank scientific papers  
**Initial Issues**: 10.7M quality problems, 2.2M PDF artifacts, 1.4M math formatting issues  
**Resolution Rate**: 40.6% improvement achieved before identifying fundamental sentence structure problems  

---

## SECTION 1: BASIC USER GUIDE

### What We Were Trying to Do

We needed to clean 6,411 scientific papers from UK Biobank that were converted from PDF to Markdown format. These papers would eventually be:
1. **Chunked** - Split into meaningful sentence-level pieces
2. **Tagged** - Labeled with medical entities (genes, diseases, drugs)
3. **Vectorized** - Converted to mathematical representations for AI search
4. **Used in RAG** - Retrieved to answer complex medical questions

### The Problem We Discovered

PDFs don&#39;t convert cleanly to text. Imagine photocopying a book, then having a computer try to read it - lots of things go wrong:

- **Missing spaces**: &quot;The patient was5years old&quot; instead of &quot;The patient was 5 years old&quot;
- **Broken sentences**: Scientific notation like &quot;1.0E-8&quot; became &quot;1.0EThe next sentence...&quot;
- **Lost content**: Entire paragraphs disappeared when the cleaner saw HTML tags
- **Mangled citations**: References like [1] had useless page anchors attached

### What We Fixed (40.6% improvement)

‚úÖ **Successfully Cleaned**:
- 2.8M multiple spaces removed
- 1.4M repeated characters fixed  
- 91K orphaned punctuation cleaned
- 18K bare URLs formatted
- 12K page breaks removed

‚ùå **Still Problematic**:
- 759K missing spaces after punctuation
- 5M inconsistent quotes
- 764K empty table cells
- 360K math formatting issues

### Why We&#39;re Stopping

We discovered that even after cleaning, sentences aren&#39;t properly connected. The punctuation fixes we made aren&#39;t enough - we need a smarter approach using spaCy (a language understanding tool) to:
- Recognize where sentences actually begin and end
- Understand context (is &quot;Dr.&quot; an abbreviation or end of sentence?)
- Properly reconnect broken scientific text

### Impact on the Project

**Without proper sentence boundaries**, the tagging system can&#39;t:
- Identify complete medical concepts
- Link related information across sentence fragments
- Provide coherent context for the AI

**The new approach** will use spaCy&#39;s medical language models to intelligently reconstruct sentences before chunking.

---

## SECTION 2: ADVANCED TECHNICAL DOCUMENTATION

### Architecture Overview

```
Pipeline: PDF ‚Üí Marker ‚Üí Markdown ‚Üí Cleaner ‚Üí Chunker ‚Üí Tagger ‚Üí Vectorizer ‚Üí DuckDB ‚Üí SLATE
                                      ‚Üë
                                 [WE ARE HERE]
```

### Initial Implementation

#### Core Cleaner Architecture (`cleaner.py`)

**Class Structure**:
```python
PolicyMDCleaner:
‚îú‚îÄ‚îÄ fix_ligatures()         # Unicode ligature replacement (Ô¨Å‚Üífi)
‚îú‚îÄ‚îÄ dehyphenate()           # Word-break repair across lines
‚îú‚îÄ‚îÄ normalize_whitespace()  # Space/newline standardization
‚îú‚îÄ‚îÄ standardize_headings()  # H4+ ‚Üí H3 max normalization
‚îú‚îÄ‚îÄ remove_pdf_artifacts()  # OCR noise removal
‚îú‚îÄ‚îÄ fix_punctuation_spacing() # [CRITICAL: 759K issues remaining]
‚îú‚îÄ‚îÄ clean_and_collect_artifacts() # [BUG: Data loss from span tags]
‚îî‚îÄ‚îÄ count_metrics_after_char_start() # Post-processing metrics
```

### Critical Issues Discovered

#### 1. **Span Tag Data Loss Bug** (Lines 506-513)
```python
# BROKEN CODE - Deletes entire lines containing span tags
if re.match(r&#39;&lt;span.*?&lt;/span&gt;&#39;, line):
    i += 1
    continue  # LOSES ENTIRE PARAGRAPH!
```

**Impact**: Lost ~5-10% of document content where PDF page markers existed

**Fix Applied**:
```python
# Remove span tags but preserve content
line = re.sub(r&#39;&lt;span[^&gt;]*id=&quot;page-[^&quot;]*&quot;[^&gt;]*&gt;&lt;/span&gt;&#39;, &#39;&#39;, line)
```

#### 2. **Decimal Number Protection Conflict**

**Problem**: Scientific notation and decimal numbers followed by text created 624,160 period-letter patterns:
- `.o` patterns: 144,737 instances (from &quot;4.0Our study...&quot;)
- `.e` patterns: 77,885 instances (from &quot;1.0EThe results...&quot;)
- `.u` patterns: 70,401 instances

**Root Cause**: Protection pattern prevented space insertion:
```python
# Protected &quot;4.0&quot; ‚Üí couldn&#39;t add space before &quot;Our&quot;
text = re.sub(r&#39;\b\d+\.\d+(?=[\s,;:!?)\]}\-]|$)&#39;, protect_pattern, text)
```

#### 3. **Over-Protection of Scientific Content**

**30,967 unique patterns protected**, including:
- P-values: &quot;P &lt; 0.05&quot; (2,410 instances)
- DOIs: &quot;10.1016/j.&quot; (1,349 instances)  
- Decimals: &quot;4.0&quot; (1,276 instances)
- Statistics: &quot;p &lt; 0.001&quot; (1,828 instances)

**Protection‚ÜíFix‚ÜíRestore Cycle Failed**: Aggressive punctuation fixes couldn&#39;t operate on protected content

### Detailed Issue Analysis

#### Punctuation Spacing Problems (759,121 remaining)

**Pattern Distribution**:
```
Period-letter:   624,160 (82.2%)
Comma-letter:      6,901 (0.9%)
Colon-letter:     11,701 (1.5%)
Other:           116,359 (15.4%)
```

**Reduction Rates by File**:
- 0-10% improvement: 569 files
- 10-25% improvement: 10,099 files
- 25-50% improvement: 12,115 files
- 50-75% improvement: 6,527 files
- 75-100% improvement: 3,171 files

#### PDF Artifact Patterns

**Successfully Removed**:
- `repeated_chars`: 92.2% reduction (1.4M instances)
- `page_breaks`: 100% removal
- `page_numbers`: 98.5% removal

**Persistent Issues**:
- `font_tags`: 236,292 remaining (30.2% reduction)
- `hyphenation`: 12,139 remaining (15.6% reduction)
- `ocr_artifacts`: 38,086 remaining (8.3% reduction)

### Implementation Challenges

#### 1. **Docker Build Cache Issues**

Despite `--no-cache` flag, changes weren&#39;t taking effect due to:
- Volume mounts overriding COPY instructions
- Python bytecode caching
- BuildKit layer caching

**Resolution**:
```bash
docker system prune -af --volumes
rm -rf ~/.docker/buildkit
find . -name &quot;*.pyc&quot; -delete
```

#### 2. **Multiprocessing Complications**

- 16 workers with batch size 128
- Process-specific logging to avoid file conflicts
- Memory management for 6,411 files

#### 3. **Regex Pattern Conflicts**

- 17 protection categories with overlapping patterns
- Order-dependent regex substitutions
- Nested protection placeholder issues

### Performance Metrics

**Processing Statistics**:
- Initial issues: 10,711,098
- Issues fixed: 4,208,580 (39.3%)
- Equations preserved: 1,142,514/1,431,603 (79.8%)
- Tables preserved: 24,984/26,446 (94.5%)

**Category Improvements**:
| Category | Before | After | Improvement |
|----------|--------|-------|------------|
| Frontmatter | 31,854 | 226 | 99.3% |
| PDF Artifacts | 2,181,740 | 581,080 | 73.4% |
| Quality | 10,711,098 | 6,502,518 | 39.3% |
| Math | 1,433,035 | 1,143,830 | 20.2% |

### Why Current Approach Failed

#### Fundamental Sentence Boundary Problem

**Example Case**:
```
INPUT:  &quot;The p-value was 1.0E-8However the results...&quot;
STEP 1: Protect &quot;1.0E-8&quot; ‚Üí &quot;__PROT_00001__However&quot;
STEP 2: Fix &quot;.H&quot; ‚Üí &quot;. H&quot; (but can&#39;t - it&#39;s protected)
STEP 3: Restore ‚Üí &quot;1.0E-8However&quot; (no improvement)
```

**Regex Limitations**:
- Can&#39;t understand context (Dr. Smith vs end of sentence)
- No semantic understanding of scientific notation
- Unable to handle ambiguous boundaries

### Proposed spaCy Solution

#### Architecture Change
```python
# NEW PIPELINE
text ‚Üí spaCy NLP ‚Üí Sentence Segmentation ‚Üí Boundary Correction ‚Üí 
       Token Analysis ‚Üí Context-Aware Cleaning ‚Üí Validated Output
```

#### Implementation Plan

1. **Load Biomedical spaCy Model**:
```python
import spacy
from scispacy.linking import EntityLinker

nlp = spacy.load(&quot;en_core_sci_lg&quot;)  # or &quot;en_ner_bc5cdr_md&quot; for biomedical
```

2. **Sentence Boundary Detection**:
```python
doc = nlp(text)
sentences = [sent.text for sent in doc.sents]
```

3. **Context-Aware Repair**:
- Merge fragments using dependency parsing
- Resolve abbreviations using biomedical dictionaries
- Reconstruct scientific notation contextually

4. **Validation Layer**:
- Ensure each sentence has subject-verb structure
- Verify citation integrity
- Confirm mathematical expression completeness

### Technical Debt &amp; Lessons Learned

1. **Protection Pattern Complexity**: 30K+ unique patterns created unmaintainable regex maze
2. **Data Loss Through Deletion**: Removing entire lines for single artifacts catastrophic
3. **Order-Dependent Regex**: Sequential substitutions created race conditions
4. **Missing Integration Tests**: No validation between pipeline stages
5. **Insufficient Logging**: Multiprocessing obscured error patterns

### Migration Path

1. **Immediate**: Stop current pipeline, preserve 40.6% improved files
2. **Short-term**: Implement spaCy sentence reconstruction
3. **Medium-term**: Validate against manually annotated subset
4. **Long-term**: Integrate with SLATE for feedback loop

### Repository Structure
```
data_conversion/ukb/
‚îú‚îÄ‚îÄ 4_tagging/
‚îÇ   ‚îú‚îÄ‚îÄ 1_chunk_enhanced/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 8_md_cleaner3/  [CURRENT]
‚îÇ   ‚îî‚îÄ‚îÄ 2_spacy_sentences/   [PROPOSED]
‚îî‚îÄ‚îÄ 5_vectorization/
    ‚îî‚îÄ‚îÄ duckdb_storage/
```

---

## Appendices

### A. Critical Code Sections

**Problematic Protection (Lines 243-245)**:
```python
text = re.sub(r&#39;\b\d+\.\d+(?=[\s,;:!?)\]}\-]|$)&#39;, protect_pattern, text)
text = re.sub(r&#39;\b\d+\.?\d*\s*[-‚Äì‚Äî]\s*\d+\.?\d*\b&#39;, protect_pattern, text)
text = re.sub(r&#39;\b\d+\.?\d*%&#39;, protect_pattern, text)
```

**Citation Cleanup (Lines 183-190)**:
```python
line = re.sub(r&#39;\[\[?(\d+(?:[,\s‚Äì-]+\d+)*)\]?\]\(#page[^)]+\)&#39;, r&#39;[\1]&#39;, line)
line = re.sub(r&#39;\\\[(\d+(?:[,\s‚Äì-]+\d+)*)\\\]&#39;, r&#39;[\1]&#39;, line)
line = re.sub(r&#39;\[(\d+)\]\([^)]+\)[‚Äì-]+\[(\d+)\]\([^)]+\)&#39;, r&#39;[\1-\2]&#39;, line)
line = re.sub(r&#39;\[\[(\d+(?:[,\s‚Äì-]+\d+)*)\]\]&#39;, r&#39;[\1]&#39;, line)
```

### B. Issue Pattern Examples

**Before Cleaning**:
```
[[1](#page - 6-0)], [\[2\]](#page - 7-0)
&lt;span id=&quot;page-1-0&quot;&gt;&lt;/span&gt;age, sex, ethnicity...
The p-value was1.0E-8However the study...
```

**After Current Cleaning**:
```
[1], [2]
age, sex, ethnicity...
The p-value was1.0E-8However the study...  [STILL BROKEN]
```

**After Proposed spaCy**:
```
[1], [2]
age, sex, ethnicity...
The p-value was 1.0E-8. However the study...  [PROPERLY SEPARATED]
```</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
# üìä UK Biobank Data Cleaning Pipeline Report: PDF-to-Markdown Post-Processing for SLATE

## Executive Summary

**Status**: Pipeline development paused for architectural revision  
**Reason**: Discovered critical sentence boundary corruption requiring spaCy NLP intervention  
**Files Processed**: 6,411 UK Biobank scientific papers  
**Initial Issues**: 10.7M quality problems, 2.2M PDF artifacts, 1.4M math formatting issues  
**Resolution Rate**: 40.6% improvement achieved before identifying fundamental sentence structure problems  

---

## SECTION 1: BASIC USER GUIDE

### What We Were Trying to Do

We needed to clean 6,411 scientific papers from UK Biobank that were converted from PDF to Markdown format. These papers would eventually be:
1. **Chunked** - Split into meaningful sentence-level pieces
2. **Tagged** - Labeled with medical entities (genes, diseases, drugs)
3. **Vectorized** - Converted to mathematical representations for AI search
4. **Used in RAG** - Retrieved to answer complex medical questions

### The Problem We Discovered

PDFs don't convert cleanly to text. Imagine photocopying a book, then having a computer try to read it - lots of things go wrong:

- **Missing spaces**: "The patient was5years old" instead of "The patient was 5 years old"
- **Broken sentences**: Scientific notation like "1.0E-8" became "1.0EThe next sentence..."
- **Lost content**: Entire paragraphs disappeared when the cleaner saw HTML tags
- **Mangled citations**: References like [1] had useless page anchors attached

### What We Fixed (40.6% improvement)

‚úÖ **Successfully Cleaned**:
- 2.8M multiple spaces removed
- 1.4M repeated characters fixed  
- 91K orphaned punctuation cleaned
- 18K bare URLs formatted
- 12K page breaks removed

‚ùå **Still Problematic**:
- 759K missing spaces after punctuation
- 5M inconsistent quotes
- 764K empty table cells
- 360K math formatting issues

### Why We're Stopping

We discovered that even after cleaning, sentences aren't properly connected. The punctuation fixes we made aren't enough - we need a smarter approach using spaCy (a language understanding tool) to:
- Recognize where sentences actually begin and end
- Understand context (is "Dr." an abbreviation or end of sentence?)
- Properly reconnect broken scientific text

### Impact on the Project

**Without proper sentence boundaries**, the tagging system can't:
- Identify complete medical concepts
- Link related information across sentence fragments
- Provide coherent context for the AI

**The new approach** will use spaCy's medical language models to intelligently reconstruct sentences before chunking.

---

## SECTION 2: ADVANCED TECHNICAL DOCUMENTATION

### Architecture Overview

```
Pipeline: PDF ‚Üí Marker ‚Üí Markdown ‚Üí Cleaner ‚Üí Chunker ‚Üí Tagger ‚Üí Vectorizer ‚Üí DuckDB ‚Üí SLATE
                                      ‚Üë
                                 [WE ARE HERE]
```

### Initial Implementation

#### Core Cleaner Architecture (`cleaner.py`)

**Class Structure**:
```python
PolicyMDCleaner:
‚îú‚îÄ‚îÄ fix_ligatures()         # Unicode ligature replacement (Ô¨Å‚Üífi)
‚îú‚îÄ‚îÄ dehyphenate()           # Word-break repair across lines
‚îú‚îÄ‚îÄ normalize_whitespace()  # Space/newline standardization
‚îú‚îÄ‚îÄ standardize_headings()  # H4+ ‚Üí H3 max normalization
‚îú‚îÄ‚îÄ remove_pdf_artifacts()  # OCR noise removal
‚îú‚îÄ‚îÄ fix_punctuation_spacing() # [CRITICAL: 759K issues remaining]
‚îú‚îÄ‚îÄ clean_and_collect_artifacts() # [BUG: Data loss from span tags]
‚îî‚îÄ‚îÄ count_metrics_after_char_start() # Post-processing metrics
```

### Critical Issues Discovered

#### 1. **Span Tag Data Loss Bug** (Lines 506-513)
```python
# BROKEN CODE - Deletes entire lines containing span tags
if re.match(r'<span.*?</span>', line):
    i += 1
    continue  # LOSES ENTIRE PARAGRAPH!
```

**Impact**: Lost ~5-10% of document content where PDF page markers existed

**Fix Applied**:
```python
# Remove span tags but preserve content
line = re.sub(r'<span[^>]*id="page-[^"]*"[^>]*></span>', '', line)
```

#### 2. **Decimal Number Protection Conflict**

**Problem**: Scientific notation and decimal numbers followed by text created 624,160 period-letter patterns:
- `.o` patterns: 144,737 instances (from "4.0Our study...")
- `.e` patterns: 77,885 instances (from "1.0EThe results...")
- `.u` patterns: 70,401 instances

**Root Cause**: Protection pattern prevented space insertion:
```python
# Protected "4.0" ‚Üí couldn't add space before "Our"
text = re.sub(r'\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}b\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}.\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+(?=[\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}s,;:!?)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}]\textbraceright{}\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}-]|$)', protect_pattern, text)
```

#### 3. **Over-Protection of Scientific Content**

**30,967 unique patterns protected**, including:
- P-values: "P < 0.05" (2,410 instances)
- DOIs: "10.1016/j." (1,349 instances)  
- Decimals: "4.0" (1,276 instances)
- Statistics: "p < 0.001" (1,828 instances)

**Protection‚ÜíFix‚ÜíRestore Cycle Failed**: Aggressive punctuation fixes couldn't operate on protected content

### Detailed Issue Analysis

#### Punctuation Spacing Problems (759,121 remaining)

**Pattern Distribution**:
```
Period-letter:   624,160 (82.2%)
Comma-letter:      6,901 (0.9%)
Colon-letter:     11,701 (1.5%)
Other:           116,359 (15.4%)
```

**Reduction Rates by File**:
- 0-10% improvement: 569 files
- 10-25% improvement: 10,099 files
- 25-50% improvement: 12,115 files
- 50-75% improvement: 6,527 files
- 75-100% improvement: 3,171 files

#### PDF Artifact Patterns

**Successfully Removed**:
- `repeated_chars`: 92.2% reduction (1.4M instances)
- `page_breaks`: 100% removal
- `page_numbers`: 98.5% removal

**Persistent Issues**:
- `font_tags`: 236,292 remaining (30.2% reduction)
- `hyphenation`: 12,139 remaining (15.6% reduction)
- `ocr_artifacts`: 38,086 remaining (8.3% reduction)

### Implementation Challenges

#### 1. **Docker Build Cache Issues**

Despite `--no-cache` flag, changes weren't taking effect due to:
- Volume mounts overriding COPY instructions
- Python bytecode caching
- BuildKit layer caching

**Resolution**:
```bash
docker system prune -af --volumes
rm -rf ~/.docker/buildkit
find . -name "*.pyc" -delete
```

#### 2. **Multiprocessing Complications**

- 16 workers with batch size 128
- Process-specific logging to avoid file conflicts
- Memory management for 6,411 files

#### 3. **Regex Pattern Conflicts**

- 17 protection categories with overlapping patterns
- Order-dependent regex substitutions
- Nested protection placeholder issues

### Performance Metrics

**Processing Statistics**:
- Initial issues: 10,711,098
- Issues fixed: 4,208,580 (39.3%)
- Equations preserved: 1,142,514/1,431,603 (79.8%)
- Tables preserved: 24,984/26,446 (94.5%)

**Category Improvements**:
| Category | Before | After | Improvement |
|----------|--------|-------|------------|
| Frontmatter | 31,854 | 226 | 99.3% |
| PDF Artifacts | 2,181,740 | 581,080 | 73.4% |
| Quality | 10,711,098 | 6,502,518 | 39.3% |
| Math | 1,433,035 | 1,143,830 | 20.2% |

### Why Current Approach Failed

#### Fundamental Sentence Boundary Problem

**Example Case**:
```
INPUT:  "The p-value was 1.0E-8However the results..."
STEP 1: Protect "1.0E-8" ‚Üí "__PROT_00001__However"
STEP 2: Fix ".H" ‚Üí ". H" (but can't - it's protected)
STEP 3: Restore ‚Üí "1.0E-8However" (no improvement)
```

**Regex Limitations**:
- Can't understand context (Dr. Smith vs end of sentence)
- No semantic understanding of scientific notation
- Unable to handle ambiguous boundaries

### Proposed spaCy Solution

#### Architecture Change
```python
# NEW PIPELINE
text ‚Üí spaCy NLP ‚Üí Sentence Segmentation ‚Üí Boundary Correction ‚Üí 
       Token Analysis ‚Üí Context-Aware Cleaning ‚Üí Validated Output
```

#### Implementation Plan

1. **Load Biomedical spaCy Model**:
```python
import spacy
from scispacy.linking import EntityLinker

nlp = spacy.load("en_core_sci_lg")  # or "en_ner_bc5cdr_md" for biomedical
```

2. **Sentence Boundary Detection**:
```python
doc = nlp(text)
sentences = [sent.text for sent in doc.sents]
```

3. **Context-Aware Repair**:
- Merge fragments using dependency parsing
- Resolve abbreviations using biomedical dictionaries
- Reconstruct scientific notation contextually

4. **Validation Layer**:
- Ensure each sentence has subject-verb structure
- Verify citation integrity
- Confirm mathematical expression completeness

### Technical Debt & Lessons Learned

1. **Protection Pattern Complexity**: 30K+ unique patterns created unmaintainable regex maze
2. **Data Loss Through Deletion**: Removing entire lines for single artifacts catastrophic
3. **Order-Dependent Regex**: Sequential substitutions created race conditions
4. **Missing Integration Tests**: No validation between pipeline stages
5. **Insufficient Logging**: Multiprocessing obscured error patterns

### Migration Path

1. **Immediate**: Stop current pipeline, preserve 40.6% improved files
2. **Short-term**: Implement spaCy sentence reconstruction
3. **Medium-term**: Validate against manually annotated subset
4. **Long-term**: Integrate with SLATE for feedback loop

### Repository Structure
```
data_conversion/ukb/
‚îú‚îÄ‚îÄ 4_tagging/
‚îÇ   ‚îú‚îÄ‚îÄ 1_chunk_enhanced/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 8_md_cleaner3/  [CURRENT]
‚îÇ   ‚îî‚îÄ‚îÄ 2_spacy_sentences/   [PROPOSED]
‚îî‚îÄ‚îÄ 5_vectorization/
    ‚îî‚îÄ‚îÄ duckdb_storage/
```

---

## Appendices

### A. Critical Code Sections

**Problematic Protection (Lines 243-245)**:
```python
text = re.sub(r'\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}b\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}.\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+(?=[\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}s,;:!?)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}]\textbraceright{}\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}-]|$)', protect_pattern, text)
text = re.sub(r'\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}b\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}.?\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d*\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}s*[-‚Äì‚Äî]\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}s*\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}.?\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d*\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}b', protect_pattern, text)
text = re.sub(r'\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}b\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}.?\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d*%', protect_pattern, text)
```

**Citation Cleanup (Lines 183-190)**:
```python
line = re.sub(r'\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}[\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}[?(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+(?:[,\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}s‚Äì-]+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+)*)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}]?\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}]\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}(#page[^)]+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{})', r'[\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}1]', line)
line = re.sub(r'\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}[(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+(?:[,\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}s‚Äì-]+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+)*)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}]', r'[\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}1]', line)
line = re.sub(r'\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}[(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}]\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}([^)]+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{})[‚Äì-]+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}[(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}]\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}([^)]+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{})', r'[\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}1-\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}2]', line)
line = re.sub(r'\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}[\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}[(\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+(?:[,\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}s‚Äì-]+\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}d+)*)\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}]\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}]', r'[\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}1]', line)
```

### B. Issue Pattern Examples

**Before Cleaning**:
```
[[1](#page - 6-0)], [\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}[2\textbackslash\textbraceleft{\textbraceright{}\textbraceright{}]](#page - 7-0)
<span id="page-1-0"></span>age, sex, ethnicity...
The p-value was1.0E-8However the study...
```

**After Current Cleaning**:
```
[1], [2]
age, sex, ethnicity...
The p-value was1.0E-8However the study...  [STILL BROKEN]
```

**After Proposed spaCy**:
```
[1], [2]
age, sex, ethnicity...
The p-value was 1.0E-8. However the study...  [PROPERLY SEPARATED]
```
\end{Verbatim}
~~~







