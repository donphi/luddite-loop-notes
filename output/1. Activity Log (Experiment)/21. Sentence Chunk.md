# Sentence Chunk


## Content


### Objective


Extract and standardize sentences, tables, and equations from Markdown into JSON, preparing them for tagging/vectorization. The aim was to reduce loss and improve accuracy across iterations.







---


### Method

- **Corpus & conversion:** 6.4M Markdown → JSON.
- **Sentence extraction (iterative):** baseline splitter → **regex** → **BioBerg** → **SaT (SaT-12l-sm)** once GPU hardware allowed, giving materially better segmentation.
- **Tables:** initially preserved as Markdown; later **converted to plain text + dataframes** for better downstream model contextualization.
- **Validation:** compared parsed sentence counts to Markdown baselines each run; audited tables/equations; fixed counting + table collapse defects.





---


### Reflection (what changed and why)

- **Counting bug fixed:** early runs collapsed entire tables to a single header, depressing recall.
- **Segmentation upgrade:** **SaT-12l-sm** captured scientific boundary cases (abbreviations, citations, parentheses) with higher fidelity.
- **Tables re-handled:** conversion to **text/dataframes** meant models could parse rows/columns as real evidence instead of flat blocks.
- **Trade-offs:** most permissive config retained the most, balanced config cut noise but lost recall—both documented.





---


### Results of Improvement (sentence retention)



<table>
  <thead>
    <tr>
      <th>Run / Validation</th>
      <th>Files</th>
      <th>Original Sentences</th>
      <th>Parsed Sentences</th>
      <th>Retention %</th>
      <th>Key change</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Run 1</strong></td>
      <td>6,411</td>
      <td>3,284,031</td>
      <td>1,110,886</td>
      <td><strong>36.3%</strong></td>
      <td>Baseline; strict filters; table/header bug present</td>
    </tr>
    <tr>
      <td><strong>Run 2</strong></td>
      <td>6,411</td>
      <td>2,305,167</td>
      <td>1,326,904</td>
      <td><strong>62.2%</strong></td>
      <td>Fixes + better heuristics; partial recovery</td>
    </tr>
    <tr>
      <td><strong>Run 3</strong></td>
      <td>6,412</td>
      <td>2,305,546</td>
      <td>2,037,844</td>
      <td><strong>93.9%</strong></td>
      <td><strong>SaT-12l-sm</strong> + corrected counting + table handling</td>
    </tr>
    <tr>
      <td><strong>Run 4</strong></td>
      <td>6,377</td>
      <td>2,294,005</td>
      <td>1,247,711</td>
      <td><strong>58.1%</strong></td>
      <td>“Balanced” config; reduced noise at cost of recall</td>
    </tr>
  </tbody>
</table>



Net effect: peak +57.6 percentage points (36.3% → 93.9%). Tables and equations also recovered substantially once header-only collapses were eliminated.







---


### Pivot & Revised Hypothesis (change of direction)


Why pivot: Time-boxed. Instead of completing full RAG, sentences were used directly to match against UK Biobank feature names/synonyms, enabling a clean test of feature convergence without cross-cohort integration.



Question:



Do UK Biobank studies converge on a small, stable “core” (age, sex, BMI, BP, smoking, etc.) across diseases, or do different fields fragment into divergent subsets?


- **H0 (null):** usage is fragmented, no consistent core.
- **H1 (alt):** a statistically significant **core–periphery** structure exists: a small recurring core vs silo-specific peripherals.

Value: Tests whether Biobank science genuinely breaks silos. Systems-level, matches the sentences+tables→dataframe pipeline, avoids cross-cohort dependency.







---


### New Insight: SLATE (Sentence-Level Annotation & Tagging Engine)


Earlier chunking for tagging exposed a flaw: once multiple sentences were fused into a chunk, overlap strategies broke JSON structure (overlapping chars cut sentences mid-way, splitting context and invalidating structured JSON).



SLATE solves this by working at the sentence level, so annotation/tagging is preserved per-sentence, and then chunks are built by stitching complete sentences. That means:


- **JSON context stays intact** (per-sentence).
- **Chunk overlap happens at sentence boundaries**, not chars.
- **Vectorization pipeline improves** since context can be fused cleanly, and sentence-level annotations can simply be collated.

This was a critical design revelation. Sentence-first processing makes downstream tagging/vectorization stable, reproducible, and extensible.



---






### Technical Note: Table Handling


The custom extractor (see diary code) ensured 100% table capture, deduplication, and conversion into:


- **raw markdown** (for reproducibility),
- **plain text** (for model context),
- **structured dataframe** (for matching Biobank fields).

This meant feature mentions embedded in tables were no longer lost—boosting both recall and accuracy for tagging.


