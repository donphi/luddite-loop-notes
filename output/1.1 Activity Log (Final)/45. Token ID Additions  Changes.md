# Token ID: Additions & Changes


## Content


---


## 1. Objective and Scope


This work addressed five identified deficiencies in the document processing pipeline's token identification system:


1. **Identifier collision risk:** Line tokens and word tokens used identical naming patterns, creating ambiguity in merged datasets
2. **Type indistinguishability:** Database queries could not efficiently filter by token type without parsing record contents
3. **Suboptimal indexing:** Long string identifiers (60+ characters) reduced B-tree index efficiency
4. **Storage overhead:** Redundant document ID embedding inflated output file sizes
5. **Human verification friction:** Extended identifiers impeded rapid visual inspection during quality control





Motivating observation: These deficiencies became apparent during the layout fusion stage (G6_03), where line and word tokens must be linked to page regions. Representative identifiers from the pipeline illustrate the problem:



Line token:  101002alz13610_genetic_risk_score_for_alzheimers_2024_0000001



Word token:  101002alz13610_genetic_risk_score_for_alzheimers_2024_0000001







The identical structure created three compounding issues: 


1. Type ambiguity required additional metadata queries to distinguish lines from words during fusion operations,
2. Visual inspection during debugging could not differentiate token types without parsing,
3. Linking hundreds of word tokens per page produced disproportionately large association tables, where identifier strings dominated storage rather than the relationships themselves.





The scope encompassed five pipeline scripts responsible for generating line tokens, word tokens, table identifiers, cell identifiers, and layout region identifiers.



---


## 2. Methods and Process


### 2.1 Analysis Phase


Current identifier generation patterns across pipeline stages:




<table>
  <thead>
    <tr>
      <th>Script</th>
      <th>Entity</th>
      <th>Original Pattern</th>
      <th>Length</th>
      <th>Quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>G2_01</td>
      <td>Line token</td>
      <td><code>{doc_id}_{counter:07d}</code></td>
      <td>~65 chars</td>
      <td>❌ too long</td>
    </tr>
    <tr>
      <td>G2_02</td>
      <td>Word token</td>
      <td><code>{doc_id}_{counter:07d}</code></td>
      <td>~65 chars</td>
      <td>❌ too long</td>
    </tr>
    <tr>
      <td>G2_02</td>
      <td>Formula</td>
      <td><em><code>(undefined)</code></em></td>
      <td>-</td>
      <td>⚠️ undefined</td>
    </tr>
    <tr>
      <td>G4_01</td>
      <td>biblStruct</td>
      <td><em><code>(undefined)</code></em></td>
      <td>-</td>
      <td>⚠️ undefined</td>
    </tr>
    <tr>
      <td>G5_00</td>
      <td>Table</td>
      <td><code>{doc_id}_table_{counter:04d}</code></td>
      <td>~70 chars</td>
      <td>❌ too long</td>
    </tr>
    <tr>
      <td>G5_02</td>
      <td>Cell</td>
      <td><code>{table_id}_r{row}_c{col}</code></td>
      <td>~80 chars</td>
      <td>❌ too long</td>
    </tr>
    <tr>
      <td>G6_00</td>
      <td>Region</td>
      <td><code>p{page}_r{idx}</code></td>
      <td>~8 chars</td>
      <td>⚠️ reasonable</td>
    </tr>
  </tbody>
</table>



### 2.2 Design Principles Applied

1. **Single-character type prefix:** Enables O(1) type discrimination: eg `L1 = Line1` (the same effort to recognise whether you have 10 tokens or 10 million)
2. **Monotonic counter suffix:** Preserves ordering and uniqueness guarantees
3. **Composite key delegation:** Global uniqueness achieved via `(doc_id, token_id)` rather than embedding
4. **Position encoding where appropriate:** Cell and region IDs encode structural position: eg. `T1_R2C3 = Table 1_row2_column3` easily (human-in-the-loop debugging and lookup)

### 2.3 Implementation


Modified identifier generation in five scripts to produce compact, type-prefixed identifiers.







---


## 3. Key Technical Outcomes


### 3.1 New Identifier Schema



<table>
  <thead>
    <tr>
      <th>Prefix</th>
      <th>Entity</th>
      <th>Format</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>L</code></td>
      <td>Line token</td>
      <td><code>L{counter}</code></td>
      <td><code>L42</code></td>
    </tr>
    <tr>
      <td><code>W</code></td>
      <td>Word token</td>
      <td><code>W{counter}</code></td>
      <td><code>W999</code></td>
    </tr>
    <tr>
      <td><code>F</code></td>
      <td>Formula</td>
      <td><code>F{counter}</code></td>
      <td><code>F2</code></td>
    </tr>
    <tr>
      <td><code>T</code></td>
      <td>Table</td>
      <td><code>T{counter}</code></td>
      <td><code>T3</code></td>
    </tr>
    <tr>
      <td><code>T*_R*C*</code></td>
      <td>Cell</td>
      <td><code>T{t}_R{r}C{c}</code></td>
      <td><code>T1_R2C3</code></td>
    </tr>
    <tr>
      <td><code>R</code></td>
      <td>Region</td>
      <td><code>R{page}_{idx}</code></td>
      <td><code>R1_0</code></td>
    </tr>
    <tr>
      <td><code>B</code></td>
      <td>biblStruct</td>
      <td><code>B{index}</code></td>
      <td><code>B1</code></td>
    </tr>
  </tbody>
</table>



### 3.2 Database Query Optimisation




~~~sql
-- Type-specific queries now use index prefix scanning
SELECT * FROM tokens WHERE id LIKE 'L%';  -- All lines
SELECT * FROM tokens WHERE id LIKE 'W%';  -- All words
SELECT * FROM tokens WHERE id LIKE 'T%' AND id NOT LIKE 'T%_R%';  -- Tables only
~~~




---


## 4. Critical Reflection


### 4.1 Strengths


The refactoring achieves the primary objectives of disambiguation and compression while maintaining backward compatibility with downstream pipeline stages that consume these identifiers.



### 4.2 Human-in-the-Loop Considerations


The design explicitly prioritises human verification capability. Research demonstrates that identifier readability significantly impacts error detection rates in data pipeline quality control. 



Visual distinguishability (L42 vs W42 vs T3) reduces cognitive load during manual inspection, critical for maintaining data quality in scientific pipelines where ground truth validation remains essential.



### 4.3 Limitations

- **Migration required:** Existing outputs must be regenerated with new schema
- **Composite key dependency:** Cross-document queries require explicit `doc_id` inclusion

---


## 5. Next Steps

1. **Regenerate pipeline outputs:** Execute full pipeline run to propagate new identifier format
2. **Validate downstream compatibility:** Confirm G6_03 layout fusion and subsequent stages handle new format correctly
3. **Update database schema documentation:** Document composite key requirements for merged datasets
4. **Consider character token prefix:** If character-level tokenisation is added, reserve `C` prefix




