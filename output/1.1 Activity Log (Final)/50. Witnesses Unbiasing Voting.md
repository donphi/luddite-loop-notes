# Witnesses: Unbiasing Voting


## Content


# 1. Objective and Scope


### Problem Statement


The pipeline's original orientation detectioni n G1_01 relied on a single source: Tesseract OSD (Orientation and Script Detection). This created a fragile decision point where one model's error would propagate through the entire document processing pipeline. A mis-rotated page cannot be corrected downstream therefore tables become unreadable, reading order inverts, and OCR confidence collapses.



### Driving Question


How do we make critical binary/categorical decisions (rotate or not, which extractor to trust, pass or fail audit) robust to individual model failures without introducing arbitrary thresholds or "magic numbers" that require manual tuning?



### Scope


This investigation covers:


- Orientation detection in `G1_01` (3 sources: Tesseract, PDF metadata, PaddleOCR, )
- Witness selection in `G3_03` (comparing Marker, Docling vs Spine extraction quality)
- Audit verification in `G8` (cross-validating fused content against witnesses)





The solution must be:


1. **Deterministic** — same inputs produce same outputs
2. **Explainable** — decisions can be audited and justified
3. **Threshold-free** — no magic numbers in core logic
4. **Empirically validatable** — assumptions can be tested post-hoc

---


## 2. Methods and Process


### 2.1 Initial Approach: Weighted Voting (Rejected)


The first design attempted confidence-weighted voting:





~~~{=html}
<pre class="notion-ascii-diagram"><code>final_score[angle] = Σ (confidence_i × weight_i)</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
final_score[angle] = Σ (confidence_i × weight_i)
\end{Verbatim}
~~~




This required setting:


- Weight per source (e.g., PDF: 0.4, Paddle: 0.4, Tesseract: 0.2)
- Confidence threshold for participation
- Margin threshold for decision confidence





Problem: These are magic numbers, refer to coding principles. Where does 0.4 come from? Why not 0.35? Each threshold becomes a tuneable parameter that requires validation data to justify, data we don't have until the pipeline runs.



### 2.2 Revised Approach: Pure Majority Vote


Strip away all weights and thresholds. The only rule:


> "We rotate if and only if at least 2 of 3 sources agree on the same non-zero angle."

In code:





~~~python
from collections import Counter

def majority_vote(votes: Dict[str, T], abstain_value: T, min_agreement: int = 2):
    counts = Counter(votes.values())
    winner, winner_count = counts.most_common(1)[0]

    if winner_count >= min_agreement and winner != abstain_value:
        return winner
    return abstain_value

~~~








Properties:


- Zero tuneable parameters
- Bias toward inaction (abstain) when uncertain
- Requires independent agreement to act





### Research: Crowdsourcing Literature


The multi-annotator consensus problem is well-studied in crowdsourcing, where multiple human labellers provide noisy labels for the same item. Key question: given disagreeing annotators with unknown reliability, what is the true label?



Foundational work discovered:


1. **Dawid & Skene (1979):** "**Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm**"
    - Original paper from medical diagnosis context
    - Models each annotator as having a confusion matrix (error pattern)
    - Uses Expectation-Maximisation to jointly learn annotator reliability and true labels
    - No ground truth required

    


2. **MACE (Hovy et al., 2013):** "**Learning Whom to Trust with MACE**"
    - Extends Dawid-Skene with explicit spammer modelling
    - Outputs per-annotator competence scores
    - Provides entropy per item (flags difficult cases)

    


3. **crowd-kit (Toloka, 2024):** Modern Python implementation
    - Implements Dawid-Skene, MACE, GLAD, and other methods
    - Actively maintained, scikit-learn compatible API
    - Supports the CrowdLayer approach for soft labels





### Understanding Dawid-Skene (Plain English)






What it does:



Imagine you have 3 doctors diagnosing patients, but you don't know which doctor is most accurate. Dawid-Skene looks at thousands of diagnoses and asks: "If Doctor A says 'cancer' but the patient actually has 'benign', how often does that happen?" It builds a confusion matrix for each doctor showing their error patterns.



The key insight:



Doctors who frequently agree with each other on "easy" cases but disagree on "hard" cases reveal information about their individual biases. A doctor who always says "benign" is unreliable when they say "benign" but very informative when they say "cancer" (because they rarely say it).







How it works:


1. **E-step:** Given current estimates of each annotator's confusion matrix, compute the probability of each true label for each item
2. **M-step:** Given current estimates of true labels, update each annotator's confusion matrix
3. **Repeat** until convergence





Output:


- Per-annotator confusion matrix: P(annotator says Y | true label is X)
- Per-item posterior: P(true label = k | all votes)
- Predicted true labels





Why majority vote is a special case:



Majority voting is mathematically equivalent to Dawid-Skene when you assume all annotators have identical confusion matrices (equal reliability). It's the maximum likelihood estimate under the equal-reliability assumption.







---


## 3. Key Findings and Technical Outcomes


### Finding 1: Majority Vote is Principled, Not Naive


The decision to use majority voting is not a simplification, it's the optimal solution given our prior knowledge. Without evidence that one source is more reliable than another, treating them equally is the correct Bayesian choice. insert cit later



Implication: We can defend majority voting in the thesis as "MLE under equal-reliability assumption" rather than "simple heuristic."







### Finding 2: Dawid-Skene Requires Categorical Labels


Dawid-Skene cannot consume continuous confidence scores directly. Signals must be discretised:




<table>
  <thead>
    <tr>
      <th>Signal Type</th>
      <th>Discretisation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Rotation angle</td>
      <td>0°, 90°, 180°, 270° (already categorical)</td>
    </tr>
    <tr>
      <td>Coverage comparison</td>
      <td><code>marker</code> / <code>docling</code> / <code>tie</code> (with margin threshold)</td>
    </tr>
    <tr>
      <td>Audit match</td>
      <td><code>pass</code> / <code>fail</code> / <code>review</code> (with percentage threshold)</td>
    </tr>
  </tbody>
</table>



Implication: The discretisation thresholds (e.g., "marker wins if coverage > docling + 5%") are the only magic numbers, and they live in resources.yaml as explicit hyperparameters—not buried in code.







### Finding 3: Two-Pass Architecture


The implementation requires two passes:







Pass 1 (Learning):


- `voting_use_learned_weights: false`
- Pipeline runs with majority voting
- All vote records stored in `report.json` per stage
- Post-hoc analysis script runs Dawid-Skene on collected votes
- Outputs confusion matrices to `/analysis/voting/*.json`





Pass 2 (Application):


- `voting_use_learned_weights: true`
- Pipeline loads pre-computed confusion matrices
- Uses Bayesian posterior instead of simple counting
- Falls back to majority vote if weights file missing





### Finding 4: Output Structure


Dawid-Skene analysis produces JSON files:





~~~json
## Dawid-Skene Output Example:

{
  "meta": {
    "n_tasks": 120000,
    "n_workers": 3,
    "label_names": ["0", "90", "180", "270"]
  },
  "worker_errors": {
    "pdf": [[0.95, 0.02, 0.02, 0.01], ...],
    "paddle": [[0.91, 0.04, 0.03, 0.02], ...],
    "tesseract": [[0.78, 0.10, 0.08, 0.04], ...]
  },
  "summary": {
    "worker_accuracies": {"pdf": 0.95, "paddle": 0.91, "tesseract": 0.78},
    "majority_vote_agreement": 0.97
  }
}

~~~








### Finding 5: Quantifiable Validation


The majority_vote_agreement metric tells us whether the equal-reliability assumption held:


- **>95%:** Majority vote was essentially optimal; sources were similarly reliable
- **80-95%:** Some bias exists; learned weights provide modest improvement
- **<80%:** Significant source bias; majority vote was suboptimal





### Technical Outcome: `lib/voting.py`


A domain-agnostic voting module was implemented with:




<table>
  <thead>
    <tr>
      <th>Function</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>majority_vote()</code></td>
      <td>Pure counting, no weights</td>
    </tr>
    <tr>
      <td><code>majority_vote_gated()</code></td>
      <td>Excludes low-confidence sources (hard gate, not soft weight)</td>
    </tr>
    <tr>
      <td><code>weighted_vote()</code></td>
      <td>Bayesian posterior using learned confusion matrices</td>
    </tr>
    <tr>
      <td><code>vote()</code></td>
      <td>Unified interface that switches based on <code>use_learned_weights</code> flag</td>
    </tr>
    <tr>
      <td><code>extract_votes_for_dawid_skene()</code></td>
      <td>Generic extraction helper for post-hoc analysis</td>
    </tr>
    <tr>
      <td><code>run_dawid_skene_analysis()</code></td>
      <td>Runs crowd-kit and saves results</td>
    </tr>
  </tbody>
</table>



---






## 4. Critical Reflection


### What Worked Well

1. **Literature grounding:** The crowdsourcing literature provided a principled framework rather than ad-hoc heuristics
2. **Separation of concerns:** Voting logic is domain-agnostic; discretisation logic lives in each stage
3. **Graceful degradation:** If weights file is missing, system falls back to majority vote with a warning
4. **Full provenance:** Every vote record is stored, enabling post-hoc debugging

### Limitations

1. **Categorical only:** Cannot directly use soft confidence scores; requires discretisation
2. **Batch requirement:** Dawid-Skene needs thousands of items to learn reliable confusion matrices
3. **Stationarity assumption:** Assumes source reliability is constant across the corpus (may not hold if document quality varies)
4. **Two-pass overhead:** Must run pipeline twice to benefit from learned weights

### Assumptions Made

1. Sources are conditionally independent given the true label (i.e., errors are uncorrelated)
2. Source reliability is consistent across document types
3. Label categories are mutually exclusive and exhaustive
4. Majority vote is acceptable for Pass 1 (errors are recoverable or tolerable)

### What I Would Do Differently

1. **Consider GLAD:** Adds item difficulty modelling, which may capture that some pages are inherently harder to orient
2. **Stratified analysis:** Run Dawid-Skene separately for different document types (scanned vs native PDF)
3. **Online learning:** Investigate streaming variants that update weights incrementally

---


## 5. Next Steps


The first step is implementing lib/voting.py with the majority and weighted vote functions, then wiring G1_01 to collect all three orientation signals and route them through the unified vote() interface. Configuration keys for the two-pass system go into resources.yaml.



Once the pipeline runs on the full corpus, the post-hoc Dawid-Skene analysis will reveal whether the equal-reliability assumption held. The key number to report is majority_vote_agreement, if it's above 95%, majority voting was essentially optimal and Pass 2 adds little value. If it's lower, the confusion matrices will show which source was systematically biased and in what direction.



The same voting pattern then extends naturally to witness selection in G3 and audit verification in G8, though whether a full Pass 2 re-run is worthwhile depends on how much disagreement the analysis uncovers.







Add to refrences



## References

- Dawid, A. P., & Skene, A. M. (1979). Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm. _Journal of the Royal Statistical Society: Series C_, 28(1), 20–28.
- Hovy, D., Berg-Kirkpatrick, T., Vaswani, A., & Hovy, E. (2013). Learning Whom to Trust with MACE. _NAACL-HLT 2013_.
- Ustalov, D., Pavlichenko, N., & Tseitlin, B. (2024). Learning from Crowds with Crowd-Kit. _Journal of Open Source Software_, 9(96), 6227.
