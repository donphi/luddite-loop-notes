# PDF to JSON (Redesigned)


## Content


### Establishing a New Standard for PDF â†’ JSON

1. **Spine** - Source of Truth
2. **Validate Spine** - Validate with different libraries and models
3. **OCR Extraction** - Scan all text and tables
4. **Validate OCR Extraction** - Use multiple models to validate effective extraction
5. **Build Layout** - Use Spine and OCR Extraction to build JSON structure
6. **Validate JSON** - Accumulate reports from Steps 1-5





Establish Pros and Cons of Libraries and Models







### Issues:


### Pipeline Refactoring: Transition to Independent Verification Architecture


Challenge



During the initial processing of multi-column academic PDF files, a critical failure mode was observed in the text alignment layer. The initial optical character recognition (OCR) process (PaddleOCR/ALTO) utilised a standard top-down scanning order. Consequently, text was read strictly by vertical position (e.g., reading Line 1 of Column A immediately followed by Line 1 of Column B), effectively scrambling the semantic coherence of the "Source of Truth" (the Token Spine).



When attempting to map this scrambled spine against block-based text engines (Marker and Docling), the alignment logic failed significantly. Furthermore, attempting to align these engines before establishing valid document metadata created a circular dependency where we were verifying body text without accurately isolating the header or references.



Process


1. **Initial Attempt (Structure Injection):** To correct the reading order, I initially attempted to embed PP-StructureV3 logic directly into the geometry spine (Scripts `G2_06` and `G2_07`). This proved to be a mistake, as hard-coding layout assumptions into the geometry corrupted the raw pixel data.
2. **Architecture Pivot:** It was determined that the "Source of Truth" (G2) must remain pure geometry. Layout interpretation (G3) must be overlaid subsequently.
3. **The Metadata Pre-requisite:** We identified that Metadata extraction (GROBID) could not be a parallel or late-stage process. It had to become **Group 4**, defining the document's identity (Title, Authors, References) _before_ the body text was processed.
4. **Confirmation Layer:** Consequently, the execution of Marker and Docling was moved to **Group 5**. They are no longer treated merely as content generators, but as an **Independent Confirmation Layer**. Their role is to verify the Spine's integrity with a strict >90% character-match threshold before any data fusion occurs.

Technical



The pipeline was re-engineered into a strict hierarchy of truth. Following the pure geometry generation (G2) and layout analysis (G3), the pipeline now executes a dedicated Metadata stage (G4) using GROBID to isolate bibliographic data. Only once the document structure and metadata are established do we invoke the text engines (G5).



This allows for a "Verification-First" logic: The pipeline now demands that two independent sources (Marker and Docling) corroborate the OCR spine. If the character match rate falls below 90%, the page is flagged for manual review or rejection before it reaches the final fusion and binding stage (G6).



Architectural Comparison: Before vs. After Refactoring




<table>
  <tbody>
    <tr>
      <td><strong>Stage</strong></td>
      <td><strong>Previous Architecture (Premature Extraction)</strong></td>
      <td><strong>Refactored Architecture (Sequential Verification)</strong></td>
    </tr>
    <tr>
      <td><strong>G2: Truth</strong></td>
      <td><strong>Hybrid Spine:</strong> Mixed geometry with layout IDs, resulting in a mutable and potentially biased spine.</td>
      <td><strong>Pure Geometry:</strong> Immutable x/y coordinate sorting. Layout opinions are strictly excluded from the raw data.</td>
    </tr>
    <tr>
      <td><strong>G3: Layout</strong></td>
      <td><strong>Missing/Implicit:</strong> Layout logic was hidden inside the spine builder or alignment scripts.</td>
      <td><strong>Explicit Layout Map:</strong> Dedicated stage (LayoutLMv3 + PP-Structure) to define &quot;Containers&quot; (Columns, Tables) before text is poured in.</td>
    </tr>
    <tr>
      <td><strong>G4: Metadata</strong></td>
      <td><strong>Late Binding:</strong> GROBID ran in parallel to text engines, causing confusion between headers and body text.</td>
      <td><strong>Metadata Pre-requisite:</strong> GROBID runs <em>before</em> text verification. Ensures titles/authors are excluded from body text matching.</td>
    </tr>
    <tr>
      <td><strong>G5: Verification</strong></td>
      <td><strong>Content Extraction:</strong> Marker/Docling were used to &quot;get text.&quot; Alignment was attempted blindly.</td>
      <td><strong>Confirmation Layer:</strong> Marker/Docling act as independent auditors. They must verify &gt;90% of the spine text to confirm OCR accuracy.</td>
    </tr>
    <tr>
      <td><strong>G6: Fusion</strong></td>
      <td><strong>Simple Stitching:</strong> Merged text based on rough overlap.</td>
      <td><strong>Validated Binding:</strong> Fuses only verified text into verified layout containers.</td>
    </tr>
  </tbody>
</table>



Reflection



The initial failure highlighted that "extracting text" and "verifying text" are distinct operations. By treating Marker and Docling initially as simple data sources, we missed their value as validational tools. The reordering to place Metadata (G4) before Verification (G5) ensures that we are not wasting computational resources verifying metadata fields as body text. This rigorous, step-wise verification process significantly increases confidence in the final dataset, as every paragraph is effectively "peer-reviewed" by three distinct AI models (PaddleOCR, Marker, Docling) before acceptance.







Next Step



Proceed to implementing the Group 6 Fusion logic, which will now bind the validated text streams into the confirmed layout structure.




Considering using surya for the bounding boxes of the spine






