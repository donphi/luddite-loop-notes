# Pipeline 3 Production Run


## Content


## 1. Objective and Scope


Pipeline 3, for PDF-to-JSON conversion, worked flawlessly on a single RTX 6000 GPU with 10-20 PDFs. When scaling to Lambda Labs B200 Virtual Machine 8xGPUs to process the final +-6,600 PDFs, severe performance degradation occurred: CPU overcommit 90% utliization, GPU starvation (not enough throughput), queues extending on certain workers, causing stale and idle containers for other GPU workers.


- The solution was a systematic analysis of threading, worker scaling, CPU parallelism caps, and crash recovery mechanisms.
- Five route causes were identified that needed to be fixed in the failed Production Run.





---


## 2. Hardware Configuration


### 2.1 Lambda Labs Instances Used



<table>
  <thead>
    <tr>
      <th><strong>Phase</strong></th>
      <th><strong>Instance</strong></th>
      <th><strong>GPUs</strong></th>
      <th><strong>VRAM</strong></th>
      <th><strong>CPU Cores</strong></th>
      <th><strong>RAM</strong></th>
      <th><strong>Cost/hr</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Test Run</strong></td>
      <td>B200 2Ã—</td>
      <td>2</td>
      <td>360 GB</td>
      <td>52</td>
      <td>256 GB</td>
      <td>+-$10/hr</td>
    </tr>
    <tr>
      <td><strong>Production</strong></td>
      <td>B200 8Ã—</td>
      <td>8</td>
      <td>1440 GB</td>
      <td>204</td>
      <td>1024 GB</td>
      <td>+-$40/hr</td>
    </tr>
  </tbody>
</table>



### 2.2 Container Architecture




~~~{=html}
<pre class="notion-ascii-diagram"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
<strong>â”‚                         LAMBDA B200 8Ã— GPU                             â”‚</strong>
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚<strong>  RabbitMQ  (Dispatch &amp; Workers)  </strong>                               â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ <strong>pdf_job_queue</strong> (orchestrator jobs)                          â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ <strong>spine_jobs</strong> (G2_00 GPU work)                                â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ <strong>mineru_jobs</strong> (G2_03 GPU work)                               â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ marker_jobs / docling_jobs / paddle_jobs (G3_xx)           â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ <strong>tatr_detect_jobs / tatr_structure_jobs</strong> (G5_xx)             â”‚   â”‚
â”‚  â”‚  â””â”€â”€ <strong>ppstructure_jobs / layoutlm_jobs</strong> (G6_xx)                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ <strong>Orchestrator</strong>   â”‚    â”‚ <strong>Orchestrator </strong>  â”‚    â”‚ <strong>Orchestrator</strong>   â”‚  <strong>Ã—80</strong>   â”‚
â”‚  â”‚ (CPU-bound)    â”‚â”€â”€â”€â”€â”‚ (CPU-bound)    â”‚â”€â”€â”€â”€â”‚ (CPU-bound)    â”‚        â”‚
â”‚  â”‚ pdf_job_queue  â”‚    â”‚ pdf_job_queue  â”‚    â”‚ pdf_job_queue  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           â”‚                    â”‚                    â”‚                  â”‚
â”‚           â–¼                    â–¼                    â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                   <strong>  GPU Worker Pool  </strong>                          â”‚    â”‚
â”‚  â”‚  <strong>spine-worker Ã—110</strong>  |  <strong>mineru Ã—16</strong>  |  <strong>tatr Ã—16</strong>  | <strong>ppstructure</strong>  â”‚    â”‚
â”‚  â”‚        GPU 0-7      |    GPU 0-7   |   GPU 0-7  |   GPU 0-7    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
\textbf{â”‚                         LAMBDA B200 8Ã— GPU                             â”‚}
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚\textbf{  RabbitMQ  (Dispatch & Workers)  }                               â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ \textbf{pdf_job_queue} (orchestrator jobs)                          â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ \textbf{spine_jobs} (G2_00 GPU work)                                â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ \textbf{mineru_jobs} (G2_03 GPU work)                               â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ marker_jobs / docling_jobs / paddle_jobs (G3_xx)           â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ \textbf{tatr_detect_jobs / tatr_structure_jobs} (G5_xx)             â”‚   â”‚
â”‚  â”‚  â””â”€â”€ \textbf{ppstructure_jobs / layoutlm_jobs} (G6_xx)                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ \textbf{Orchestrator}   â”‚    â”‚ \textbf{Orchestrator }  â”‚    â”‚ \textbf{Orchestrator}   â”‚  \textbf{Ã—80}   â”‚
â”‚  â”‚ (CPU-bound)    â”‚â”€â”€â”€â”€â”‚ (CPU-bound)    â”‚â”€â”€â”€â”€â”‚ (CPU-bound)    â”‚        â”‚
â”‚  â”‚ pdf_job_queue  â”‚    â”‚ pdf_job_queue  â”‚    â”‚ pdf_job_queue  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           â”‚                    â”‚                    â”‚                  â”‚
â”‚           â–¼                    â–¼                    â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                   \textbf{  GPU Worker Pool  }                          â”‚    â”‚
â”‚  â”‚  \textbf{spine-worker Ã—110}  |  \textbf{mineru Ã—16}  |  \textbf{tatr Ã—16}  | \textbf{ppstructure}  â”‚    â”‚
â”‚  â”‚        GPU 0-7      |    GPU 0-7   |   GPU 0-7  |   GPU 0-7    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

\end{Verbatim}
~~~




---


## 3. Timeline of Issues and Fixes


### 3.1 Phase 1: 2-GPU Test Run (Dec 23)






Configuration:


- 20 orchestrator containers
- +-25 spine-workers





ğŸ’¡ Very important efficiency issue


- Models re-downloading per container
- No PNG prefetch threading





Observations:





~~~{=html}
<pre class="notion-ascii-diagram"><code>âŒÂ ğŸ¥µÂ <strong>CPU:</strong> 90% constant (unhealthy)
âŒÂ ğŸ˜´Â <strong>GPU:</strong> +-30% utilization (underutilized)
âŒÂ ğŸ¢Â <strong>Throughput: +-</strong>100 PDFs/hour (slow)</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
âŒÂ ğŸ¥µÂ \textbf{CPU:} 90% constant (unhealthy)
âŒÂ ğŸ˜´Â \textbf{GPU:} +-30% utilization (underutilized)
âŒÂ ğŸ¢Â \textbf{Throughput: +-}100 PDFs/hour (slow)
\end{Verbatim}
~~~








Root cause identified: 


- No CPU/GPU overlap.
- GPU workers waited for PNG decode to complete before inference could begin.





### 3.2 Phase 2: Threading Patch (Dec 25)


Patch: Introduced PNG prefetch threading to G2_00_spine_worker:





~~~{=html}
<pre class="notion-ascii-diagram"><code>Python

<strong># BEFORE: Sequential decode â†’ GPU inference</strong>
for batch in batches:
    images = load_pngs(batch)    <strong># CPU blocks</strong>
    results = surya_ocr(images)  <strong># GPU waits</strong>

<strong># AFTER: Overlap CPU decode with GPU inference</strong>
with ThreadPoolExecutor(max_workers=decode_thread_workers) as pool:
    next_future = pool.submit(load_pngs, next_batch)

    for batch in batches:
        images = next_future.result()                     <strong># Wait for prefetched</strong>
        next_future = pool.submit(load_pngs, next_batch)  <strong># Start next</strong>
        results = surya_ocr(images)                       <strong># GPU runs while CPU decodes</strong>
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
Python

\textbf{# BEFORE: Sequential decode â†’ GPU inference}
for batch in batches:
    images = load_pngs(batch)    \textbf{# CPU blocks}
    results = surya_ocr(images)  \textbf{# GPU waits}

\textbf{# AFTER: Overlap CPU decode with GPU inference}
with ThreadPoolExecutor(max_workers=decode_thread_workers) as pool:
    next_future = pool.submit(load_pngs, next_batch)

    for batch in batches:
        images = next_future.result()                     \textbf{# Wait for prefetched}
        next_future = pool.submit(load_pngs, next_batch)  \textbf{# Start next}
        results = surya_ocr(images)                       \textbf{# GPU runs while CPU decodes}

\end{Verbatim}
~~~




Impact:





~~~{=html}
<pre class="notion-ascii-diagram"><code>âŒÂ ğŸ¥µÂ <strong>CPU:</strong> 90% constant (unhealthy)
âœ…Â âš¡Â <strong>GPU:</strong> 96% utilisation: 
âœ…Â ğŸ‡Â <strong>Throughput:</strong> 330 PDFs/hr (3.3Ã— improvement)</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
âŒÂ ğŸ¥µÂ \textbf{CPU:} 90% constant (unhealthy)
âœ…Â âš¡Â \textbf{GPU:} 96% utilisation: 
âœ…Â ğŸ‡Â \textbf{Throughput:} 330 PDFs/hr (3.3Ã— improvement)
\end{Verbatim}
~~~








### 3.3 Phase 3: 8-GPU Production Run (Dec 25-26)


Initial configuration:


- 80 orchestrator containers
- 110 spine-workers (most important GPU workers)
- `decode_thread_workers: 4` (4 threads per worker)

Disaster timeline:




<table>
  <thead>
    <tr>
      <th><strong>Time</strong></th>
      <th><strong>Orchestrators</strong></th>
      <th><strong>Spine Queue</strong></th>
      <th><strong>PDFs Done</strong></th>
      <th><strong>Status</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>T+0h</td>
      <td>80</td>
      <td>0 deep</td>
      <td>0</td>
      <td>Queue exploding</td>
    </tr>
    <tr>
      <td>T+1h</td>
      <td>80</td>
      <td>0 deep</td>
      <td>100</td>
      <td>Stabilizing</td>
    </tr>
    <tr>
      <td>T+2h</td>
      <td>80</td>
      <td>20 deep</td>
      <td>600</td>
      <td>Stable</td>
    </tr>
    <tr>
      <td>T+3h</td>
      <td>44 (reduced 36)</td>
      <td>80 deep</td>
      <td>1,000</td>
      <td>Uneasy</td>
    </tr>
    <tr>
      <td>T+8h (wake)</td>
      <td>44</td>
      <td><strong>315 deep</strong></td>
      <td>1,400</td>
      <td><strong>DISASTER</strong></td>
    </tr>
  </tbody>
</table>







---


## 4. Root Cause: CPU Overcommit


The B200 has 204 ğŸ§ CPU cores. These are separate from the GPU



### 4.1 Two Sources of CPU Overload


We had two different problems happening at the same time:



### ğŸ® Problem 1: GPU Workers Spawn Helper Threads


Each GPU worker creates a ThreadPoolExecutor with 4 threads to load PNGs while the GPU runs.



But only 1 thread is ever used at a timeâ€”the other 3 are wasted.




<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Calculation</th>
      <th>Threads</th>
      <th>Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Spine workers</td>
      <td>110 Ã— 4 threads</td>
      <td>440</td>
      <td>ğŸ§  CPU</td>
    </tr>
    <tr>
      <td>Other GPU workers</td>
      <td>~50 Ã— 4 threads</td>
      <td>200</td>
      <td>ğŸ§  CPU</td>
    </tr>
    <tr>
      <td><strong>Problem 1 Total</strong></td>
      <td></td>
      <td><strong>640</strong></td>
      <td></td>
    </tr>
  </tbody>
</table>



### âš™ï¸ Problem 2: CPU Stages Spawn Parallel Processes


Each orchestrator runs CPU stages (G1_00, G2_01, G4_01, etc.).



Each stage uses parallel_map() with workers=16 from config.



Multiple stages can run at the same time per orchestrator.




<table>
  <thead>
    <tr>
      <th><strong>Component</strong></th>
      <th><strong>Calculation</strong></th>
      <th><strong>Processes</strong></th>
      <th><strong>Type</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Orchestrators</td>
      <td>80 Ã— 2 stages Ã— 16 workers</td>
      <td>2,560</td>
      <td>ğŸ§  CPU</td>
    </tr>
    <tr>
      <td><strong>Problem 2 Total</strong></td>
      <td></td>
      <td><strong>2,560</strong></td>
      <td></td>
    </tr>
  </tbody>
</table>



### 4.2 Combined Impact



<table>
  <thead>
    <tr>
      <th><strong>Source</strong></th>
      <th><strong>Count</strong></th>
      <th><strong>Type</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ğŸ® Problem 1: GPU worker threads</td>
      <td>640</td>
      <td>ğŸ§  CPU</td>
    </tr>
    <tr>
      <td>âš™ï¸ Problem 2: CPU stage processes</td>
      <td>2,560</td>
      <td>ğŸ§  CPU</td>
    </tr>
    <tr>
      <td><strong>TOTAL</strong></td>
      <td><strong>3,200</strong></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>Hardware available</strong></td>
      <td><strong>204 cores</strong></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>Overcommit ratio</strong></td>
      <td><strong>15.7Ã—</strong></td>
      <td>âš ï¸ CPU</td>
    </tr>
  </tbody>
</table>


> ğŸ’¥ Result: CPU spends all its time context-switching between threads instead of doing work. GPUs starve waiting for data.





### 4.3 Orchestrator/Worker Ratio Imbalance




~~~{=html}
<pre class="notion-ascii-diagram"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               <strong>     QUEUE EXPLOSION DIAGRAM    </strong>                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚               44 orchestrators producing jobs                   â”‚
â”‚                            â”‚                                    â”‚
â”‚                            â–¼                                    â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚           â”‚        <strong>spine_jobs queue </strong>            â”‚               â”‚
â”‚           â”‚                                     â”‚               â”‚
â”‚           â”‚  <strong>Queue:   â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ </strong>215<strong>  â”‚ â—„â”€â”€ </strong>WAITING<strong>   â”‚
â”‚           â”‚  Workers:</strong> â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 110            â”‚ â—„â”€â”€ BUSY      â”‚
â”‚           â”‚                                     â”‚               â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                            â”‚                                    â”‚
â”‚                            â–¼                                    â”‚
â”‚       âš ï¸ <strong>110 spine-workers</strong> (all busy, can&#39;t keep up)            â”‚
â”‚                                                                 â”‚
â”‚       <strong>Meanwhile:</strong> paddle, mineru, tatr workers = ğŸ’¤ IDLE         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               \textbf{     QUEUE EXPLOSION DIAGRAM    }                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚               44 orchestrators producing jobs                   â”‚
â”‚                            â”‚                                    â”‚
â”‚                            â–¼                                    â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚           â”‚        \textbf{spine_jobs queue }            â”‚               â”‚
â”‚           â”‚                                     â”‚               â”‚
â”‚           â”‚  \textbf{Queue:   â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ }215\textbf{  â”‚ â—„â”€â”€ }WAITING\textbf{   â”‚
â”‚           â”‚  Workers:} â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 110            â”‚ â—„â”€â”€ BUSY      â”‚
â”‚           â”‚                                     â”‚               â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                            â”‚                                    â”‚
â”‚                            â–¼                                    â”‚
â”‚       âš ï¸ \textbf{110 spine-workers} (all busy, can't keep up)            â”‚
â”‚                                                                 â”‚
â”‚       \textbf{Meanwhile:} paddle, mineru, tatr workers = ğŸ’¤ IDLE         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\end{Verbatim}
~~~








Cause: Threading made orchestrators produce jobs 3Ã— faster than workers could consume.



Fix: Reduce orchestrators per GPU from 10 â†’ 5:





~~~{=html}
<pre class="notion-ascii-diagram"><code>Yaml

<strong># scaling.yaml</strong>
orchestrators_per_gpu: 5  <strong># Was 10</strong></code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
Yaml

\textbf{# scaling.yaml}
orchestrators_per_gpu: 5  \textbf{# Was 10}
\end{Verbatim}
~~~




New ratio: 40 orchestrators â†’ 110 spine-workers = 2.75 workers per orchestrator (healthy).



### 4.4 Lost Work on Container Crash âŒ


When containers died mid-processing (OOM, manual restart), documents "in flight" were lost because:


1. RabbitMQ message already ACKed (removed from queue)
2. No external checkpoint tracking in-progress docs
3. `make run-4gpu` restarts from scratch

Fix: Implemented checkpoint system (lib/checkpoint.py):





~~~{=html}
<pre class="notion-ascii-diagram"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
<strong>â”‚                        CHECKPOINT SYSTEM                        â”‚</strong>
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  output/.pipeline_checkpoint.json                               â”‚
â”‚  {                                                              â”‚
â”‚    &quot;docs&quot;: {                                                    â”‚
â”‚      &quot;doc_001&quot;: {&quot;status&quot;: &quot;success&quot;, ...},                     â”‚
â”‚      &quot;doc_002&quot;: {&quot;status&quot;: &quot;in_progress&quot;, ...},  <strong>â—„â”€â”€ Orphaned</strong>   â”‚
â”‚      &quot;doc_003&quot;: {&quot;status&quot;: &quot;failed&quot;, ...}                       â”‚
â”‚    }                                                            â”‚
â”‚  }                                                              â”‚
â”‚                                                                 â”‚
â”‚ <strong> On restart: </strong>                                                   â”‚
â”‚  $ python scripts/resume_incomplete.py                          â”‚
â”‚  â†’ Finds in_progress docs                                       â”‚
â”‚  â†’ Re-queues to RabbitMQ                                        â”‚
â”‚  â†’ Clears checkpoint entries                                    â”‚
â”‚  â†’ Restart pipeline: make run GPU_COUNT=8                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
\textbf{â”‚                        CHECKPOINT SYSTEM                        â”‚}
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  output/.pipeline_checkpoint.json                               â”‚
â”‚  \textbraceleft{\textbraceright{}                                                              â”‚
â”‚    "docs": \textbraceleft{\textbraceright{}                                                    â”‚
â”‚      "doc_001": \textbraceleft{\textbraceright{}"status": "success", ...\textbraceright{},                     â”‚
â”‚      "doc_002": \textbraceleft{\textbraceright{}"status": "in_progress", ...\textbraceright{},  \textbf{â—„â”€â”€ Orphaned}   â”‚
â”‚      "doc_003": \textbraceleft{\textbraceright{}"status": "failed", ...\textbraceright{}                       â”‚
â”‚    \textbraceright{}                                                            â”‚
â”‚  \textbraceright{}                                                              â”‚
â”‚                                                                 â”‚
â”‚ \textbf{ On restart: }                                                   â”‚
â”‚  $ python scripts/resume_incomplete.py                          â”‚
â”‚  â†’ Finds in_progress docs                                       â”‚
â”‚  â†’ Re-queues to RabbitMQ                                        â”‚
â”‚  â†’ Clears checkpoint entries                                    â”‚
â”‚  â†’ Restart pipeline: make run GPU_COUNT=8                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

\end{Verbatim}
~~~




### 4.5 G4_02 Aspect Ratio Mismatch âš ï¸


Error: Page 9 aspect ratio mismatch: GROBID=0.761, Surya=1.314



Root cause: PyMuPDF's page.set_rotation(angle) only sets the PDF /Rotate display flagâ€”it does NOT physically alter the MediaBox dimensions.





~~~{=html}
<pre class="notion-ascii-diagram"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                <strong>   PDF ROTATION BEHAVIOR      </strong>              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  <strong>G1_00:</strong> Renders PNG from PDF                               â”‚
â”‚         â””â”€â”€ PNG dimensions: 595Ã—842 (portrait)             â”‚
â”‚                                                            â”‚
â”‚  <strong>G1_02:</strong> PyMuPDF reads PNG                                  â”‚
â”‚					â”œâ”€â”€ /rotate flag: 90Â°		                           â”‚
â”‚					â”œâ”€â”€	page.set_rotation(90)                          â”‚
â”‚         â””â”€â”€ PDF MediaBox: 842Ã—595 (landscape)              â”‚
â”‚                                                            â”‚
â”‚  <strong>G2_00:</strong> Surya processes PNG                                â”‚
â”‚         â””â”€â”€ Reports: 842Ã—595 (landscape) â—„â”€â”€ CORRECT       â”‚
â”‚                                                            â”‚
â”‚                          BUT                               â”‚
â”‚                                                            â”‚
â”‚  <strong>G4_00:</strong> GROBID OCR reads PDF MediaBox                      â”‚
â”‚         â””â”€â”€ Reports: 595Ã—842 (portrait) â—„â”€â”€ âš ï¸Â WRONG       â”‚                                                               
â”‚                                                            â”‚
â”‚  <strong>G4_02:</strong> Tries to join GROBID + Surya                       â”‚
â”‚         â””â”€â”€ Aspect ratio mismatch! 0.761 â‰  1.314           â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                \textbf{   PDF ROTATION BEHAVIOR      }              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  \textbf{G1_00:} Renders PNG from PDF                               â”‚
â”‚         â””â”€â”€ PNG dimensions: 595Ã—842 (portrait)             â”‚
â”‚                                                            â”‚
â”‚  \textbf{G1_02:} PyMuPDF reads PNG                                  â”‚
â”‚					â”œâ”€â”€ /rotate flag: 90Â°		                           â”‚
â”‚					â”œâ”€â”€	page.set_rotation(90)                          â”‚
â”‚         â””â”€â”€ PDF MediaBox: 842Ã—595 (landscape)              â”‚
â”‚                                                            â”‚
â”‚  \textbf{G2_00:} Surya processes PNG                                â”‚
â”‚         â””â”€â”€ Reports: 842Ã—595 (landscape) â—„â”€â”€ CORRECT       â”‚
â”‚                                                            â”‚
â”‚                          BUT                               â”‚
â”‚                                                            â”‚
â”‚  \textbf{G4_00:} GROBID OCR reads PDF MediaBox                      â”‚
â”‚         â””â”€â”€ Reports: 595Ã—842 (portrait) â—„â”€â”€ âš ï¸Â WRONG       â”‚                                                               
â”‚                                                            â”‚
â”‚  \textbf{G4_02:} Tries to join GROBID + Surya                       â”‚
â”‚         â””â”€â”€ Aspect ratio mismatch! 0.761 â‰  1.314           â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

\end{Verbatim}
~~~




Workaround applied: _coerce_surya_page_dims_for_rotation() in G4_02 detects and swaps dimensions.



Proper fix (future): Modify G1_02 to perform physical rotation (re-render PDF content) instead of just setting the display flag.



---


## 5. Key Findings Summary



<table>
  <thead>
    <tr>
      <th><strong>Issue</strong></th>
      <th><strong>Severity</strong></th>
      <th><strong>Root Cause</strong></th>
      <th><strong>Fix Applied</strong></th>
      <th><strong>Status</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CPU overcommit</td>
      <td>âŒ Critical</td>
      <td>2,560 processes vs 204 cores</td>
      <td><code>PARALLEL_MAP_MAX_WORKERS=4</code></td>
      <td>âœ… Fixed</td>
    </tr>
    <tr>
      <td>Thread waste</td>
      <td>âš ï¸ Medium</td>
      <td>4 threads, only 1 used</td>
      <td><code>decode_thread_workers: 1</code></td>
      <td>âœ… Fixed</td>
    </tr>
    <tr>
      <td>Queue explosion</td>
      <td>âŒ Critical</td>
      <td>Too many orchestrators</td>
      <td><code>orchestrators_per_gpu: 5</code></td>
      <td>âœ… Fixed</td>
    </tr>
    <tr>
      <td>Lost work on crash</td>
      <td>âŒ Critical</td>
      <td>No checkpoint system</td>
      <td><code>lib/checkpoint.py</code> + resume script</td>
      <td>âœ… Fixed</td>
    </tr>
    <tr>
      <td>Aspect ratio mismatch</td>
      <td>âš ï¸ Medium</td>
      <td>set_rotation() vs MediaBox</td>
      <td>Workaround in G4_02</td>
      <td>âš ï¸ Workaround</td>
    </tr>
    <tr>
      <td>Model re-download</td>
      <td>âš ï¸ Medium</td>
      <td>No cache volume mounts</td>
      <td>Added <code>/root/.cache</code> mounts</td>
      <td>âœ… Fixed</td>
    </tr>
  </tbody>
</table>



---


## 6. Critical Reflection

1. **Threading is a double-edged sword.**

    The PNG prefetch threading in patch 005 delivered a 3.3Ã— throughput improvement by overlapping CPU decode with GPU inference. However, when scaled to 110+ workers, the thread pool overhead combined with CPU parallelism created catastrophic overcommit. The lesson: always pair threading optimizations with hard caps (PARALLEL_MAP_MAX_WORKERS) to prevent runaway scaling.
    

2. **RabbitMQ queues are symptoms, not causes.**

    A 315-deep spine queue wasn't a RabbitMQ problemâ€”it was a producer/consumer imbalance. The orchestrators, now 3Ã— faster due to threading, overwhelmed the GPU workers. Queue depth monitoring is essential, but the fix is always on the producer side (reduce orchestrators) or consumer side (add workers), never RabbitMQ tuning.
    

3. **Container crashes are inevitable at scale; recovery must be designed in.**

    With 150+ containers running for hours, OOM kills and transient failures are guaranteed. The checkpoint system (lib/checkpoint.py) was added reactively after losing work. Future pipelines should include checkpointing from day one, treating it as infrastructure, not an afterthought.
    

4. **PyMuPDF's rotation semantics are non-obvious.**

    The assumption that page.set_rotation() physically rotates the page is incorrectâ€”it only sets a display hint. GROBID reads raw PDF geometry, not rendered geometry. This semantic gap caused a silent failure that only surfaced when G4_02's strict validation caught the mismatch. The workaround is correct but adds cognitive load; a proper fix would modify G1_02 to perform physical rotation.
    


---


## 7. Recommended Configuration


### 7.1 Production Settings (B200 8Ã— GPU)




~~~{=html}
<pre class="notion-ascii-diagram"><code>Yaml

<strong># config.yaml</strong>
<strong># hardware</strong>
resources:
	b200_per_gpu:                <strong># resource type </strong>
		vram: 180                  <strong># gigabyte v-ram per GPU</strong> 
		cores: 26                  <strong># cpu cores per GPU</strong>
	
<strong># scaling</strong>
orchestrators_per_gpu: 5       <strong># 40 total (was 10 â†’ 80 total)</strong>

workers:
  spine-worker:    { ratio: 12 }  <strong># 96 total</strong>
  mineru-worker:   { ratio: 2 }   <strong># 16 total</strong>
  marker-worker:   { ratio: 2 }   <strong># 16 total</strong>
  tatr-worker:     { ratio: 2 }   <strong># 16 total</strong>
  ppstructure:     { ratio: 2 }   <strong># 16 total</strong></code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
Yaml

\textbf{# config.yaml}
\textbf{# hardware}
resources:
	b200_per_gpu:                \textbf{# resource type }
		vram: 180                  \textbf{# gigabyte v-ram per GPU} 
		cores: 26                  \textbf{# cpu cores per GPU}
	
\textbf{# scaling}
orchestrators_per_gpu: 5       \textbf{# 40 total (was 10 â†’ 80 total)}

workers:
  spine-worker:    \textbraceleft{\textbraceright{} ratio: 12 \textbraceright{}  \textbf{# 96 total}
  mineru-worker:   \textbraceleft{\textbraceright{} ratio: 2 \textbraceright{}   \textbf{# 16 total}
  marker-worker:   \textbraceleft{\textbraceright{} ratio: 2 \textbraceright{}   \textbf{# 16 total}
  tatr-worker:     \textbraceleft{\textbraceright{} ratio: 2 \textbraceright{}   \textbf{# 16 total}
  ppstructure:     \textbraceleft{\textbraceright{} ratio: 2 \textbraceright{}   \textbf{# 16 total}
\end{Verbatim}
~~~






~~~{=html}
<pre class="notion-ascii-diagram"><code><strong># resources.b200.yaml</strong>
G2_00:
  decode_prefetch_batches: 1
  decode_thread_workers: 1      <strong># was 4</strong>
  page_batch_size: 16</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
\textbf{# resources.b200.yaml}
G2_00:
  decode_prefetch_batches: 1
  decode_thread_workers: 1      \textbf{# was 4}
  page_batch_size: 16
\end{Verbatim}
~~~






~~~{=html}
<pre class="notion-ascii-diagram"><code><strong># docker-compose.base.yaml (environment)</strong>
PARALLEL_MAP_MAX_WORKERS: 4     <strong># Hard cap on CPU parallelism</strong>
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
\textbf{# docker-compose.base.yaml (environment)}
PARALLEL_MAP_MAX_WORKERS: 4     \textbf{# Hard cap on CPU parallelism}

\end{Verbatim}
~~~




### 7.2 Expected Performance



<table>
  <thead>
    <tr>
      <th><strong>Metric</strong></th>
      <th><strong>Before Fix</strong></th>
      <th><strong>After Fix</strong></th>
      <th><strong>Improvement</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Throughput</td>
      <td>30 PDFs/hr</td>
      <td>330 PDFs/hr</td>
      <td>11Ã—</td>
    </tr>
    <tr>
      <td>CPU usage</td>
      <td>99% (thrashing)</td>
      <td>70-80% (healthy)</td>
      <td>Stable</td>
    </tr>
    <tr>
      <td>GPU usage</td>
      <td>96% (starved)</td>
      <td>96% (fed)</td>
      <td>Same but working</td>
    </tr>
    <tr>
      <td>Queue depth</td>
      <td>315+</td>
      <td>&lt;20</td>
      <td>Stable</td>
    </tr>
    <tr>
      <td>Crash recovery</td>
      <td>None</td>
      <td>Checkpoint + resume</td>
      <td>Robust</td>
    </tr>
  </tbody>
</table>



---


## 8. Next Steps


### 8.1 Immediate Priorities

1. **Test checkpoint resume:** Run `resume_incomplete.py --dry-run` to verify detection logic before production.
2. **Validate PARALLEL_MAP_MAX_WORKERS:** Monitor CPU with `htop` during first 100 PDFs to confirm no thrashing.
3. **Verify queue steady-state:** Spine queue should stabilize at <30 with 5 orchestrators per GPU.
