# Pipeline 3 Test Run


## Content


## 1. Objective and Scope


### 1.1 The Challenge


When scaling from a single-GPU RTX workstation to a DGX B200 (2Ã— 180GB VRAM, 104 CPU cores), the pipeline required significant infrastructure adaptation:




<table>
  <thead>
    <tr>
      <th><strong>Challenge</strong></th>
      <th><strong>Single GPU</strong></th>
      <th><strong>B200 Scale</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Containers running</strong></td>
      <td>~15</td>
      <td>75-297</td>
    </tr>
    <tr>
      <td><strong>GPU workers per stage</strong></td>
      <td>1</td>
      <td>2-16 (per GPU)</td>
    </tr>
    <tr>
      <td><strong>Orchestrators</strong></td>
      <td>3</td>
      <td>14-56 (scaled)</td>
    </tr>
    <tr>
      <td><strong>Job throughput</strong></td>
      <td>Sequential</td>
      <td>Massively parallel</td>
    </tr>
    <tr>
      <td><strong>Failure detection</strong></td>
      <td>Watch single log</td>
      <td>Need centralized monitoring</td>
    </tr>
    <tr>
      <td><strong>Connection handling</strong></td>
      <td>Simple</td>
      <td>Long jobs (30+ min) timeout</td>
    </tr>
  </tbody>
</table>



The Problem: At scale, several assumptions from single-GPU development broke down:


- Workers processing 30+ minute jobs exceeded RabbitMQ's consumer timeout
- No visibility into which of 100+ containers was stuck
- Dispatch waited 30 minutes before detecting worker failures
- SSH disconnects killed backup processes





### 1.2 The Implementation




~~~{=html}
<pre class="notion-ascii-diagram"><code><strong>**BEFORE SCALING:                          AFTER SCALING:**
</strong>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
<strong>â”‚  Single GPU Pipeline        â”‚           â”‚  Multi-GPU B200 Pipeline        â”‚
</strong>â”‚                             â”‚           â”‚                                 â”‚
â”‚  15 containers              â”‚           â”‚  75-297 containers              â”‚
â”‚  3 orchestrator             â”‚           â”‚  14-56 orchestrators            â”‚
â”‚  No health monitoring       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  Real-time health dashboard     â”‚
â”‚  30min timeout on failure   â”‚           â”‚  Fail-fast sentinel system      â”‚
â”‚  Fixed worker count         â”‚           â”‚  Docker-based B2 sync           â”‚
â”‚                             â”‚           â”‚  Hot-scaling (no restart)       â”‚
â”‚                             â”‚           â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
\textbf{**BEFORE SCALING:                          AFTER SCALING:**
}â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
\textbf{â”‚  Single GPU Pipeline        â”‚           â”‚  Multi-GPU B200 Pipeline        â”‚
}â”‚                             â”‚           â”‚                                 â”‚
â”‚  15 containers              â”‚           â”‚  75-297 containers              â”‚
â”‚  3 orchestrator             â”‚           â”‚  14-56 orchestrators            â”‚
â”‚  No health monitoring       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  Real-time health dashboard     â”‚
â”‚  30min timeout on failure   â”‚           â”‚  Fail-fast sentinel system      â”‚
â”‚  Fixed worker count         â”‚           â”‚  Docker-based B2 sync           â”‚
â”‚                             â”‚           â”‚  Hot-scaling (no restart)       â”‚
â”‚                             â”‚           â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

\end{Verbatim}
~~~




---


## 2. Methods and Architecture


### 2.1 Container Health Monitor (G0_03)


Created a comprehensive monitoring tool that shows all 100+ containers at once:





~~~{=html}
<pre class="notion-ascii-diagram"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTAINER HEALTH MONITOR                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
<strong>â”‚  ğŸ—ï¸  INFRASTRUCTURE    </strong>                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚  RabbitMQ: ğŸŸ¢ ACTIVE   Uptime: 4h12m   ğŸ“¬ 1,234 msgs, 45 consumers          â”‚
â”‚  GROBID:   ğŸŸ¢ ACTIVE   Uptime: 4h12m                                        â”‚
â”‚                                                                             â”‚
<strong>â”‚  ğŸ“¦ ORCHESTRATORS (56 running)                                              â”‚
</strong>â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  e3f2a1b  orchestrator-1   ğŸŸ¢ ACTIVE   â—â—â—â—â—‰â—‹â—‹â—‹â—‹â—‹ G5_00 [D:Tables]          â”‚
â”‚  a8c4d2e  orchestrator-2   ğŸ”µ WAITING  â—â—â—â—â—â—â—â—â—â— G9_02 [done]              â”‚
â”‚  f1b3c4d  orchestrator-3   ğŸ”´ STUCK    â—â—â—‰â—‹â—‹â—‹â—‹â—‹â—‹â—‹ G3_02 [B:Witnesses]       â”‚
â”‚                                                                             â”‚
<strong>â”‚  ğŸ”Œ GPU WORKERS (48 running)                                                â”‚
</strong>â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  c2d3e4f  spine-worker-gpu0-0    G2_00  ğŸŸ¢ ACTIVE   Q:5   Processing...     â”‚
â”‚  d3e4f5a  paddle-worker-gpu1-0   G3_02  ğŸ”´ STUCK    Q:12  Stalled 5m ago    â”‚
â”‚                                                                             â”‚
<strong>â”‚  âš ï¸  Quick Access (problem containers):                                     â”‚
â”‚  STUCK ORCHESTRATORS:                                                       â”‚
</strong>â”‚    f1b3c4d (orchestrator-3) @ G3_02                                         â”‚
â”‚      â†’ 4 workers handle this stage: d3e4f5a, e4f5a6b (check if crashed)     â”‚
â”‚                                                                             â”‚
â”‚  Commands: docker logs &lt;ID&gt;  â”‚  docker exec -it &lt;ID&gt; bash                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTAINER HEALTH MONITOR                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
\textbf{â”‚  ğŸ—ï¸  INFRASTRUCTURE    }                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚  RabbitMQ: ğŸŸ¢ ACTIVE   Uptime: 4h12m   ğŸ“¬ 1,234 msgs, 45 consumers          â”‚
â”‚  GROBID:   ğŸŸ¢ ACTIVE   Uptime: 4h12m                                        â”‚
â”‚                                                                             â”‚
\textbf{â”‚  ğŸ“¦ ORCHESTRATORS (56 running)                                              â”‚
}â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  e3f2a1b  orchestrator-1   ğŸŸ¢ ACTIVE   â—â—â—â—â—‰â—‹â—‹â—‹â—‹â—‹ G5_00 [D:Tables]          â”‚
â”‚  a8c4d2e  orchestrator-2   ğŸ”µ WAITING  â—â—â—â—â—â—â—â—â—â— G9_02 [done]              â”‚
â”‚  f1b3c4d  orchestrator-3   ğŸ”´ STUCK    â—â—â—‰â—‹â—‹â—‹â—‹â—‹â—‹â—‹ G3_02 [B:Witnesses]       â”‚
â”‚                                                                             â”‚
\textbf{â”‚  ğŸ”Œ GPU WORKERS (48 running)                                                â”‚
}â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  c2d3e4f  spine-worker-gpu0-0    G2_00  ğŸŸ¢ ACTIVE   Q:5   Processing...     â”‚
â”‚  d3e4f5a  paddle-worker-gpu1-0   G3_02  ğŸ”´ STUCK    Q:12  Stalled 5m ago    â”‚
â”‚                                                                             â”‚
\textbf{â”‚  âš ï¸  Quick Access (problem containers):                                     â”‚
â”‚  STUCK ORCHESTRATORS:                                                       â”‚
}â”‚    f1b3c4d (orchestrator-3) @ G3_02                                         â”‚
â”‚      â†’ 4 workers handle this stage: d3e4f5a, e4f5a6b (check if crashed)     â”‚
â”‚                                                                             â”‚
â”‚  Commands: docker logs <ID>  â”‚  docker exec -it <ID> bash                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\end{Verbatim}
~~~








Key Features:


- Uses Docker Socket API (works inside containers without docker CLI)
- Classifies containers: Workers, Orchestrators, Infrastructure
- Shows DAG position with branch visualization (â—â—â—â—‰â—‹â—‹â—‹)
- Detects stale/stuck containers (configurable thresholds)
- Quick access section for problem containers with copy-paste commands





### 2.2 Fail-Fast Sentinel System


The 30-Minute Problem:
When a worker crashes or encounters an error, the dispatch script waits for a sentinel file that never appears. Default timeout: 30 minutes of wasted orchestrator time.







Solution: Failure Sentinels





~~~{=html}
<pre class="notion-ascii-diagram"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       <strong>  FAIL-FAST MECHANISM   </strong>                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚<strong>  BEFORE (30min timeout): </strong>                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  Worker crashes â†’ No sentinel written â†’ Dispatch waits 30min â†’ Timeout      â”‚
â”‚                                                                             â”‚
â”‚ <strong> AFTER (fail-fast):   </strong>                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚  Worker catches exception â†’ write_failure_sentinel(path, error)             â”‚
â”‚                          â†’ sentinel.json.failed created                     â”‚
â”‚                                                                             â”‚
â”‚  Dispatch polling â†’ Checks for sentinel.json                                â”‚
â”‚                  â†’ ALSO checks for sentinel.json.failed                     â”‚
â”‚                  â†’ Reads error message â†’ Fails immediately                  â”‚
â”‚                                                                             â”‚
â”‚  <strong>Time saved: 30 minutes â†’ &lt;5 seconds</strong>                                        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       \textbf{  FAIL-FAST MECHANISM   }                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚\textbf{  BEFORE (30min timeout): }                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  Worker crashes â†’ No sentinel written â†’ Dispatch waits 30min â†’ Timeout      â”‚
â”‚                                                                             â”‚
â”‚ \textbf{ AFTER (fail-fast):   }                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚  Worker catches exception â†’ write_failure_sentinel(path, error)             â”‚
â”‚                          â†’ sentinel.json.failed created                     â”‚
â”‚                                                                             â”‚
â”‚  Dispatch polling â†’ Checks for sentinel.json                                â”‚
â”‚                  â†’ ALSO checks for sentinel.json.failed                     â”‚
â”‚                  â†’ Reads error message â†’ Fails immediately                  â”‚
â”‚                                                                             â”‚
â”‚  \textbf{Time saved: 30 minutes â†’ <5 seconds}                                        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

\end{Verbatim}
~~~








### 2.3 Worker Connection Manager


GPU workers processing large PDFs can run 30+ minutes per job, exceeding RabbitMQ's consumer timeout:





~~~{=html}
<pre class="notion-ascii-diagram"><code>Python

<strong># OLD PATTERN (manual, times out on long jobs):</strong>
conn = get_connection(logger)
ch = conn.channel()
ch.basic_consume(...)
ch.start_consuming()  <strong># Connection dies after 30min!</strong>

<strong># NEW PATTERN (auto-reconnection, handles timeouts):</strong>
manager = WorkerConnectionManager(
    queue_name=JOB_QUEUE,
    on_message=on_message,
    logger=logger,
    prefetch_count=1,
    heartbeat_interval=60,
    reconnect_delay=5,
    max_reconnect_attempts=0,  <strong># Infinite retries (Container [daemon] stays up)</strong>
)
manager.start()  # Handles everything automatically
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
Python

\textbf{# OLD PATTERN (manual, times out on long jobs):}
conn = get_connection(logger)
ch = conn.channel()
ch.basic_consume(...)
ch.start_consuming()  \textbf{# Connection dies after 30min!}

\textbf{# NEW PATTERN (auto-reconnection, handles timeouts):}
manager = WorkerConnectionManager(
    queue_name=JOB_QUEUE,
    on_message=on_message,
    logger=logger,
    prefetch_count=1,
    heartbeat_interval=60,
    reconnect_delay=5,
    max_reconnect_attempts=0,  \textbf{# Infinite retries (Container [daemon] stays up)}
)
manager.start()  # Handles everything automatically

\end{Verbatim}
~~~




Key Fix: Disabled pika heartbeat (caused race condition: IndexError: pop from an empty deque) and rely on RabbitMQ's consumer_timeout=7200000 (2 hours) instead.







### 2.4 Hot-Scaling


Add or remove orchestrators while the pipeline is running - no restart needed:





~~~{=html}
<pre class="notion-ascii-diagram"><code>Bash

<strong># Makefile

# Add more orchestrators (jobs redistributed automatically via RabbitMQ)</strong>
make scale-orch SCALE=32

<strong># Current counts</strong>
make scale-status
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
Bash

\textbf{# Makefile

# Add more orchestrators (jobs redistributed automatically via RabbitMQ)}
make scale-orch SCALE=32

\textbf{# Current counts}
make scale-status

\end{Verbatim}
~~~




---


## 3. Key Technical Outcomes


### 3.1 Changes Made



<table>
  <thead>
    <tr>
      <th><strong>Component</strong></th>
      <th><strong>Change</strong></th>
      <th><strong>Impact</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>G0_03_container_health.py</code></td>
      <td>New 1100-line monitoring tool</td>
      <td>Visibility into 100+ containers</td>
    </tr>
    <tr>
      <td><code>lib/rabbitmq.py</code></td>
      <td>Added <code>write_failure_sentinel()</code></td>
      <td>Fail-fast (30min â†’ 5sec)</td>
    </tr>
    <tr>
      <td>All worker scripts</td>
      <td>Call failure sentinel on exception</td>
      <td>Orchestrators detect failures immediately</td>
    </tr>
    <tr>
      <td><code>WorkerConnectionManager</code></td>
      <td>Disabled heartbeat, use consumer_timeout</td>
      <td>No more connection drops on long jobs</td>
    </tr>
    <tr>
      <td><code>docker-compose.yaml</code></td>
      <td>Added Docker socket mount for health monitor</td>
      <td>Container inspection works</td>
    </tr>
    <tr>
      <td><code>Makefile</code></td>
      <td>Added hot-scaling and health targets</td>
      <td>Dynamic capacity adjustment</td>
    </tr>
  </tbody>
</table>



### 3.2 Worker Template Updates


Every GPU worker now follows this pattern for robust error handling:





~~~{=html}
<pre class="notion-ascii-diagram"><code>Python

try:
    metrics = process_job(doc_id, contract, artifacts_root, logger)
    write_sentinel(sentinel_path, logger)
    ch.basic_ack(delivery_tag=tag)

except Exception as e:
    logger.exception(&quot;Failed: %s&quot;, e)
    <strong># NEW: Write failure sentinel so dispatch fails fast</strong>
    if sentinel_path and sentinel_path.name:
        write_failure_sentinel(sentinel_path, str(e), logger)
    ch.basic_nack(delivery_tag=tag, requeue=False)

finally:
    <strong># NEW: Always clear progress (prevents stale display)</strong>
    clear_worker_progress(WORKER_NAME)
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
Python

try:
    metrics = process_job(doc_id, contract, artifacts_root, logger)
    write_sentinel(sentinel_path, logger)
    ch.basic_ack(delivery_tag=tag)

except Exception as e:
    logger.exception("Failed: %s", e)
    \textbf{# NEW: Write failure sentinel so dispatch fails fast}
    if sentinel_path and sentinel_path.name:
        write_failure_sentinel(sentinel_path, str(e), logger)
    ch.basic_nack(delivery_tag=tag, requeue=False)

finally:
    \textbf{# NEW: Always clear progress (prevents stale display)}
    clear_worker_progress(WORKER_NAME)

\end{Verbatim}
~~~




### 3.3 Docker Socket Integration


The health monitor runs inside a container but needs to inspect other containers:





~~~{=html}
<pre class="notion-ascii-diagram"><code>Yaml

<strong># docker-compose.yaml</strong>
container-health:
  volumes:
    - /var/run/docker.sock:/var/run/docker.sock:ro  <strong># Docker socket mount</strong>
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
Yaml

\textbf{# docker-compose.yaml}
container-health:
  volumes:
    - /var/run/docker.sock:/var/run/docker.sock:ro  \textbf{# Docker socket mount}

\end{Verbatim}
~~~








Using the Docker Engine API via Unix socket (no docker CLI needed):





~~~{=html}
<pre class="notion-ascii-diagram"><code>Python

class DockerSocketConnection(http.client.HTTPConnection):
    def connect(self):
        self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        self.sock.connect(&quot;/var/run/docker.sock&quot;)

<strong># GET /containers/json?all=true
# GET /containers/{id}/json (for restart count, started_at)</strong>
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
Python

class DockerSocketConnection(http.client.HTTPConnection):
    def connect(self):
        self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        self.sock.connect("/var/run/docker.sock")

\textbf{# GET /containers/json?all=true
# GET /containers/\textbraceleft{\textbraceright{}id\textbraceright{}/json (for restart count, started_at)}

\end{Verbatim}
~~~




---


## 4. Critical Reflection


### 4.1 What Worked Well



<table>
  <thead>
    <tr>
      <th><strong>Aspect</strong></th>
      <th><strong>Success Factor</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Fail-fast sentinels</strong></td>
      <td>Purely additive - no changes to success path, just catches failures faster</td>
    </tr>
    <tr>
      <td><strong>Docker socket API</strong></td>
      <td>Works inside containers without docker CLI, no dependencies</td>
    </tr>
    <tr>
      <td><strong>Hot-scaling</strong></td>
      <td>RabbitMQ&#39;s consumer model naturally supports adding/removing workers</td>
    </tr>
    <tr>
      <td><strong>Profile-based deployment</strong></td>
      <td>Pre-generated compose files (2gpu.yaml, 4gpu.yaml) - no runtime generation</td>
    </tr>
  </tbody>
</table>



### 4.2 Problems Encountered



<table>
  <thead>
    <tr>
      <th><strong>Problem</strong></th>
      <th><strong>Root Cause</strong></th>
      <th><strong>Solution</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>IndexError: pop from empty deque</code></td>
      <td>pika 1.3.2 race condition with heartbeat thread</td>
      <td>Disable heartbeat, use consumer_timeout</td>
    </tr>
    <tr>
      <td>Workers showing &quot;Idle&quot; when busy</td>
      <td>No progress tracking during initialization</td>
      <td>Call <code>write_worker_progress()</code> immediately on job start</td>
    </tr>
    <tr>
      <td>30-minute timeout delays</td>
      <td>Dispatch only checked for success sentinel</td>
      <td>Add failure sentinel check alongside success</td>
    </tr>
    <tr>
      <td>SSH disconnect kills sync</td>
      <td>Shell process tied to SSH session</td>
      <td>Docker-based B2 sync with <code>restart: unless-stopped</code></td>
    </tr>
  </tbody>
</table>



### 4.3 Lessons Learned

1. **Timeouts are silent killers at scale** - A 30-minute timeout is fine for development but devastating when you have 56 orchestrators waiting in parallel.
2. **Heartbeat handling is tricky** - pika's `BlockingConnection` doesn't automatically process heartbeats during callbacks. The "correct" solution (heartbeat thread) has race conditions. The pragmatic solution: disable heartbeats and use longer server-side timeouts.
3. **Monitoring is not optional at scale** - You cannot tail logs for 100+ containers. Centralized status with problem detection is essential.
4. **Fail-fast is always better than fail-slow** - When something goes wrong, know about it immediately. The failure sentinel pattern is purely additive (no risk to success path) but saves hours of wasted compute.





### 4.4 Technical Debt / Risks



<table>
  <thead>
    <tr>
      <th><strong>Risk</strong></th>
      <th><strong>Mitigation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Health monitor depends on resources.yaml structure</td>
      <td>DAG_STRUCTURE dict mirrors <code>lib/core.py</code> - keep in sync</td>
    </tr>
    <tr>
      <td>Failure sentinel uses <code>.failed</code> suffix convention</td>
      <td>Documented in lib/rabbitmq.py, all workers follow pattern</td>
    </tr>
    <tr>
      <td>Consumer timeout of 2 hours may be too short</td>
      <td>Monitor for <code>ChannelClosedByBroker</code> errors in logs</td>
    </tr>
  </tbody>
</table>



---


## 5. Next Steps


### 5.1 Immediate (Current Session)

- âœ… Deploy pipeline on B200 2-GPU profile
- âœ… Validate failure sentinel works end-to-end
- â³ Monitor for remaining timeout/connection issues
- â³ Collect logs and identify any new failure modes





### 5.2 Next Phase: Full-Scale B200 Run



<table>
  <thead>
    <tr>
      <th><strong>Task</strong></th>
      <th><strong>Description</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Analyze 2-GPU run outcomes</strong></td>
      <td>Review logs from current run, identify any crashes, stalls, or failure patterns</td>
    </tr>
    <tr>
      <td><strong>Fix identified issues</strong></td>
      <td>Address any new problems discovered during 2-GPU run</td>
    </tr>
    <tr>
      <td><strong>Scale to 8-GPU profile</strong></td>
      <td>Deploy <code>make run-8gpu</code> (297 containers, 56 orchestrators)</td>
    </tr>
    <tr>
      <td><strong>Process full corpus</strong></td>
      <td>~8,000 PDFs through complete pipeline</td>
    </tr>
    <tr>
      <td><strong>Validate output quality</strong></td>
      <td>Spot-check fused documents for correctness</td>
    </tr>
  </tbody>
</table>



### 5.3 Full-Scale Run Plan




~~~{=html}
<pre class="notion-ascii-diagram"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
<strong>â”‚                           B200 8-GPU FULL RUN                               â”‚</strong>
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  <strong>Hardware:</strong>      DGX B200 (8Ã— GPUs, 1.44TB VRAM, 208 CPU cores)              â”‚
â”‚  <strong>Corpus:</strong>        ~8,000 PDFs (UK Biobank research papers)                    â”‚
â”‚  <strong>Containers:</strong>    321 (28 workers per GPU Ã— 8 GPUs + orchestrators + infra)   â”‚
â”‚  <strong>Orchestrators:</strong> 80 (10 per GPU)                                             â”‚
â”‚                                                                             â”‚
â”‚  Expected throughput: ~500-1000 docs/hour (TBD after 2-GPU validation)      â”‚
â”‚                                                                             â”‚
â”‚  <strong>Monitoring:  </strong>                                                              â”‚
â”‚    make health-8gpu      â†’ Container status                                 â”‚
â”‚    make monitor-8gpu     â†’ Document progress                                â”‚
â”‚    make sync             â†’ B2 backup (runs in Docker)                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
\textbf{â”‚                           B200 8-GPU FULL RUN                               â”‚}
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  \textbf{Hardware:}      DGX B200 (8Ã— GPUs, 1.44TB VRAM, 208 CPU cores)              â”‚
â”‚  \textbf{Corpus:}        ~8,000 PDFs (UK Biobank research papers)                    â”‚
â”‚  \textbf{Containers:}    321 (28 workers per GPU Ã— 8 GPUs + orchestrators + infra)   â”‚
â”‚  \textbf{Orchestrators:} 80 (10 per GPU)                                             â”‚
â”‚                                                                             â”‚
â”‚  Expected throughput: ~500-1000 docs/hour (TBD after 2-GPU validation)      â”‚
â”‚                                                                             â”‚
â”‚  \textbf{Monitoring:  }                                                              â”‚
â”‚    make health-8gpu      â†’ Container status                                 â”‚
â”‚    make monitor-8gpu     â†’ Document progress                                â”‚
â”‚    make sync             â†’ B2 backup (runs in Docker)                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\end{Verbatim}
~~~




### 5.4 Future Improvements (Post-Dissertation)

1. **Metrics to Prometheus/Grafana** - Export container health to time-series DB for historical analysis
2. **Auto-recovery** - Automatically restart stuck workers based on health monitor detection
3. **Queue depth alerts** - Notify when worker queues grow beyond threshold (workers falling behind)
4. **GPU memory tracking** - Add VRAM usage to health monitor (currently skipped due to stats API complexity)








