# NER Build


## Content






## 1. Objective and Scope


### 1.1 The Challenge: Extracting Diseases from UK Biobank Papers


We are processing a frozen corpus of UK Biobank papers (up to July 2025). The goal is to extract disease entities from scientific text and map them to standardised medical ontologies (UMLS/SNOMED). This requires a Named Entity Recognition (NER) pipeline that can:


1. **Detect disease mentions** in titles, abstracts, and body text
2. **Normalise** these mentions to Concept Unique Identifiers (CUIs)
3. **Rank** diseases by their importance within each paper (primary vs. incidental)

### 1.2 Initial Architecture


The original design used a ensemble approach with three NER models:





~~~{=html}
<pre class="notion-ascii-diagram"><code>               ┌────────────────────────┐
               │       <strong>PAPER.JSON </strong>      │
               ├────────────────────────┤    
               │     <strong>FROZEN CORPUS </strong>     │
               │    6,600+ journals     │
               └───────────┬────────────┘     
                           │
                           ▼
┌───────────────────────────────────────────────────────┐
│                <strong>  NER 3x ENSEMBLE     </strong>                 │
│  ┌─────────────────────────────────────────────────┐  │
│  │ <strong>MedCat (PRIMARY) </strong>  ────► CUI + Preferred Name   │  │
│  │ Word2Vec + UMLS lookup                          │  │
│  │                                                 │  │
│  │ <strong>BERN2 (WITNESS 1)</strong>  ────► BioBERT Confirmation   │  │
│  │                                                 │  │
│  │ <strong>GLiNER (WITNESS 2)</strong> ────► Zero-Shot Confirmation │  │
│  └─────────────────────────────────────────────────┘  │
└───────────────────────────────────────────────────────┘</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
               ┌────────────────────────┐
               │       \textbf{PAPER.JSON }      │
               ├────────────────────────┤    
               │     \textbf{FROZEN CORPUS }     │
               │    6,600+ journals     │
               └───────────┬────────────┘     
                           │
                           ▼
┌───────────────────────────────────────────────────────┐
│                \textbf{  NER 3x ENSEMBLE     }                 │
│  ┌─────────────────────────────────────────────────┐  │
│  │ \textbf{MedCat (PRIMARY) }  ────► CUI + Preferred Name   │  │
│  │ Word2Vec + UMLS lookup                          │  │
│  │                                                 │  │
│  │ \textbf{BERN2 (WITNESS 1)}  ────► BioBERT Confirmation   │  │
│  │                                                 │  │
│  │ \textbf{GLiNER (WITNESS 2)} ────► Zero-Shot Confirmation │  │
│  └─────────────────────────────────────────────────┘  │
└───────────────────────────────────────────────────────┘
\end{Verbatim}
~~~








---


## 2. Methods and Process


### 2.1 Phase 1: MedCat as Primary Model


### 2.1.1 Rationale for MedCat Selection


MedCat (Medical Concept Annotation Tool) was initially selected as the primary NER model because:


1. **Direct UMLS/SNOMED linking** — MedCat provides CUI (Concept Unique Identifier) codes, essential for standardising disease mentions to a controlled vocabulary
2. **NHS clinical alignment** — The SNOMED-UK model pack aligned with UK Biobank's medical record linkages
3. **Available model pack** — Downloaded via UMLS license (obtained through institutional credentials)

### 2.1.2 Technical Architecture of MedCat


MedCat uses a Word2Vec-based architecture for entity recognition and concept linking:





~~~{=html}
<pre class="notion-ascii-diagram"><code>┌─────────────────────────────────────────────────────────────────────────────┐
│                        <strong> MEDCAT WORD2VEC ARCHITECTURE </strong>                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│                                  <strong>INPUT TEXT</strong>                                 │                                                             
│                                      │                                      │
│                                      ▼                                      │
│   ┌───────────────────────────────────────────────────────────────────┐     │
│   │  <strong>TOKENISATION + CONTEXT WINDOW </strong><strong>(configurable, unlimited width)</strong><strong> </strong>   │     │
│   └───────────────────────────────────────────────────────────────────┘     │
│                                      │                                      │
│                                      ▼                                      │
│   ┌───────────────────────────────────────────────────────────────────┐     │
│   │  <strong>WORD2VEC EMBEDDINGS </strong><strong>(pre-trained on biomedical corpus) </strong>          │     │
│   │  - Maps words to vector space                                     │     │
│   │  - Context window determines co-occurrence statistics             │     │
│   └───────────────────────────────────────────────────────────────────┘     │
│                                      │                                      │
│                                      ▼                                      │
│   ┌───────────────────────────────────────────────────────────────────┐     │
│   │  <strong>CONCEPT LINKING </strong><strong>(dictionary lookup against UMLS/MeSH)</strong><strong> </strong>           │     │
│   │  - Matches embedding vectors to known concept vectors             │     │
│   │  - Returns CUI code + preferred name + confidence                 │     │
│   └───────────────────────────────────────────────────────────────────┘     │
│                                      │                                      │
│                                      ▼                                      │
│   <strong>OUTPUT:</strong> {text_span, cui, preferred_name, confidence, type_ids}            │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
┌─────────────────────────────────────────────────────────────────────────────┐
│                        \textbf{ MEDCAT WORD2VEC ARCHITECTURE }                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│                                  \textbf{INPUT TEXT}                                 │                                                             
│                                      │                                      │
│                                      ▼                                      │
│   ┌───────────────────────────────────────────────────────────────────┐     │
│   │  \textbf{TOKENISATION + CONTEXT WINDOW }\textbf{(configurable, unlimited width)}\textbf{ }   │     │
│   └───────────────────────────────────────────────────────────────────┘     │
│                                      │                                      │
│                                      ▼                                      │
│   ┌───────────────────────────────────────────────────────────────────┐     │
│   │  \textbf{WORD2VEC EMBEDDINGS }\textbf{(pre-trained on biomedical corpus) }          │     │
│   │  - Maps words to vector space                                     │     │
│   │  - Context window determines co-occurrence statistics             │     │
│   └───────────────────────────────────────────────────────────────────┘     │
│                                      │                                      │
│                                      ▼                                      │
│   ┌───────────────────────────────────────────────────────────────────┐     │
│   │  \textbf{CONCEPT LINKING }\textbf{(dictionary lookup against UMLS/MeSH)}\textbf{ }           │     │
│   │  - Matches embedding vectors to known concept vectors             │     │
│   │  - Returns CUI code + preferred name + confidence                 │     │
│   └───────────────────────────────────────────────────────────────────┘     │
│                                      │                                      │
│                                      ▼                                      │
│   \textbf{OUTPUT:} \textbraceleft{\textbraceright{}text_span, cui, preferred_name, confidence, type_ids\textbraceright{}            │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

\end{Verbatim}
~~~




Key Observation: Word2Vec (Mikolov et al., 2013) creates static word embeddings, each word has exactly one vector regardless of context. The word "cancer" has the same vector in "diagnosed with cancer" and "negative for cancer". Modern transformer models (BERT, BioBERT) create contextual embeddings where meaning changes based on surrounding words.



### 2.2 Phase 2: Discovering the CPU/GPU Bottleneck


### 2.2.1 The Problem


When running all three models together, we discovered a critical resource contention iss





~~~{=html}
<pre class="notion-ascii-diagram"><code>┌─────────────────────────────────────────────────────────────────────────────┐
│                     <strong>      ORIGINAL WORKFLOW   </strong>                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   <strong>FOR EACH TEXT SEGMENT:</strong>                                                    │
│       │                                                                     │
│       ├──► <strong>MedCat.extract()</strong> ──────────► [CPU-BOUND, SEQUENTIAL]             │
│       │         ⏱️ ~500ms per segment                                       │
│       │                                                                     │
│       ├──► <strong>BERN2.extract()</strong> ───────────► [GPU-ACCELERATED]                   │
│       │         ⏱️ ~50ms per batch                                          │
│       │                                                                     │
│       └──► <strong>GLiNER.extract()</strong> ──────────► [GPU-ACCELERATED]                   │
│                 ⏱️ ~30ms per batch                                          │
│                                                                             │
│   <strong>RESULT:</strong> GPU sits idle 80%+ of time waiting for MedCat CPU operations      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
┌─────────────────────────────────────────────────────────────────────────────┐
│                     \textbf{      ORIGINAL WORKFLOW   }                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   \textbf{FOR EACH TEXT SEGMENT:}                                                    │
│       │                                                                     │
│       ├──► \textbf{MedCat.extract()} ──────────► [CPU-BOUND, SEQUENTIAL]             │
│       │         ⏱️ ~500ms per segment                                       │
│       │                                                                     │
│       ├──► \textbf{BERN2.extract()} ───────────► [GPU-ACCELERATED]                   │
│       │         ⏱️ ~50ms per batch                                          │
│       │                                                                     │
│       └──► \textbf{GLiNER.extract()} ──────────► [GPU-ACCELERATED]                   │
│                 ⏱️ ~30ms per batch                                          │
│                                                                             │
│   \textbf{RESULT:} GPU sits idle 80%+ of time waiting for MedCat CPU operations      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
\end{Verbatim}
~~~




MedCat's underlying spaCy tokenisation and Word2Vec lookup are fundamentally CPU-bound operations. The model cannot leverage GPU acceleration for its core processing loop.



### 2.2.2 Attempted Solution: Phase Separation


We introduced phase separation to run CPU and GPU workloads independently:


- Phase A: Run MedCat (CPU-bound) with CPU sharding
- Phase B: Run GPU witnesses (BERN2 + GLiNER) after CPU phase completes

This helped but introduced a new problem: database write contention.



### 2.3 Phase 3: The DuckDB Single-Writer Bottleneck


### 2.3.1 The Problem: Multiple Writers, Single Database


With phase separation, we scaled to multiple containers per model (3 containers each for MedCat, BERN2, and GLiNER). This created a new bottleneck:





~~~{=html}
<pre class="notion-ascii-diagram"><code>┌─────────────────────────────────────────────────────────────────────────────┐
│                      <strong>DUCKDB SINGLE-WRITER LIMITATION  </strong>                      │
├─────────────────────────────────────────────────────────────────────────────┤
│  <strong>Docker containers x9</strong>                                                       │
│         ▼                                                                   │
│   ┌────────────┐                                                            │
│   │ <strong>MedCat ×3</strong>  │──┐                                                         │
│   └────────────┘  │                                                         │
│                   │                <strong>Docker container</strong>                         │
│   ┌────────────┐  │   ┌─────────────────────────────────────────┐           │
│   │ <strong>BERN2 ×3</strong>   │──┼──►│        <strong>DuckDB (SINGLE WRITER)</strong>           │           │
│   └────────────┘  │   │                                         │           │
│                   │   │       &quot;Conflicting lock is held&quot;        │           │
│   ┌────────────┐  │   │   ONE connection, ONE write at a time   │           
│   │ <strong>GLiNER ×3</strong>  │──┘   │                                         │           │
│   └────────────┘      └─────────────────────────────────────────┘           │
│                                                                             │
│   <strong>TOTAL:</strong> 9 docker containers attempting concurrent writes = <strong>FAILURE</strong>         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
┌─────────────────────────────────────────────────────────────────────────────┐
│                      \textbf{DUCKDB SINGLE-WRITER LIMITATION  }                      │
├─────────────────────────────────────────────────────────────────────────────┤
│  \textbf{Docker containers x9}                                                       │
│         ▼                                                                   │
│   ┌────────────┐                                                            │
│   │ \textbf{MedCat ×3}  │──┐                                                         │
│   └────────────┘  │                                                         │
│                   │                \textbf{Docker container}                         │
│   ┌────────────┐  │   ┌─────────────────────────────────────────┐           │
│   │ \textbf{BERN2 ×3}   │──┼──►│        \textbf{DuckDB (SINGLE WRITER)}           │           │
│   └────────────┘  │   │                                         │           │
│                   │   │       "Conflicting lock is held"        │           │
│   ┌────────────┐  │   │   ONE connection, ONE write at a time   │           
│   │ \textbf{GLiNER ×3}  │──┘   │                                         │           │
│   └────────────┘      └─────────────────────────────────────────┘           │
│                                                                             │
│   \textbf{TOTAL:} 9 docker containers attempting concurrent writes = \textbf{FAILURE}         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
\end{Verbatim}
~~~




DuckDB is an OLAP (Online Analytical Processing) database designed for analytical queries, not concurrent writes. It uses a single-writer, multiple-reader model — only one process can hold a write lock at any time.



### 2.3.2 Solution Attempt: Results Writer Pattern


We introduced a dedicated Results Writer service that consumed results via RabbitMQ:





~~~{=html}
<pre class="notion-ascii-diagram"><code>┌─────────────────────────────────────────────────────────────────────────────┐
│                          <strong>DECOUPLED WRITE ARCHITECTURE   </strong>                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│                  <strong>NER WORKERS (inference only, no DB writes) </strong>                │
│                                                                             │
│              ┌────────────┐    ┌────────────┐    ┌────────────┐             │
│          <strong>    │ MedCat ×3  │    │ BERN2 ×3   │    │ GLiNER ×3  │   </strong>          │
│              └─────┬──────┘    └─────┬──────┘    └─────┬──────┘             │
│                    │                 │                 │                    │
│                    └─────────────────┼─────────────────┘                    │
│                                      │                                      │
│                                      ▼                                      │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                             <strong> RABBITMQ  </strong>                             │   │
│   │          Messages contain full DiseaseMentionRecord payloads        │   │
│   └──────────────────────────────────┬──────────────────────────────────┘   │
│                                      │                                      │
│                                      ▼                                      │
│                     ┌───────────────────────────────────┐                   │
│                     │      <strong>RESULTS WRITER (single) </strong>     │                   │
│                     │                                   │                   │
│                     │   Consumes from queue, batches    │                   │
│                     │        inserts to DuckDB          │                   │
│                     └────────────────┬──────────────────┘                   │
│                                      │                                      │
│                                      ▼                                      │
│                              ┌───────────────┐                              │
│                              │    <strong>DuckDB </strong>    │                              │
│                              └───────────────┘                              │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
┌─────────────────────────────────────────────────────────────────────────────┐
│                          \textbf{DECOUPLED WRITE ARCHITECTURE   }                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│                  \textbf{NER WORKERS (inference only, no DB writes) }                │
│                                                                             │
│              ┌────────────┐    ┌────────────┐    ┌────────────┐             │
│          \textbf{    │ MedCat ×3  │    │ BERN2 ×3   │    │ GLiNER ×3  │   }          │
│              └─────┬──────┘    └─────┬──────┘    └─────┬──────┘             │
│                    │                 │                 │                    │
│                    └─────────────────┼─────────────────┘                    │
│                                      │                                      │
│                                      ▼                                      │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                             \textbf{ RABBITMQ  }                             │   │
│   │          Messages contain full DiseaseMentionRecord payloads        │   │
│   └──────────────────────────────────┬──────────────────────────────────┘   │
│                                      │                                      │
│                                      ▼                                      │
│                     ┌───────────────────────────────────┐                   │
│                     │      \textbf{RESULTS WRITER (single) }     │                   │
│                     │                                   │                   │
│                     │   Consumes from queue, batches    │                   │
│                     │        inserts to DuckDB          │                   │
│                     └────────────────┬──────────────────┘                   │
│                                      │                                      │
│                                      ▼                                      │
│                              ┌───────────────┐                              │
│                              │    \textbf{DuckDB }    │                              │
│                              └───────────────┘                              │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────
\end{Verbatim}
~~~




Problem: The single writer could not keep up with 9 NER workers. Even with batching (500 records per flush), the writer became a throughput bottleneck. NER workers produced results faster than the writer could consume them.



### 2.3.3 Final Solution: Parquet Decoupling


The solution was to completely decouple IO from NER processing using Parquet files:





~~~{=html}
<pre class="notion-ascii-diagram"><code>┌─────────────────────────────────────────────────────────────────────────────┐
│                        <strong>PARQUET DECOUPLED ARCHITECTURE  </strong>                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│           <strong>PHASE 1: NER PROCESSING </strong><strong>(high throughput, no DB writes)</strong><strong>  </strong>         │
│                                                                             │
│              ┌────────────┐    ┌────────────┐    ┌────────────┐             │
│            <strong>  │ NER Model  │    │ NER Model  │    │ NER Model  │   </strong>          │
│              │ Worker ×N  │    │ Worker ×N  │    │ Worker ×N  │             │
│              └─────┬──────┘    └─────┬──────┘    └─────┬──────┘             │
│                    │                 │                 │                    │
│                    ▼                 ▼                 ▼                    │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                      <strong>PARQUET OUTPUT DIRECTORY   </strong>                    │   │
│   │   output/parquet/                                                   │   │
│   │   ├── disease_mentions/                                             │   │
│   │   │   └── disease_mentions_&lt;timestamp&gt;_&lt;uuid&gt;.parquet               │   │
│   │   └── papers/                                                       │   │
│   │       └── papers_&lt;timestamp&gt;_&lt;uuid&gt;.parquet                         │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│         <strong>PHASE 2: IMPORT </strong><strong>(single writer, after processing completes)</strong><strong>  </strong>       │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                        <strong>PARQUET IMPORT SCRIPT </strong>                       │   │
│   │                                                                     │   │
│   │           <strong>DuckDB:</strong> COPY FROM &#39;output/parquet/**/*.parquet&#39;           │   │
│   │                                                                     │   │
│   │             ⏱️ <strong>Bulk load:</strong> 6,600 papers in &lt; 30 seconds              │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
┌─────────────────────────────────────────────────────────────────────────────┐
│                        \textbf{PARQUET DECOUPLED ARCHITECTURE  }                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│           \textbf{PHASE 1: NER PROCESSING }\textbf{(high throughput, no DB writes)}\textbf{  }         │
│                                                                             │
│              ┌────────────┐    ┌────────────┐    ┌────────────┐             │
│            \textbf{  │ NER Model  │    │ NER Model  │    │ NER Model  │   }          │
│              │ Worker ×N  │    │ Worker ×N  │    │ Worker ×N  │             │
│              └─────┬──────┘    └─────┬──────┘    └─────┬──────┘             │
│                    │                 │                 │                    │
│                    ▼                 ▼                 ▼                    │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                      \textbf{PARQUET OUTPUT DIRECTORY   }                    │   │
│   │   output/parquet/                                                   │   │
│   │   ├── disease_mentions/                                             │   │
│   │   │   └── disease_mentions_<timestamp>_<uuid>.parquet               │   │
│   │   └── papers/                                                       │   │
│   │       └── papers_<timestamp>_<uuid>.parquet                         │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│         \textbf{PHASE 2: IMPORT }\textbf{(single writer, after processing completes)}\textbf{  }       │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                        \textbf{PARQUET IMPORT SCRIPT }                       │   │
│   │                                                                     │   │
│   │           \textbf{DuckDB:} COPY FROM 'output/parquet/**/*.parquet'           │   │
│   │                                                                     │   │
│   │             ⏱️ \textbf{Bulk load:} 6,600 papers in < 30 seconds              │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

\end{Verbatim}
~~~




This eliminated write contention entirely — each worker writes to independent Parquet files, and a single import step loads everything into DuckDB after processing completes.



### 2.4 Phase 4: Evaluating Model Performance


With the infrastructure issues resolved, we could properly evaluate model performance.



### 2.4.1 Word2Vec vs Transformer Embeddings


The fundamental limitation of MedCat became clear when analysing extraction quality:





~~~{=html}
<pre class="notion-ascii-diagram"><code><strong>WORD2VEC vs TRANSFORMER EMBEDDINGS:</strong>

<strong>Example sentence:</strong> &quot;The patient was negative for breast cancer.&quot;

┌─────────────────────────────────────────────────────────────────────────────┐
│ <strong>WORD2VEC (MedCat): </strong>                                                         │
│                                                                             │
│   &quot;cancer&quot; → [0.23, -0.45, 0.12, ...] (same vector always)                  │
│   &quot;negative&quot; → [0.11, 0.33, -0.21, ...]                                     │
│                                                                             │
│   <strong>PROBLEM:</strong> No negation awareness — &quot;cancer&quot; detected as positive mention    │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ <strong>TRANSFORMER (BERN2 / BioBERT):</strong>                                              │
│                                                                             │
│   &quot;cancer&quot; in &quot;negative for breast cancer&quot;                                  │
│       → [0.05, -0.67, 0.34, ...] (contextual vector reflecting negation)    │
│                                                                             │
│   &quot;cancer&quot; in &quot;diagnosed with breast cancer&quot;                                │
│       → [0.89, 0.12, -0.45, ...] (contextual vector reflecting positive)    │
│                                                                             │
│   <strong>RESULT:</strong> Context-aware embeddings enable negation detection                │
└─────────────────────────────────────────────────────────────────────────────┘</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
\textbf{WORD2VEC vs TRANSFORMER EMBEDDINGS:}

\textbf{Example sentence:} "The patient was negative for breast cancer."

┌─────────────────────────────────────────────────────────────────────────────┐
│ \textbf{WORD2VEC (MedCat): }                                                         │
│                                                                             │
│   "cancer" → [0.23, -0.45, 0.12, ...] (same vector always)                  │
│   "negative" → [0.11, 0.33, -0.21, ...]                                     │
│                                                                             │
│   \textbf{PROBLEM:} No negation awareness — "cancer" detected as positive mention    │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ \textbf{TRANSFORMER (BERN2 / BioBERT):}                                              │
│                                                                             │
│   "cancer" in "negative for breast cancer"                                  │
│       → [0.05, -0.67, 0.34, ...] (contextual vector reflecting negation)    │
│                                                                             │
│   "cancer" in "diagnosed with breast cancer"                                │
│       → [0.89, 0.12, -0.45, ...] (contextual vector reflecting positive)    │
│                                                                             │
│   \textbf{RESULT:} Context-aware embeddings enable negation detection                │
└─────────────────────────────────────────────────────────────────────────────┘
\end{Verbatim}
~~~




### 2.4.2 Throughput Comparison



<table>
  <thead>
    <tr>
      <th><strong>Metric</strong></th>
      <th><strong>MedCat</strong></th>
      <th><strong>BERN2</strong></th>
      <th><strong>GLiNER</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Processing Speed</strong></td>
      <td>~2 papers/sec</td>
      <td>~50 papers/sec</td>
      <td>~40 papers/sec</td>
    </tr>
    <tr>
      <td><strong>Compute Type</strong></td>
      <td>CPU-bound</td>
      <td>GPU-accelerated</td>
      <td>GPU-accelerated</td>
    </tr>
    <tr>
      <td><strong>Embedding Type</strong></td>
      <td>Static (Word2Vec)</td>
      <td>Contextual (BioBERT)</td>
      <td>Contextual (Transformer)</td>
    </tr>
    <tr>
      <td><strong>Native CUI Linking</strong></td>
      <td>Yes</td>
      <td>Via SapBERT post-processing</td>
      <td>No</td>
    </tr>
  </tbody>
</table>



### 2.4.3 Extraction Scope Reduction


During testing, we discovered that extracting from full paper body produced ~1,200 disease mentions per paper, creating noise that obscured the primary research focus.



Solution: Limit extraction to title + abstract only, yielding ~5-20 high-quality mentions per paper.





~~~{=html}
<pre class="notion-ascii-diagram"><code><strong>EXTRACTION SCOPE:</strong>

<strong>BEFORE (full extraction):</strong>
├── Title ───────────► 2-5 mentions
├── Abstract ────────► 10-30 mentions
├── Body text ───────► 800-1000 mentions  ← <strong>NOISE</strong>
├── Tables ──────────► 100-200 mentions   ← <strong>NOISE</strong>
└── Captions ────────► 50-100 mentions    ← <strong>NOISE</strong>

<strong>AFTER (title + abstract only):</strong>
├── Title ───────────► 2-5 mentions    ✓
└── Abstract ────────► 10-30 mentions  ✓
    <strong>TOTAL:</strong> 15-35 high-quality mentions</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
\textbf{EXTRACTION SCOPE:}

\textbf{BEFORE (full extraction):}
├── Title ───────────► 2-5 mentions
├── Abstract ────────► 10-30 mentions
├── Body text ───────► 800-1000 mentions  ← \textbf{NOISE}
├── Tables ──────────► 100-200 mentions   ← \textbf{NOISE}
└── Captions ────────► 50-100 mentions    ← \textbf{NOISE}

\textbf{AFTER (title + abstract only):}
├── Title ───────────► 2-5 mentions    ✓
└── Abstract ────────► 10-30 mentions  ✓
    \textbf{TOTAL:} 15-35 high-quality mentions
\end{Verbatim}
~~~




---


## 3. Key Findings and Technical Outcomes


### 3.1 MedCat Limitations for This Use Case



<table>
  <thead>
    <tr>
      <th>Factor</th>
      <th>Observation</th>
      <th>Impact</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Embedding Type</strong></td>
      <td>Word2Vec creates static embeddings</td>
      <td>Cannot distinguish negated mentions from positive mentions</td>
    </tr>
    <tr>
      <td><strong>Processing Model</strong></td>
      <td>CPU-bound spaCy pipeline</td>
      <td>Cannot leverage GPU acceleration</td>
    </tr>
    <tr>
      <td><strong>Throughput</strong></td>
      <td>~2 papers/sec</td>
      <td>25× slower than transformer alternatives</td>
    </tr>
    <tr>
      <td><strong>Architecture Age</strong></td>
      <td>2018-era Word2Vec</td>
      <td>Does not reflect current state-of-the-art</td>
    </tr>
  </tbody>
</table>



Important Clarification: MedCat is not fundamentally flawed — it remains a valid choice for:


- CPU-only infrastructure (no GPU available)
- Real-time clinical annotation (where SNOMED-CT UK codes are specifically required)
- Interactive annotation workflows

Our specific constraints (batch processing at scale, GPU available, contextual disambiguation required) made it suboptimal for this pipeline.



### 3.2 CUI Normalisation Without MedCat


A key concern with removing MedCat was losing CUI linking capability. We evaluated SapBERT-based neural normalisation as an alternative:





~~~{=html}
<pre class="notion-ascii-diagram"><code>┌─────────────────────────────────────────────────────────────────────────────┐
│                  <strong>  SAPBERT + FAISS NEAREST NEIGHBOUR   </strong>                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│ <strong>  INPUT:</strong> text_span = &quot;Type 2 Diabetes Mellitus&quot;                             │
│                                                                             │
│   <strong>STEP 1: SapBERT Embedding</strong>                                                 │
│   ───────────────────────────────────────────────────────────────────────   │
│   text_span ───► SapBERT ───► embedding vector [768 dims]                   │
│                                                                             │
│   <strong>STEP 2: FAISS Nearest Neighbour Search </strong>                                   │
│   ───────────────────────────────────────────────────────────────────────   │
│   embedding ───► FAISS index (pre-computed UMLS concept embeddings)         │
│                                                                             │
│   <strong>STEP 3: CUI Retrieval </strong>                                                    │
│   ───────────────────────────────────────────────────────────────────────   │
│   <strong>Nearest match: </strong>                                                           │
│      CUI = &quot;C0011860&quot;                                                       │
│      preferred_name = &quot;Diabetes Mellitus Type 2&quot;                            │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
┌─────────────────────────────────────────────────────────────────────────────┐
│                  \textbf{  SAPBERT + FAISS NEAREST NEIGHBOUR   }                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│ \textbf{  INPUT:} text_span = "Type 2 Diabetes Mellitus"                             │
│                                                                             │
│   \textbf{STEP 1: SapBERT Embedding}                                                 │
│   ───────────────────────────────────────────────────────────────────────   │
│   text_span ───► SapBERT ───► embedding vector [768 dims]                   │
│                                                                             │
│   \textbf{STEP 2: FAISS Nearest Neighbour Search }                                   │
│   ───────────────────────────────────────────────────────────────────────   │
│   embedding ───► FAISS index (pre-computed UMLS concept embeddings)         │
│                                                                             │
│   \textbf{STEP 3: CUI Retrieval }                                                    │
│   ───────────────────────────────────────────────────────────────────────   │
│   \textbf{Nearest match: }                                                           │
│      CUI = "C0011860"                                                       │
│      preferred_name = "Diabetes Mellitus Type 2"                            │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
\end{Verbatim}
~~~




SapBERT (Self-Alignment Pretraining for BERT) was specifically trained on UMLS synonymy relationships, making it highly effective for biomedical concept normalisation. This approach achieves equivalent CUI linking while allowing any NER model to be used for entity detection.



### 3.3 Witness Model Evaluation


GLiNER Strengths:




<table>
  <thead>
    <tr>
      <th>Strength</th>
      <th>Explanation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Zero-shot capability</strong></td>
      <td>Can recognise entity types not in training data</td>
    </tr>
    <tr>
      <td><strong>Table text handling</strong></td>
      <td>Fixed-vocabulary BERT models often truncate table columns; GLiNER handles arbitrary spans</td>
    </tr>
    <tr>
      <td><strong>Novelty</strong></td>
      <td>Released 2024; captures latest biomedical knowledge</td>
    </tr>
    <tr>
      <td><strong>Recall boost</strong></td>
      <td>Catches entities that BioBERT-based models miss</td>
    </tr>
  </tbody>
</table>



PubMedBERT-NER Strengths:




<table>
  <thead>
    <tr>
      <th>Strength</th>
      <th>Explanation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Legacy corpus training</strong></td>
      <td>Trained on all PubMed abstracts (massive domain coverage)</td>
    </tr>
    <tr>
      <td><strong>High precision</strong></td>
      <td>Conservative predictions reduce false positives</td>
    </tr>
    <tr>
      <td><strong>Complementary errors</strong></td>
      <td>Different training regime means different failure modes vs BERN2</td>
    </tr>
  </tbody>
</table>



### 3.4 Disease Ranking Algorithm


To determine which disease is the "primary focus" of a paper vs. incidental mentions:





~~~{=html}
<pre class="notion-ascii-diagram"><code>Yaml

<strong># Zone-based weighting</strong>
zone_weights:
  title: 8.0        <strong># Disease in title = likely primary focus</strong>
  abstract: 6.0    <strong> # Disease in abstract = strong signal</strong>
  section_header: 5.0
  body: 2.0

<strong># Witness confirmation bonus</strong>
witness_bonus:
  gliner: 1.0       <strong># +1 if GLiNER also found it</strong>
  pubmedbert_ner: 1.0

<strong># Thresholds for role assignment</strong>
thresholds:
  primary_min_score: 12.0    <strong># Need title OR abstract + witness</strong>
  secondary_min_score: 6.0
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
Yaml

\textbf{# Zone-based weighting}
zone_weights:
  title: 8.0        \textbf{# Disease in title = likely primary focus}
  abstract: 6.0    \textbf{ # Disease in abstract = strong signal}
  section_header: 5.0
  body: 2.0

\textbf{# Witness confirmation bonus}
witness_bonus:
  gliner: 1.0       \textbf{# +1 if GLiNER also found it}
  pubmedbert_ner: 1.0

\textbf{# Thresholds for role assignment}
thresholds:
  primary_min_score: 12.0    \textbf{# Need title OR abstract + witness}
  secondary_min_score: 6.0

\end{Verbatim}
~~~




Scoring Formula:





~~~{=html}
<pre class="notion-ascii-diagram"><code><strong>score</strong> = Σ(zone_weight × mention_count) + Σ(witness_bonus) + confidence_bonus</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
\textbf{score} = Σ(zone_weight × mention_count) + Σ(witness_bonus) + confidence_bonus
\end{Verbatim}
~~~




---


## 4. Critical Reflection


### 4.1 Strengths of Final Architecture



<table>
  <thead>
    <tr>
      <th><strong>Aspect</strong></th>
      <th><strong>Evaluation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Throughput</strong></td>
      <td>25× improvement after switching to GPU-accelerated models</td>
    </tr>
    <tr>
      <td><strong>Contextual Understanding</strong></td>
      <td>Transformer embeddings enable negation/context awareness</td>
    </tr>
    <tr>
      <td><strong>Determinism</strong></td>
      <td>No stochastic elements; identical inputs produce identical outputs</td>
    </tr>
    <tr>
      <td><strong>Provenance</strong></td>
      <td>Every mention tracks back to word-level bounding boxes</td>
    </tr>
    <tr>
      <td><strong>Scalability</strong></td>
      <td>Parquet decoupling eliminates database write contention</td>
    </tr>
    <tr>
      <td><strong>CUI Linking</strong></td>
      <td>SapBERT normalisation provides equivalent capability to MedCat</td>
    </tr>
  </tbody>
</table>



### 4.2 Weaknesses and Risks



<table>
  <thead>
    <tr>
      <th><strong>Risk</strong></th>
      <th><strong>Mitigation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Single-model dependency</strong></td>
      <td>Witness models provide confirmation (require at least 1 witness)</td>
    </tr>
    <tr>
      <td><strong>Scope reduction</strong></td>
      <td>Title+abstract may miss diseases only mentioned in methods/results</td>
    </tr>
    <tr>
      <td><strong>Short-span ambiguity</strong></td>
      <td>Guardrails for short acronyms (2-5 chars) that normalise incorrectly</td>
    </tr>
    <tr>
      <td><strong>Model updates</strong></td>
      <td>Pre-trained models may become outdated; versioning required</td>
    </tr>
  </tbody>
</table>



### 4.3 Lessons Learned

1. **Infrastructure before algorithms** — The DuckDB bottleneck consumed more engineering time than model selection. Always prototype at scale early.
2. **Static vs contextual embeddings** — Word2Vec was revolutionary in 2013, but transformer-based models now represent the baseline for biomedical NER.
3. **Decoupling is key** — Separating NER inference from database writes enabled horizontal scaling.
4. **Witness confirmation** — Multiple models with different training regimes provide robust entity detection without requiring perfect agreement.

---


## 5. Next Steps


### 5.1 Recommended Architecture Change


Based on the findings above, BERN2 should be promoted to primary NER model with the following rationale:




<table>
  <thead>
    <tr>
      <th><strong>Criterion</strong></th>
      <th><strong>MedCat</strong></th>
      <th><strong>BERN2</strong></th>
      <th><strong>Decision</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Throughput</strong></td>
      <td>~2 papers/sec</td>
      <td>~50 papers/sec</td>
      <td>BERN2 ✅</td>
    </tr>
    <tr>
      <td><strong>Context awareness</strong></td>
      <td>Static embeddings</td>
      <td>Contextual embeddings</td>
      <td>BERN2 ✅</td>
    </tr>
    <tr>
      <td><strong>GPU utilisation</strong></td>
      <td>~5%</td>
      <td>~95%</td>
      <td>BERN2 ✅</td>
    </tr>
    <tr>
      <td><strong>CUI linking</strong></td>
      <td>Native</td>
      <td>Via SapBERT</td>
      <td>Equivalent 🟰</td>
    </tr>
    <tr>
      <td><strong>Model currency</strong></td>
      <td>2018 architecture</td>
      <td>2020+ architecture</td>
      <td>BERN2 ✅</td>
    </tr>
  </tbody>
</table>



Proposed Final Configuration:





~~~{=html}
<pre class="notion-ascii-diagram"><code>┌─────────────────────────────────────────────────────────────────────────────┐
│                        <strong>     NER 3xENSEMBLE (v2)      </strong>                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │ <strong>BERN2 (PRIMARY)</strong> ──────────────────────────────────────────────────► │   │
│   │     BioBERT-based transformer NER                                   │   │
│   │     GPU-accelerated, contextual embeddings                          │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │ <strong>GLiNER (WITNESS 1)</strong> ───────────────────────────────────────────────► │   │
│   │     Zero-shot NER, handles novel entity types                       │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │ <strong>PubMedBERT-NER (WITNESS 2)</strong> ───────────────────────────────────────► │   │
│   │     High-precision confirmation, complementary training             │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   <strong>POST-PROCESSING: </strong>                                                         │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │ SapBERT + FAISS ───► UMLS CUI normalisation                         │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre>
~~~

~~~{=latex}
\begin{Verbatim}[commandchars=\\\{\}]
┌─────────────────────────────────────────────────────────────────────────────┐
│                        \textbf{     NER 3xENSEMBLE (v2)      }                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │ \textbf{BERN2 (PRIMARY)} ──────────────────────────────────────────────────► │   │
│   │     BioBERT-based transformer NER                                   │   │
│   │     GPU-accelerated, contextual embeddings                          │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │ \textbf{GLiNER (WITNESS 1)} ───────────────────────────────────────────────► │   │
│   │     Zero-shot NER, handles novel entity types                       │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │ \textbf{PubMedBERT-NER (WITNESS 2)} ───────────────────────────────────────► │   │
│   │     High-precision confirmation, complementary training             │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   \textbf{POST-PROCESSING: }                                                         │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │ SapBERT + FAISS ───► UMLS CUI normalisation                         │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

\end{Verbatim}
~~~




### 5.2 Remaining Work

1. ~~**SPECTOR2 Integration**~~ ~~— Use document embeddings to validate disease-feature associations~~ # not enough time
2. **UK Biobank Showcase Mapping:** Link normalised CUIs to UKB field IDs
3. **Elastic Search Index:** Enable semantic search over 10,000 UKB features
4. **Human-in-the-Loop Validation:** Review low-confidence primary disease assignments

---


## Appendix: Model Selection Rationale


### A.1 Why BERN2 as Primary


BERN2 (Biomedical Entity Recognition and Normalisation) represents the current state-of-the-art for biomedical NER:


- **Architecture:** BioBERT encoder with multi-task NER heads
- **Training:** PubMed abstracts + PMC full-text articles
- **Entity types:** Diseases, genes, chemicals, mutations, species
- **Normalisation:** Optional neural normalisation via SapBERT

### A.2 Why GLiNER as Witness


GLiNER provides zero-shot NER capability, valuable for:


- Recognising disease synonyms not in BERN2's training vocabulary
- Handling table text where BERT tokenisation often fails
- Detecting emerging disease terminology

### A.3 Why PubMedBERT-NER as Witness


PubMedBERT-NER offers:


- Training on the full PubMed corpus (30M+ abstracts)
- Different tokenisation and training regime than BERN2
- Conservative predictions that reduce false positive rate

The combination of these three models provides robust disease extraction with built-in redundancy.


