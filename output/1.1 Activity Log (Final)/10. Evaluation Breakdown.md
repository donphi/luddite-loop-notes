# Evaluation Breakdown


## Content


## 1. Objective


The objective of this experimental block was to design, evaluate, and stress-test a scalable, GPU-accelerated document analysis workflow capable of transforming heterogeneous biomedical PDFs into structured, ontology-ready JSON. This study aimed to establish the architectural foundations for a production-grade, multi-pipeline system supporting disease tagging, feature extraction, and downstream hypothesis testing using the UK Biobank literature.  This study was motivated by an early research question explored in a separate statistical analysis: whether UK Biobank research exhibits a stable core feature set across studies. The formal hypothesis and preliminary results are documented in Section 2.‚úÖ Results and Reflection



The work focused on:


- verifying whether multiple open-source models can be integrated deterministically without collapse at scale
- identifying where extraction pipelines fail under real-world noise
- establishing the design constraints for a future production-grade, multi-pipeline system
- benchmarking GPU performance under sustained, multi-model workloads

This requires building a reproducible computational environment, unifying extraction standards, and designing the early stages of a continuous research pipeline.







## 2. Methods and Process Summary


A 121-day iterative workflow was executed between 16 May 2025 and 14 September 2025 with 46 dedicated coding days and 188 hours of active development.16 May 202514 September 2025 The focus was rapid prototyping, architectural testing, model comparison, and stress-testing GPU workloads under real-world corpus noise.



The process included:


- multi-source PDF retrieval and metadata consolidation
- PDF ‚Üí Markdown/JSON conversion testing Surya OCR, Docling, Marker, PyMuPDF, and GROBID
- evolution of a YAML-driven hyperparameter controls
- multi-model section, header, and topic detection
- large-scale NER testing (HunFlair2, PubTator3, BERN2, SciSpacy)
- ontology audits across MONDO, MeSH, DOID, LOINC and HPO
- early architecture choices for _‚Äútoken-spine‚Äù_ alignment and model redundancy
- evaluation of *RAG-related chunking constraints (sentence-level vs passage-level)

    * Retrieval Augmented Generation was initial considered but was eventually rejected 
    

- exploratory feature synonym generation via Elasticsearch

The activity timelines and metadata remain preserved in the structured table below (notes removed intentionally for academic clarity).







üí° A token is the smallest unit of text identified by an Optical Character Recognition (OCR) system.token A token is typically a word (or word-like segment) paired with its exact bounding box coordinates on a page.token It is the atomic building block for reconstructing document structure.‚Äù



üí° The token spine is a canonical, page-ordered sequence of all OCR tokens (with coordinates) that serves as a single source of truth for text content and layout. All downstream tasks, NER, ontology mapping, chunking, and section detection, anchor back to this spine to ensure deterministic alignment.







### Time Analysis



<table>
  <tbody>
    <tr>
      <td><strong>Metric</strong></td>
      <td><strong>Value</strong></td>
    </tr>
    <tr>
      <td><strong>Experiment Duration</strong></td>
      <td>121 total days</td>
    </tr>
    <tr>
      <td><strong>Start Date</strong></td>
      <td>2025-05-16 (Thursday)</td>
    </tr>
    <tr>
      <td><strong>End Date</strong></td>
      <td>2025-09-14 (Friday)</td>
    </tr>
    <tr>
      <td><strong>Coding Duration</strong></td>
      <td>46 total days</td>
    </tr>
    <tr>
      <td><strong>Coding Start Date</strong></td>
      <td>2025-08-07</td>
    </tr>
    <tr>
      <td><strong>Coding End Date</strong></td>
      <td>2025-09-14</td>
    </tr>
    <tr>
      <td><strong>Screen Time</strong></td>
      <td>188 hours</td>
    </tr>
    <tr>
      <td><strong>Experiments Completed</strong></td>
      <td>24</td>
    </tr>
  </tbody>
</table>



The experiment began on 2025-05-16 and concluded on 2025-09-14, with concentrated coding work commencing from 2025-08-07.2025-05-162025-09-142025-08-07 The timeline below documents the iterative development of the pipeline, evolution of the extraction strategy, and emergence of stable design principles.2025-05-162025-09-142025-08-07







---






Untitled


All internal notes were removed; the table represents the high-level progression of the experiment. See link belowlink for access to granular notes.



https://fragrant-mapusaurus-1cb.notion.site/24db39f042938123a4a4c518037b63c2?v=24db39f0429381dc8e5b000cf4d04620&source=copy_link







---


## 3. Critical Reflection


This experiment exposed the full complexity of constructing a scalable scientific PDF pipeline capable of supporting downstream ontology-linked biomarker research. Several observations were consistent across the entire workflow.


1. **Environmental determinism is non-negotiable.**

    Models that work in an ad hoc environment fail under container orchestration unless the dependencies are locked and the CUDA stacks are uniform.
    

2. **The PDF structure is the primary bottleneck.**

    The variability in layout, encoding, and OCR quality remains the dominant source of extraction errors, not the models themselves. A more structured pipeline and process must be considered.
    

3. **Single-model extraction was not viable.**

    Marker, Docling, Surya, GROBID, and NER models each solve different failure modes. Reliability only emerges from the model-level redundancy. There is a need to examine the pros and cons of each model in the pipeline before construction.
    

4. **Journal Structure and Ontology normalisation require strict discipline.**

    Without a unified ‚Äútoken-spine‚Äù and repeatable tagging logic, biomedical entity normalisation collapses at a large scale. Consider fine-tuning with SetFit, which is
    

5. **Scaling introduces new failure modes in the system.**

    Techniques that work on 100 PDFs fail when pushed to thousands due to the index size, chunk explosion, and GPU memory constraints.
    

6. **The pipeline is now mature enough to be formalised into three continuous research pipelines.**

    The experiment revealed the boundaries of feasibility and motivated the development of a modular pipeline architecture.
    






---


## 4. Next Steps and Implementation Plan


The next phase formalises the system into three continuous, validated pipelines, each architected for reproducibility and large-scale processing.



### 4.1 Pipeline: PDF ‚Üí Clean JSON (Canonical Extraction Pipeline)


Goal: Produce a deterministic JSON representation of every paper backed by the ALTO XML ‚Äútoken spine.



Stages:


1. PDF ingestion ‚Üí Surya OCR + ALTO XML
2. Multi-model structure detection (Marker, Docling, GROBID, Layout cues)
3. Markdown + JSON formation
4. End-to-end validation:
    - header structure
    - section ordering
    - table/figure detection
    - text continuity
    - metadata consistency

Outcome: A stable, compressed, ontology-ready JSON dataset was stored in a columnar database.







### **4.2 Pipeline 2: Clean JSON ‚Üí NER & Disease Ontology Mapping**


Goal: Apply multiple biomedical NER models to every sentence to assign ontology IDs.



Stages:


1. Sentence-level chunking (atomic spans)
2. Run 3 disease NER models (e.g., BERN2, HunFlair2, MedCAT)
3. Voting system: 3-way witness, no ties
4. Mapping to MONDO, HPO, MeSH, SNOMED, DOID (Possibly only DOID as time pressured)
5. Validation:
    - inter-model agreement
    - ontology ID coverage
    - unresolved entities flagged

Outcome: A disease-entity dataset supporting phenotype clustering, biomarker linkage, and ontology-driven hypothesis generation.







### **4.3 Pipeline 3: Clean JSON ‚Üí Feature Extraction (Keywords, Synonyms, Metadata)**


Goal: Extract every feature relevant to the UK Biobank modelling.



Stages:


1. Elasticsearch synonym expansion
2. Reverse indexing for field detection
3. Additional NLP models for non-disease entities
4. Mapping to UKB field IDs
5. Validation:
    - duplicate features
    - false positives/negatives
    - feature frequency distribution

Outcome: A harmonised feature dataset enabling scalable hypothesis testing and cross-paper feature overlap analysis.







### **4.4 Final Statistical Pipeline (Cross-Pipeline Output)**


Goal: To determine whether 10,000 initial UK Biobank features reduce to stable, repeatedly selected subsets.



Tools:


- Bootstrapping
- Stability selection
- Mutual information
- Sparse logistic regression
- Cluster analysis of ontology-normalised features

Deliverables:


1. Feature stability curves
2. Rank-ordered feature lists
3. Evidence of convergent feature selection across models and papers




















