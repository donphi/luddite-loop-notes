# Original JSON Tags


## Content


### Objective


We needed to start compiling and adding the features to the 6,411 JSON sentence chunks we just created. The original JSON files we initially created when we converted the PDFs into MD files had important metadata we could copy over. These metadata needed to be copied over to the document information section of the newly create JSON sentence chunks.  







---


### **Methods**


Data Architecture



The pipeline operates on paired JSON datasets, keyed by filenames:


- **Document chunks**
    - (`/data/sentence_chunk_json/ *.json`): 6,411 files containing the individual sentences chunked and JSON skeleton template.
- **Bibliographic metadata**
    - (`/data/original_json/*.json`): 6,411 corresponding files containing the document section JSON data





Processing



We deployed a simple script to use 16 parallel workers on the 48-core system, CPU, achieving processing speeds of 89.4 files/second. The architecture implements:
- Multiprocessing with spawn method for Docker compatibility
- Thread-safe file locking mechanisms
- Batch processing with automated load balancing







### **Metadata**


Each document chunk file contains multiple segment groups. This segment was for the document data, which included 11 standardised fields:







Document Section



```markdown
# Metadata Schema Overview

| Category               | Fields                                                   |
|------------------------|----------------------------------------------------------|
| **Core bibliographic** | `title`, `abstract`, `authors`, `journal`, `year`        |
| **Identifiers**        | `doi`, `pmid`, `url`                                     |
| **Metrics**            | `citations_count`, `match_confidence`                    |
| **Validation scores**  | `extraction_quality_score`, `extraction_completeness`    |
```






---


### **Quality Control Analysis**


We implemented comprehensive validation incorporating:
1. Type validation : Enforcing strict typing (str, list, int, float)
2. HTML contamination detection: Regex-based identification of 97 unique HTML entities and 23 tag types
3. Data completeness verification: Null value detection and field presence validation







---


### **Results**


```markdown
# Processing Summary
| Category               | Count  | Percentage |
|------------------------|--------|------------|
| Successfully processed | 6,376  | 99.45%     |
| Failed extraction      | 35     | 0.55%      |
| **Total**              | 6,411  | 100.00%    |

---

# Cleaning Stats
| Metric                   | Value      |
|--------------------------|------------|
| Total chunks updated     | 2,027,573  |
| Total fields cleaned     | 3,633,821  |
```






---


### **Failure Analysis**


Diagnostic investigation revealed 35 files with catastrophic metadata extraction failure, characterized by:


- Absence of bibliographic fields (0% coverage)
- Presence of only filesystem metadata (`filename`, `content`)
- Complete loss of structured publication data

Distribution analysis showed no systematic bias in failure patterns, indicating that errors were stochastic in nature. In other words, the extraction failures appeared random and non-deterministic rather than reproducible flaws linked to the pipeline itself, supporting the conclusion that no systematic processing issue was present.







---


### **Data Cleaning Efficacy**


We found some cleaning inconsistencies and created the following remedies:




```markdown
# Tagging Removal
Before (raw tags)     → After (cleaned)
---------------------------------------
<span>                → [removed]
</span>               → [removed]
<i>                   → [removed]
...

# Cleanup Stats
**HTML entities:** 97 unique entities across 40,627 occurrences  
**HTML tags:** Removed 34,540 field instances in total
```


```markdown
# Type Conversions
Before (string)       → After (numeric)
----------------------------------------
"00123"               → 123
"45.600"              → 45.6
"7,890"               → 7890
"0.000"               → 0
...

# Conversion Stats
***Type conversions:*** 12,304 successful numeric type corrections
```


```markdown
# Pipe Fixes
Before                         → After
---------------------------------------
J. Smith | A. Bellingham       → J. Smith, A. Bellingham
M. Taylor | K. Johnson         → M. Taylor, K. Johnson
...

# Author Standardisation
Standardized 2,841 pipe-delimited author strings to list format.
```






---


### **Validation**


During quality control analysis, we identified a critical data integrity issue affecting 0.55% (n=35) of the JSON files, where metadata extraction had failed during upstream processing. We developed diagnostic tools and an archival system to maintain data quality. We found that the n=35 original JSON files had not a single meta data sample available for copy. We noticed that these didnt even inlcude the important dois numbers! We have yet to find the reason for this loss. The n=35 only made up 0.55% of the data and we decided for the sake of time not to pursue the loss. 







---


### Reflection


The 0.55% failure rate, while minimal, represents a significant data quality concern given the biomedical nature of the corpus. Constant iterations after non-uniform PDF extractions can create erratic behavior on larger data sizes. These failures appear to originate from the metadata extraction layer, potentially due to:



1. PDF parsing errors in heavily formatted documents
2. Non-standard publication layouts
3. Encoding issues with special characters in filenames





The parallel processing architecture demonstrated linear scalability up to 16 workers allowed for a throughput of 89.4 files/second presenting a 12.3x improvement over sequential processing.










