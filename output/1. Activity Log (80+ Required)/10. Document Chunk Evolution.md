# Document Chunk Evolution


## Content


### Objective


Convert UK Biobank Markdown files into structured, semantically meaningful chunks for NER tagging. Preserve document hierarchy, section context, and keep tables atomic. The enhanced pipeline was built to improve performance, GPU utilization, and section normalization while fixing limitations in the original design.







---


### Process

1. **Original Approach (1_chunk)**
    - Two-step pipeline:
        - `unstructured_chunker.py` → parse Markdown with Unstructured.
        - `semantic_chunker.py` → chunk text blocks with LlamaIndex.
    - Section tracking minimal, no hierarchy.
    - Sequential execution, no GPU optimization.
2. **Enhanced Approach (1_chunk_enhanced)**
    - Unified script: `enhanced_chunker.py` / `enhanced_chunker_gpu.py`.
    - Hierarchical section tracking + section type classification.
    - GPU batch processing with parallel workers.
    - Section normalization via `normalise_sections.py`.
    - YAML metadata headers handled cleanly.
    - Robust error reporting and recovery.
3. **Key Libraries**
    - **Unstructured** – document parsing and element extraction.
    - **LlamaIndex** – semantic chunking with embeddings.
    - **HuggingFace** – embeddings (`BAAI/bge-small-en-v1.5`).
    - **PyTorch** – GPU acceleration.
    - **Docker** – containerized environment.
4. **Processing Statistics**
    - Successfully processed 6,411 files with 0 failures
    - Generated 1,349,914 total chunks
    - Average of ~210 chunks per document
    - 100% completion rate with no missing files

    



---


### Validation


Performance


- Batch GPU processing improved throughput.
- Parallel workers kept GPU fully utilized.
- Large datasets processed with progress tracking + resumable runs.

Quality


- Hierarchical section tracking preserved document context.
- Section normalization standardized headings.
- YAML headers validated and preserved.
- Error handling produced actionable reports.

Technical


- Optimized for RTX 6000 Pro with CUDA configs.
- Batch execution improved memory use.
- Docker configs updated with proper env vars.





---


### Reflection


Original Limitations


- Sequential → slow on large corpora.
- Poor GPU utilization, no batching.
- Section tracking flat, no hierarchy.
- Section names inconsistent without normalization.
- YAML headers ignored. Redid the code for enhancement.
- Limited error handling.
- Two-step process created unnecessary intermediates.
- No recovery for failed files.

Enhanced Fixes


- Unified pipeline, batch GPU processing.
- Hierarchical section handling + normalization.
- Proper YAML header support.
- Stronger error handling and resumable execution.
- Optimized containerization for reproducibility.





---


### Next Step


Proceed to NER tagging with HunFlair in data_conversion/ukb/4_tagging/2_tag.



Enhanced chunks become input for biomedical NER to extract diseases, genes, drugs, and other key entities.


