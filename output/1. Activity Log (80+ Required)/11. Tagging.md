## Activity Log (80+ Required) â€” Entry 11: Tagging

**Next Steps:** The next step involves clearing the cache every 500 files processed to efficiently manage GPU memory usage and prevent out-of-memory crashes.  
**Category:** ðŸ§¹ Data Preprocessing  
**Summary:**  
The document discusses the challenges encountered while processing files for tagging using the HunFlair2 tool, including a HTTP 403 error and a NumPy compatibility issue. The author implemented a caching strategy to manage memory usage effectively and avoid out-of-memory crashes during the processing of a large number of files.  

**Entry:** Tagging  

## Content


with the pre chunking I installed 0.15.7 for unstruct and was constantly getting a HTTP permissions 403 error, I checked and the documentation mentioned that anything above 0.16.3 would fix the error. I decided to try the newer version, less stable, but wanted to ensure it fixes the issue for sure which was 0.18.13







Finally was able to make it work by downloading the model to cache and forcing it to check the cached model before looking to huggingface







```bash
Processing files:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 3660/6411 [06:13<04:43,  9.69it/s]2025-08-17 18:52:45,722 - __main__ - INFO - Processing: 2021_Nouwen_j_jad_2021_05_031_2021_Nouwen_1010.md
2025-08-17 18:52:45,807 - __main__ - INFO - Extracted 163 elements from 2021_Nouwen_j_jad_2021_05_031_2021_Nouwen_1010.md
2025-08-17 18:52:45,811 - __main__ - INFO - Created 45 chunks from 2021_Nouwen_j_jad_2021_05_031_2021_Nouwen_1010.md
2025-08-17 18:52:45,823 - __main__ - INFO - Saved 45 chunks to /app/data/ukb/json_chunk_tag/2021_Nouwen_j_jad_2021_05_031_2021_Nouwen_1010_chunks.json
2025-08-17 18:52:45,823 - __main__ - INFO - Processing: 2022_Esme_jns_2022_96_2022_Tuttiett_2022.md
2025-08-17 18:52:45,883 - __main__ - INFO - Extracted 148 elements from 2022_Esme_jns_2022_96_2022_Tuttiett_2022.md
```



based on the following paper we used hunflr of our tagging:




Data and text mining
HunFlair2 in a cross-corpus evaluation of biomedical
named entity recognition and normalization tools



Bioinformatics, 2024, 40(10), btae564
https://doi.org/10.1093/bioinformatics/btae564
Advance Access Publication Date: 20 September 2024
Original Pape







```bash
Processing files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6411/6411 [10:57<00:00,  9.75it/s]
2025-08-17 18:57:30,198 - __main__ - INFO - ============================================================
2025-08-17 18:57:30,198 - __main__ - INFO - Processing complete!
2025-08-17 18:57:30,198 - __main__ - INFO - Successful files: 6411
2025-08-17 18:57:30,198 - __main__ - INFO - Failed files: 0
2025-08-17 18:57:30,198 - __main__ - INFO - Total chunks created: 349446
2025-08-17 18:57:30,198 - __main__ - INFO - Average chunks per file: 54.5

==========================================
Unstructured chunking complete!
Output saved to: data/ukb/json_chunk_tag/
```




tagging:




```bash
## NumPy Compatibility Error in HunFlair2 Tagger

### Error Summary
Running `./test_tagging.sh` fails for both **with** and **without normalization** due to a **NumPy binary incompatibility** error:

```bash
ValueError: numpy.dtype size changed, may indicate binary incompatibility. 
Expected 96 from C header, got 88 from PyObject
```

### Explanation
This occurs when **NumPy** and a compiled dependency (here `thinc`/`spacy`) are built against different NumPy versions or ABI headers.  
The model loads correctly, but execution fails at tokenizer initialization.  

**Impact:**  
- No tagged output is generated (`output_with_norm` and `output_no_norm` are missing).  
- Cleanup partially fails due to permission issues.  

**Fix (typical):**  
Rebuild or reinstall the affected libraries against the installed NumPy version (e.g., `pip install --force-reinstall --no-cache-dir numpy spacy thinc`).
```






realised I was running at 88gb of full memory, the pytorch needed to be cleared, I decided on a arbatory number of 500 clear the cache at each 500 files that way it smore efficienct and wont runn in against memory issues Oout of Memory crashes

I was 50% in when I saw the 88gb of ran in the gpu


