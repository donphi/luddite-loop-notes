# Sentence Chunk


## Content


### Objective


Extract and standardize sentences, tables, and equations from Markdown into JSON, preparing them for tagging/vectorization. The aim was to reduce loss and improve accuracy across iterations.







---


### Method

- **Corpus & conversion:** 6.4M Markdown → JSON.
- **Sentence extraction (iterative):** baseline splitter → **regex** → **BioBerg** → **SaT (SaT-12l-sm)** once GPU hardware allowed, giving materially better segmentation.
- **Tables:** initially preserved as Markdown; later **converted to plain text + dataframes** for better downstream model contextualization.
- **Validation:** compared parsed sentence counts to Markdown baselines each run; audited tables/equations; fixed counting + table collapse defects.





---


### Reflection (what changed and why)

- **Counting bug fixed:** early runs collapsed entire tables to a single header, depressing recall.
- **Segmentation upgrade:** **SaT-12l-sm** captured scientific boundary cases (abbreviations, citations, parentheses) with higher fidelity.
- **Tables re-handled:** conversion to **text/dataframes** meant models could parse rows/columns as real evidence instead of flat blocks.
- **Trade-offs:** most permissive config retained the most, balanced config cut noise but lost recall—both documented.





---


### Results of Improvement (sentence retention)


| Run / Validation
 | Files
 | Original Sentences
 | Parsed Sentences
 | Retention %
 | Key change
                                         |
| ----------------- | ------ | ------------------- | ----------------- | ------------ | --------------------------------------------------- |
| Run 1
            | 6,411
 | 3,284,031
          | 1,110,886
        | 36.3%
       | Baseline; strict filters; table/header bug present
 |
| Run 2
            | 6,411
 | 2,305,167
          | 1,326,904
        | 62.2%
       | Fixes + better heuristics; partial recovery
        |
| Run 3
            | 6,412
 | 2,305,546
          | 2,037,844
        | 93.9%
       | SaT-12l-sm + corrected counting + table handling
   |
| Run 4
            | 6,377
 | 2,294,005
          | 1,247,711
        | 58.1%
       | “Balanced” config; reduced noise at cost of recall
 |


Net effect: peak +57.6 percentage points (36.3% → 93.9%). Tables and equations also recovered substantially once header-only collapses were eliminated.







---


### Pivot & Revised Hypothesis (change of direction)


Why pivot: Time-boxed. Instead of completing full RAG, sentences were used directly to match against UK Biobank feature names/synonyms, enabling a clean test of feature convergence without cross-cohort integration.



Question:



Do UK Biobank studies converge on a small, stable “core” (age, sex, BMI, BP, smoking, etc.) across diseases, or do different fields fragment into divergent subsets?


- **H0 (null):** usage is fragmented, no consistent core.
- **H1 (alt):** a statistically significant **core–periphery** structure exists: a small recurring core vs silo-specific peripherals.

Value: Tests whether Biobank science genuinely breaks silos. Systems-level, matches the sentences+tables→dataframe pipeline, avoids cross-cohort dependency.







---


### New Insight: SLATE (Sentence-Level Annotation & Tagging Engine)


Earlier chunking for tagging exposed a flaw: once multiple sentences were fused into a chunk, overlap strategies broke JSON structure (overlapping chars cut sentences mid-way, splitting context and invalidating structured JSON).



SLATE solves this by working at the sentence level, so annotation/tagging is preserved per-sentence, and then chunks are built by stitching complete sentences. That means:


- **JSON context stays intact** (per-sentence).
- **Chunk overlap happens at sentence boundaries**, not chars.
- **Vectorization pipeline improves** since context can be fused cleanly, and sentence-level annotations can simply be collated.

This was a critical design revelation. Sentence-first processing makes downstream tagging/vectorization stable, reproducible, and extensible.



---






### Technical Note: Table Handling


The custom extractor (see diary code) ensured 100% table capture, deduplication, and conversion into:


- **raw markdown** (for reproducibility),
- **plain text** (for model context),
- **structured dataframe** (for matching Biobank fields).

This meant feature mentions embedded in tables were no longer lost—boosting both recall and accuracy for tagging.


