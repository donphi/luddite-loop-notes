## Activity Log (80+ Required) â€” Entry 4: Final PDF Collection

**Hours:** 3  
**Date:** 2025-08-05  
**Next Steps:** Proceed with metadata verification and prepare the cleaned dataset for semantic search and feature extraction.  
**Category:** ðŸ“˜ Data Collection  
**Summary:**  
The objective was to validate and consolidate a dataset of PDFs by removing duplicates, resulting in a final unique set of 6,841 PDFs. The process proved essential for ensuring the dataset's reliability for future analyses, despite ~9% of publications remaining inaccessible.  

**Related Experiments:**  
- PDF Download Pipeline  

**Nr:** 4  
**Phase:** Phase 1: Data Collection & Setup  
**Progress:** âœ… Complete  
**Entry:** Final PDF Collection  

## Content


Objective



To validate and consolidate the collected PDFs by removing duplicates, ensuring the dataset is reliable for downstream analysis.



Process


- Developed a script to calculate the **checksum** of each PDF.
- Identified duplicate files, some occurring two or three times.
- Removed duplicates to prevent over-representation that could distort semantic search outputs.
- Results:
    - Initial total: **8,528** PDFs
    - Located: **7,773** PDFs (~755 missing)
    - Duplicates removed: **932**
    - Final unique set: **6,841** PDFs

Reflection



Deduplication proved essential: 452 files were present in multiple copies, which could have introduced noise and biased analyses. By validating against checksums rather than filenames, the process ensured accuracy and reproducibility. Although ~9% of publications remain inaccessible, the final dataset is sufficiently large and reliable for meaningful downstream analysis.



Next Step



Proceed with metadata verification and preparation of the cleaned dataset for semantic search and feature extraction.


