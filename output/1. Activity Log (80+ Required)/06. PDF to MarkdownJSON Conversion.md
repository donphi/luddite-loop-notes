# PDF to Markdown/JSON Conversion


## Content


```markdown
## Objective
Convert ~6,600 UK Biobank PDFs into structured Markdown + JSON. Preserve text, tables, and metadata. This is the foundation for validation, tagging, and feature extraction.

---

## Process

### 1. PDF Collection & Prep
- ~6,600 PDFs in batches (`batch_001`–`batch_010`).
- Metadata pre-matched in `0_meta_match`.
- Batch approach avoids GPU overload and eases recovery.
- Filenames normalized with `sed`/`tr`.

### 2. Conversion Infrastructure
- **Docker**: Python 3.10 + CUDA 12.8. Orchestration via `docker-compose.gpu.test2.yml`. Volumes for persistence + cache.
- **GPU**: RTX 6000 Blackwell, 96GB VRAM. PyTorch 2.2+ with cuDNN. Memory fraction cap set per process.
- **Marker v1.8.3**: hybrid text extraction, OCR fallback (Tesseract + Surya), tables → Markdown, image embedding.
- **Python stack**: torch, fitz (PyMuPDF), tqdm, multiprocessing, subprocess, regex, yaml, hashlib.

### 3. Optimization & Testing
- Best config: batch size 128, 4 workers.
- ~6.5s per PDF, ~77% GPU util, ~11.5h total.
- GPU env tuned:
   - CUDA_VISIBLE_DEVICES=0
   - TORCH_DEVICE=cuda
   - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
   - OMP_NUM_THREADS=4
   - MKL_NUM_THREADS=4
   
 ### 4. Conversion Details
- **Text**: PyMuPDF for digital PDFs, Marker OCR fallback with confidence thresholding.
- **Tables**: Marker table model → Markdown with row/col + merge preserved.
- **Metadata**: priority external `matches.json`; fallback PyMuPDF info, font heuristics, regex (DOI/PMID/dates).
- **Output**: `.md` with YAML frontmatter + `.json` with structured schema.

### 5. Error Handling
- `ProcessingIndex` checkpoints processed files.
- Recovery via `final_convert_missing.sh`.
- Batch GPU resets; subprocess timeouts; detailed logs.
- Containers stopped + removed after runs.

### 6. File Organization
- `/data/ukb/md/` → Markdown  
- `/data/ukb/json/` → JSON  
- Filenames: year + author + DOI/PMID (fallback: MD5).  
- JSON schema includes metadata + markdown content.

---

## Validation

- **Quality**: manual checks, hybrid vs OCR, metrics = word count, tables, runtime.  
- **Metadata**: ~5,000 with external metadata; ~1,600 extracted internally; ~5,800 with DOI/PMID; ~4,200 contain tables.  
- **Performance**: ~6.5s/PDF, ~77% GPU util, ~24GB VRAM used, >90% success rate.  
- **Recovery**: 296 missing PDFs reprocessed + validated.

---

## Reflection

- **Challenges**: GPU OOM risks, thread contention, inconsistent PDF formats, missing metadata.  
- **Insights**: batch size drives throughput; 4 workers optimal; hybrid extraction beats OCR-only; checkpoints essential.  
- **Limits**: complex layouts still break tables; metadata from PDFs unreliable; processing times vary.  
- **Improvements**: robust recovery scripts, GPU tuning for Blackwell, extended metadata fallback, detailed logs.

---

## Next Step
Run validation/deduplication in `data_conversion/ukb/2_validation_md_json/`:
1. Detect duplicates via DOI.  
2. Score quality + completeness.  
3. Keep best copy, quarantine others.  
4. Deliver a clean dataset for feature extraction, synonym mapping, and tagging.

---

## Technical Implementation

- **Classes**:  
- `BlackwellOptimizedConverter`: orchestrates conversion, GPU settings, metadata, outputs.  
- `ProcessingIndex`: checkpoints with thread-safe ops.  
- `MedicalIdentifierExtractor`: regex for DOI/PMID/PMCID/CTIDs.  
- `ModelCacheManager`: manages cache lifecycle.  

- **Functions**:  
- `_convert_with_marker()`: GPU-enabled Marker call.  
- `_extract_metadata_from_markdown()`: multi-strategy fallback extraction.  
- `_process_batch()`: parallel pool with staggered start.  
- `_generate_filename()`: safe unique file naming.  

- **Scripts**:  
- `final_convert_resume.sh`: resume batches with GPU reset + cleanup.  
- `final_convert_missing.sh`: isolate + process missing PDFs, validate, merge.
```


### Objective


Convert ~6,600 UK Biobank PDFs into structured Markdown + JSON. Preserve text, tables, and metadata. This is the foundation for validation, tagging, and feature extraction.







---


### Process


1. PDF Collection & Prep


- ~6,600 PDFs in batches (`batch_001`–`batch_010`).
- Metadata pre-matched in `0_meta_match`.
- Batch approach avoids GPU overload and eases recovery.
- Filenames normalized with `sed`/`tr`.

2. Conversion Infrastructure


- **Docker**: Python 3.10 + CUDA 12.8. Orchestration via `docker-compose.gpu.test2.yml`. Volumes for persistence + cache.
- **GPU**: RTX 6000 Blackwell, 96GB VRAM. PyTorch 2.2+ with cuDNN. Memory fraction cap set per process.
- **Marker v1.8.3**: hybrid text extraction, OCR fallback (Tesseract + Surya), tables → Markdown, image embedding.
- **Python stack**: torch, fitz (PyMuPDF), tqdm, multiprocessing, subprocess, regex, yaml, hashlib.

3. Optimization & Testing


- Best config: batch size 128, 4 workers.
- ~6.5s per PDF, ~77% GPU util, ~11.5h total.
- GPU env tuned:
    - CUDA_VISIBLE_DEVICES=0
    - TORCH_DEVICE=cuda
    - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    - OMP_NUM_THREADS=4
    - MKL_NUM_THREADS=4

4. Conversion Details


- **Text**: PyMuPDF for digital PDFs, Marker OCR fallback with confidence thresholding.
- **Tables**: Marker table model → Markdown with row/col + merge preserved.
- **Metadata**: priority external `matches.json`; fallback PyMuPDF info, font heuristics, regex (DOI/PMID/dates).
- **Output**: `.md` with YAML frontmatter + `.json` with structured schema.

5. Error Handling


- `ProcessingIndex` checkpoints processed files.
- Recovery via `final_convert_missing.sh`.
- Batch GPU resets; subprocess timeouts; detailed logs.
- Containers stopped + removed after runs.

6. File Organization


- `/data/ukb/md/` → Markdown
- `/data/ukb/json/` → JSON
- Filenames: year + author + DOI/PMID (fallback: MD5).
- JSON schema includes metadata + markdown content.





---


### Validation

- **Quality**: manual checks, hybrid vs OCR, metrics = word count, tables, runtime.
- **Metadata**: ~5,000 with external metadata; ~1,600 extracted internally; ~5,800 with DOI/PMID; ~4,200 contain tables.
- **Performance**: ~6.5s/PDF, ~77% GPU util, ~24GB VRAM used, >90% success rate.
- **Recovery**: 296 missing PDFs reprocessed + validated.





---


### Reflection

- **Challenges**: GPU OOM risks, thread contention, inconsistent PDF formats, missing metadata.
- **Insights**: batch size drives throughput; 4 workers optimal; hybrid extraction beats OCR-only; checkpoints essential.
- **Limits**: complex layouts still break tables; metadata from PDFs unreliable; processing times vary.
- **Improvements**: robust recovery scripts, GPU tuning for Blackwell, extended metadata fallback, detailed logs.





---


### Next Step


Run validation/deduplication in data_conversion/ukb/2_validation_md_json/:


1. Detect duplicates via DOI.
2. Score quality + completeness.
3. Keep best copy, quarantine others.
4. Deliver a clean dataset for feature extraction, synonym mapping, and tagging.





---


### Technical Implementation

- **Classes**:
- `BlackwellOptimizedConverter`: orchestrates conversion, GPU settings, metadata, outputs.
- `ProcessingIndex`: checkpoints with thread-safe ops.
- `MedicalIdentifierExtractor`: regex for DOI/PMID/PMCID/CTIDs.
- `ModelCacheManager`: manages cache lifecycle.
- **Functions**:
- `_convert_with_marker()`: GPU-enabled Marker call.
- `_extract_metadata_from_markdown()`: multi-strategy fallback extraction.
- `_process_batch()`: parallel pool with staggered start.
- `_generate_filename()`: safe unique file naming.
- **Scripts**:
- `final_convert_resume.sh`: resume batches with GPU reset + cleanup.
- `final_convert_missing.sh`: isolate + process missing PDFs, validate, merge.
