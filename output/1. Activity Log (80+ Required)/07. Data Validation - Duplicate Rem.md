## Activity Log (80+ Required) â€” Entry 8: Data Validation - Duplicate Rem

**Hours:** 2  
**Date:** 2025-08-08  
**Next Steps:** Proceed to extract UK Biobank-specific features from the cleaned, deduplicated dataset.  
**Category:** ðŸ§¹ Data Preprocessing  
**Summary:**  
The document outlines a data validation process for removing duplicate JSON/Markdown file pairs based on DOI matching and quality scoring. It identifies 303 duplicate groups, prioritizes keeping the highest quality files, and confirms that 67.4% of the retained files scored as High or Perfect in quality.  

**Nr:** 8  
**Entry:** Data Validation - Duplicate Rem  

## Content


```markdown
# UKB Data Validation â€“ Duplicate Removal

## Objective
Identify and remove duplicate JSON/Markdown file pairs using DOI matching and metadata quality scoring. This step ensures data integrity by eliminating redundant entries while keeping only the highest-quality version of each document.

---

## Process

1. **Duplicate Detection**
   - All JSON files (main + missing directories) analyzed with `metadata_analyser.py`.  
   - DOIs normalized to ISO 26324 standards using `robust_doi_matcher.py`.  
   - Found **303 duplicate DOI groups** with multiple files pointing to the same DOI.  

2. **Quality-Based Selection**
   - Each duplicate group scored on:  
     - `extraction_quality_score` (0â€“1).  
     - `extraction_completeness` (0â€“1).  
     - `doi_exact_match` (boolean).  
   - Scores combined into a composite metadata score.  
   - Highest scoring file kept; others flagged for removal.  

3. **File Management**
   - Duplicate files moved to a backup directory at `/data/ukb/json_validation_issue/`.  
   - Scripts generated directory structures, validated moves, and logged all actions.  
   - Safety checks prevented accidental overwrites.  

---

## Validation

Comparison of quality distribution between kept vs deleted files:

| Quality Level | Files Kept | % of Kept | Files Deleted | % of Deleted |
|---------------|------------|-----------|---------------|--------------|
| Perfect (EQ=1.0, EC=1.0, DOI=True) | 28 | 12.8% | 13 | 4.3% |
| High Quality (EQ â‰¥ 0.9) | 119 | 54.6% | 106 | 35.0% |
| Medium Quality (EQ â‰¥ 0.7) | 66 | 30.3% | 137 | 45.2% |
| Low Quality (EQ < 0.7) | 5 | 2.3% | 47 | 15.5% |

**Result**: 67.4% of kept files scored High or Perfect, confirming the scoring algorithm prioritized quality.

---

## Reflection
- 303 duplicate groups confirmed â€” a significant redundancy issue fixed before feature extraction.  
- The scoring system worked well, though in edge cases with similar scores, tie-breakers (metadata richness, content length) were needed.  
- ISO 26324 DOI normalization was essential â€” DOIs appear in multiple inconsistent formats in the literature.  
- Running in Docker containers provided reproducibility and isolated environments.  
- Trade-off: some high-quality duplicates were discarded when scores tied closely.  

---

## Next Step
Proceed to `3_extract_syn_ukb_features`: extract UK Biobank-specific features from the cleaned, deduplicated dataset.

---

## Technical Implementation

### Libraries
- **Python Standard**: `json`, `os`, `re`, `pathlib`, `collections`.  
- **Fast JSON**: `orjson`.  
- **DOI Handling**: `idutils` (CERN standard DOI normalization/validation).  
- **String Comparison**: `rapidfuzz` (Levenshtein distance for fuzzy matching).  
- **HTTP/Retry**: `httpx`, `tenacity` (for optional DOI verification).  
- **Progress**: `tqdm`.  

### Docker
Two container builds:  

1. **Dockerfile** â€“ Standards-compliant DOI analysis:  
   ```dockerfile
   FROM python:3.11-slim
   RUN pip install --no-cache-dir orjson polars idutils rapidfuzz httpx tenacity tqdm
   
2. **Dockerfile.meta** â€“ Metadata quality analysis:
	FROM python:3.11-slim
```

