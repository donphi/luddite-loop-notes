# UK Biobank DOI Extraction


## Content


### Objective


The project necessitated the compilation of an extensive corpus of data, specifically comprising approximately 8,500 biobank journal articles as of April 2025.







---


### Process

1. I retrieved the schema files from the UK Biobank Showcase portal, which consists of a single tab-separated values (TSV) file that is publicly accessible. Access to the UK Biobank is not required to obtain the list of digital object identifiers (DOIs), journal names, and authors associated with official publications utilizing UK Biobank data. As stipulated by the UK Biobank's guidelines, all researchers who utilize its resources are mandated to disseminate their findings and share their publications accordingly, this makes this resource very comprehensive.
2. Access to the resource was through the public showcase data on the UK Biobank website, accesseable through the Schema 19 (Publications), which can be found at https://biobank.ndph.ox.ac.uk/showcase/download.cgi.
3. Throughout the project, I utilized Visual Studio Code as my integrated development environment (IDE) along with Docker for containerization. I established a Document Framework intended to support various experimental phases of the project. This framework is anticipated to evolve as the research progresses and encounters significant turning points that prompt adjustments in direction.





---


### Predicted Project Folder Structure


A forward-looking design of how the repository will evolve



```plain text
.
├── _deprecated                     # Archived / unused code or data (kept for reference)
├── data                            # Core data storage
│   ├── raw                         # Immutable raw data (UKB extracts, PDFs, JSONs)
│   ├── processed                   # Cleaned & structured datasets for analysis
│   └── external                    # Third-party or reference datasets (ontologies, vocabularies)
│
├── data_conversion                 # Preprocessing and format transformations
│   ├── meta_match                  # Aligns metadata across multiple sources
│   ├── pdf_to_md_json              # Converts raw PDFs into Markdown + JSON
│   ├── validation_md_json          # Validates and cleans Markdown/JSON conversions
│   └── feature_extraction          # Extracts UKB features and maps to schema
│
├── data_db                         # Database layer
│   ├── sql                         # DuckDB / SQL scripts
│   └── scripts                     # Helper scripts for ETL and query automation
│
├── data_ingestion                  # Pipelines for acquiring and validating documents
│   ├── ukb_journals_extraction     # Automated ingestion of UKB-related journals
│   ├── duplicates_check            # Ensures no duplicate papers enter workflow
│   └── validation_check            # Sanity and integrity checks before processing
│
├── rag                             # Core Retrieval-Augmented Generation system
│   ├── vector_database             # Vector store management (Milvus / FAISS / DuckDB hybrid)
│   ├── orchestration               # Pipeline orchestration (agentic workflows, SLATE)
│   └── prompts                     # Prompt templates and schema-driven prompt logic
│
├── experiments                     # Experimental workflows and prototypes
│   ├── configs                     # YAML/JSON configs for reproducibility
│   ├── logs                        # Training & run logs
│   └── results                     # Interim evaluation outputs
│
├── results                         # Curated outputs for dissertation
│   ├── figures                     # Final plots, SHAP diagrams, Kaplan-Meier curves
│   ├── tables                      # Summary tables (CSV/XLSX)
│   └── reports                     # Generated MD/PDF reports for chapters
│
├── paper                           # Dissertation writing area
│   ├── sections                     # Draft sections (Intro, Methods, Results, Discussion)
│   ├── figures                      # Paper-ready figure exports
│   └── submission                   # Final submission-ready versions
│
├── references                      # Bibliography files, citation libraries, and PDFs
│
├── scripts                         # Utility scripts (cleaning, visualization, evaluation)
│
├── docs                            # Internal documentation (pipeline notes, how-tos)
│   └── project_overview.md
│
└── tests                           # Unit and integration tests to ensure reproducibility
```





1. Furthermore, I developed a foundational Python script designed to systematically extract DOI identifiers and PDF links for each publication of interest. In total, I identified a comprehensive list of publications, each accompanied by titles and corresponding DOI numbers. However, there remain 5,623 records that require DOI-based resolution due to the absence or inactivity of schema links.





---


### **Reflection**


The conversion of PDF documents to Markdown format presents a significant challenge due to the inherent lack of structure in the data. Each author chooses their own method of presentation, which can vary widely even when adhering to basic frameworks required for scientific rigor. This variability may result in inconsistencies that complicate the extraction and organization of information. Consequently, the ability to effectively manage this conversion process will likely serve as a critical determinant in the success of the project. It is anticipated that this task will require the most considerable investment of effort.







---


### **Next Step**


I will need to identify several resources that provide access to publicly available materials, specifically targeting open-access journal articles, using the recently acquired DOI numbers. A critical challenge will be to ensure that the downloaded DOIs correspond accurately to the titles associated with the UK Biobank. Furthermore, I anticipate an additional difficulty in that there may be numerous instances of spurious or fraudulent papers within the search results.






