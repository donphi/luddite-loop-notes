# Topic Reclassification Report


## Content


```markdown
# Topic Reclassification Report

This report explains the output logs from the BERTopic model during the topic extraction and section classification process. 
The goal is to show how clusters of text (`topics`) were reclassified into journal paper sections (e.g., abstract, methods, references).

---

## Example Log Extracts

Here are **three raw examples** from the system logs, exactly as they appeared during processing:

```
INFO:__main__:  Topic 692 -> references
INFO:__main__:    Score: 0.638 (emb: 0.911, kw: 0.000)
INFO:__main__:    Words: info, article, , , 
```

```
INFO:__main__:  Topic 695 -> tables_and_figures
INFO:__main__:    Score: 0.741 (emb: 0.898, kw: 0.375)
INFO:__main__:    Words: lv, rv, chamber, indexes, es
```

```
INFO:__main__:  Topic 697 -> acknowledgments
INFO:__main__:    Score: 0.700 (emb: 0.928, kw: 0.167)
INFO:__main__:    Words: acknowledgements, funding, advancements, acknowledgement, acknowledged
```

These extracts represent how each topic (a cluster of related text sections) is scored and then mapped to a likely canonical section.

---

## How to Interpret the Output

- **Topic ID** – the cluster number assigned by BERTopic.
- **Section** – the reclassified journal section name (e.g., references, discussion, abstract).
- **Score** – combined metric from embeddings and keyword matching.
  - **emb**: similarity to canonical embeddings.
  - **kw**: keyword overlap with predefined section keywords.
- **Top Words** – the most representative words for that topic cluster.

The **progress bars** (e.g., `Batches: 100%|█████| 1/1 [...]`) reflect the process of extracting these top words. They run quickly because each topic-word extraction is short and CPU-based.

---

## Reclassified Topics (Sample)

| Topic ID | Section                  | Score | Embedding Sim | Keyword Score | Top Words                                      |
|----------|--------------------------|-------|---------------|---------------|------------------------------------------------|
| 692      | references               | 0.638 | 0.911         | 0.000         | info, article, , ,                             |
| 693      | discussion               | 0.688 | 0.921         | 0.143         | limitations, limitation, study, ,              |
| 694      | abstract                 | 0.669 | 0.902         | 0.125         | author, summary, , ,                           |
| 695      | tables_and_figures       | 0.741 | 0.898         | 0.375         | lv, rv, chamber, indexes, es                   |
| 696      | data_and_code_availability | 0.679 | 0.909         | 0.143         | biobank, uk, data, mellitus, diabetes          |
| 697      | acknowledgments          | 0.700 | 0.928         | 0.167         | acknowledgements, funding, advancements, acknowledgement, acknowledged |
| 698      | funding                  | 0.628 | 0.897         | 0.000         | gla, publications, glasgow, members, ac        |
| 699      | methods                  | 0.735 | 0.942         | 0.250         | legend, description, biologists, repeatedly, monte |
| 700      | abstract                 | 0.671 | 0.904         | 0.125         | setup, experimental, experiment, experiments, setups |
| 701      | supplementary_materials  | 0.639 | 0.913         | 0.000         | software, code, , ,                            |
| 702      | tables_and_figures       | 0.708 | 0.905         | 0.250         | mediation, modelling, annu, vanderweele, fow   |
| 703      | results                  | 0.648 | 0.878         | 0.111         | ahead, print, publish, revised, editorial      |
| 704      | supplementary_materials  | 0.771 | 0.917         | 0.429         | jaha, 1161, suppl, ahajournals, athttps        |
| 705      | ethics_approval          | 0.803 | 0.902         | 0.571         | ethics, approval, consent, committee, standards |

---

## Summary

- **GPU handled the clustering (UMAP + HDBSCAN).**
- **CPU handled the topic representation (vectorizer + c-TF-IDF).**
- The progress bars you see are from **topic word extraction**, not embeddings.
- Scores show how confident the model is when mapping clusters back to canonical sections.

This output confirms the pipeline is functioning: clusters are being created, scored, and labeled with interpretable sections and top words.
```


```markdown
# RAG + Topic Reclassification: How Vectors, Retrieval, and BERTopic Scores Fit Together

This report connects the dots between your **vector database**, **vector space embeddings**, **retrieval**, and the **BERTopic scores** you’re seeing in the logs. It also clarifies **which progress bars** correspond to which stage so you can read runtime output with confidence.

---

## 1) End-to-End Overview (Plain English)

**Goal:** take many journal chunks and (a) make them searchable with vectors, and (b) reclassify “other/unknown” chunks into real sections (abstract, methods, results, etc.).

**Pipeline (two intertwined tracks):**

```
          ┌───────────┐
          │  Chunks   │  (text bodies + metadata)
          └─────┬─────┘
                │
                │ 1. Embedding  f(text) → R^d   (GPU)
                ▼
        ┌───────────────┐
        │ Vector Space  │  (one vector per chunk)
        └─────┬─────────┘
              │
              │ 2. Index in Vector DB (e.g., Milvus) for ANN search
              ▼
        ┌───────────────┐           ┌──────────────────────────┐
        │  Retrieval    │◄── query ─┤ embed(query) → vector q  │
        └───────────────┘           └──────────────────────────┘
              │  returns top-k by cosine/dot-product similarity
              ▼
      (RAG consumer: downstream ranking/reading/QA, etc.)


In parallel for section reclassification:

Chunks ──embeddings (GPU)──► UMAP (GPU) ─► HDBSCAN (GPU) ─► BERTopic topic words (CPU c‑TF‑IDF)
                                            │
                                            ▼
                                     Section mapping (your Score = 0.7*emb + 0.3*kw)
```

- **Vector DB track:** makes your chunks searchable with **similarity scores** (cosine or dot-product).
- **BERTopic track:** groups similar chunks into topics and **names** those groups with top words; you then **map topics → canonical sections** using your combined score.

**Important:** the “Score” values in your logs are **classification scores** (for section mapping), **not** retrieval scores from the vector DB.

---

## 2) Vector Database & “Vector Space” (Technical)

- Each chunk \(t_i\) is embedded via \( \mathbf{x}_i = f(t_i) \in \mathbb{R}^d \).
- Store \((\text{id}, \mathbf{x}_i, \text{metadata})\) in a **vector database** (e.g., Milvus).
- Build an **ANN (Approximate Nearest Neighbor)** index:
  - Common choices: **HNSW**, **IVF-FLAT/IVF-PQ**, **DiskANN** (vendor-dependent).
  - Trade‑off: recall vs speed vs memory.
- Metadata (e.g., `paper_id`, `section_type`, `doi`) is kept for **filters** and downstream joins (often in the same vector DB or in a side store like DuckDB).

**Schema sketch (conceptual):**
```text
chunk_id: str PRIMARY KEY
embedding: float[d]  # dense vector
text: str
section_type: str
paper_id: str
source: str
... (other attributes for filters)
```

---

## 3) Embeddings & Similarity (Math)

Let \( f(\cdot) \) be the encoder (e.g., SentenceTransformer). For any text:

- **Embedding:** \( \mathbf{x} = f(\text{text}) \in \mathbb{R}^d \).
- Often we **L2-normalize** embeddings so cosine and dot-product are aligned.

**Cosine similarity:**
\[
\operatorname{cos\_sim}(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \; \|\mathbf{y}\|}
\]

**Retrieval:** Given a query \(q\) with embedding \(\mathbf{q}\), return top‑\(k\) chunks by similarity \(s_i = \operatorname{cos\_sim}(\mathbf{q}, \mathbf{x}_i)\) (or by dot‑product if that’s how your embeddings are trained).

**Mini numeric example (illustrative):**
- \(\mathbf{q}=(1,0,0)\), \(\mathbf{x}_1=(0.9,0.1,0)\), \(\mathbf{x}_2=(0.3,0.95,0)\)
- \(\cos(\mathbf{q},\mathbf{x}_1)\approx 0.99\), \(\cos(\mathbf{q},\mathbf{x}_2)\approx 0.30\)
- Top‑1 = \(x_1\). The **retrieval score** is the similarity value returned by the vector DB.

---

## 4) BERTopic: Topics, Words, and Your Section “Score”

**Clustering path (you enabled GPU here):**
1. **UMAP** reduces dimensionality of embeddings (still preserves local structure).
2. **HDBSCAN** finds dense clusters; points with no clear cluster → noise.

**Topic representation (CPU):**
3. **Vectorizer (CountVectorizer)** builds a vocabulary for each topic’s documents.
4. **c‑TF‑IDF** weights words to identify the **most representative words** per topic.

**Your section mapping:** you compute a **combined score** for each topic to select a canonical section (abstract, methods, …):

\[
\text{Score} = 0.7 \times \text{EmbeddingSimilarity} \;+\; 0.3 \times \text{KeywordScore}
\]

- **EmbeddingSimilarity** = cosine similarity between the topic’s pooled text embedding and a canonical section embedding.
- **KeywordScore** = fraction of section‑specific keywords found in the pooled text.

That’s the **Score** in your logs (e.g., `0.803 (emb: 0.902, kw: 0.571)`).

---

## 5) Progress Bars: What You’re Seeing

- **Embeddings:** `Batches: xx% | ... | n/N [.. it/s]` with many batches. *GPU-heavy* (your SentenceTransformer on CUDA).
- **Clustering:** brief logs from cuML/UMAP/HDBSCAN. *GPU-heavy but short*.
- **BERTopic “Fitting model” & `Batches: 100%| 1/1 ...` lines:** fast single‑batch updates while **topic words** are computed with **CountVectorizer + c‑TF‑IDF**. *CPU‑heavy; GPU idle is normal here.*

---

## 6) Sample Log Extracts (Verbatim)

```
INFO:__main__:  Topic 692 -> references
INFO:__main__:    Score: 0.638 (emb: 0.911, kw: 0.000)
INFO:__main__:    Words: info, article, , , 
```

```
INFO:__main__:  Topic 695 -> tables_and_figures
INFO:__main__:    Score: 0.741 (emb: 0.898, kw: 0.375)
INFO:__main__:    Words: lv, rv, chamber, indexes, es
```

```
INFO:__main__:  Topic 697 -> acknowledgments
INFO:__main__:    Score: 0.700 (emb: 0.928, kw: 0.167)
INFO:__main__:    Words: acknowledgements, funding, advancements, acknowledgement, acknowledged
```

---

## 7) Reclassified Topics (Your Provided Slice)

| Topic ID | Section                     | Score | Embedding Sim | Keyword Score | Top Words                                      |
|----------|-----------------------------|-------|---------------|---------------|------------------------------------------------|
| 692      | references                  | 0.638 | 0.911         | 0.000         | info, article, , ,                             |
| 693      | discussion                  | 0.688 | 0.921         | 0.143         | limitations, limitation, study, ,              |
| 694      | abstract                    | 0.669 | 0.902         | 0.125         | author, summary, , ,                           |
| 695      | tables_and_figures          | 0.741 | 0.898         | 0.375         | lv, rv, chamber, indexes, es                   |
| 696      | data_and_code_availability  | 0.679 | 0.909         | 0.143         | biobank, uk, data, mellitus, diabetes          |
| 697      | acknowledgments             | 0.700 | 0.928         | 0.167         | acknowledgements, funding, advancements, acknowledgement, acknowledged |
| 698      | funding                     | 0.628 | 0.897         | 0.000         | gla, publications, glasgow, members, ac        |
| 699      | methods                     | 0.735 | 0.942         | 0.250         | legend, description, biologists, repeatedly, monte |
| 700      | abstract                    | 0.671 | 0.904         | 0.125         | setup, experimental, experiment, experiments, setups |
| 701      | supplementary_materials     | 0.639 | 0.913         | 0.000         | software, code, , ,                            |
| 702      | tables_and_figures          | 0.708 | 0.905         | 0.250         | mediation, modelling, annu, vanderweele, fow   |
| 703      | results                     | 0.648 | 0.878         | 0.111         | ahead, print, publish, revised, editorial      |
| 704      | supplementary_materials     | 0.771 | 0.917         | 0.429         | jaha, 1161, suppl, ahajournals, athttps        |
| 705      | ethics_approval             | 0.803 | 0.902         | 0.571         | ethics, approval, consent, committee, standards |

---

## 8) How Retrieval Scores Relate to This

- **Retrieval score** (from the vector DB) = similarity between **query embedding** and **chunk embedding** (cosine or dot-product). Used to **rank** chunks for a question/need.
- **Your “Score” in logs** = **section-classification** confidence combining embedding similarity to a canonical section and keyword overlap. Used to **label** topics.

These are different scores for different purposes. It’s valid (and common) to use both: retrieval to **find** the right chunks, BERTopic+mapping to **understand** and **organize** them.

---

## 9) Practical Knobs

- **Retrieval quality:**
  - Ensure embeddings are **normalized** if using cosine similarity.
  - Pick an ANN index (e.g., **HNSW**) with enough `ef_construction`/`M`; at query time tune `ef` (HNSW) for recall.
  - Filter by metadata (e.g., section) to reduce noise before ranking.

- **Topic quality (BERTopic):**
  - Keep `ngram_range=(1,1)`, `min_df≥5`, `max_df≤0.9` for speed + cleaner vocab.
  - Improve tokenization to drop punctuation-only tokens: `token_pattern=r'(?u)\b[A-Za-z][A-Za-z]+\b'`.
  - Adjust your weight mix if needed: `Score = α*emb + (1-α)*kw` (you currently use α=0.7).

---

## 10) Pseudocode: Insert & Search (Milvus-style)

```python
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType

# 1) Connect
connections.connect("default", host="milvus", port="19530")

# 2) Define schema
fields = [
    FieldSchema(name="chunk_id", dtype=DataType.VARCHAR, is_primary=True, max_length=64),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=d),  # d per your encoder
    FieldSchema(name="paper_id", dtype=DataType.VARCHAR, max_length=64),
    FieldSchema(name="section_type", dtype=DataType.VARCHAR, max_length=64),
]
schema = CollectionSchema(fields, description="UKB chunks")
col = Collection("ukb_chunks", schema=schema)

# 3) Create index (HNSW example)
index_params = {"index_type": "HNSW", "metric_type": "IP", "params": {"M": 32, "efConstruction": 200}}
col.create_index(field_name="embedding", index_params=index_params)

# 4) Insert data (ids, vectors, metadata)
col.insert([chunk_ids, embeddings, paper_ids, section_types])
col.flush()

# 5) Search with a query vector q
search_params = {"metric_type": "IP", "params": {"ef": 256}}
res = col.search([q], "embedding", param=search_params, limit=20, output_fields=["paper_id", "section_type"])
# res returns top-k with similarity scores per hit
```

---

### TL;DR

- **Vector DB + embeddings** → fast **retrieval scores** for search (cosine/dot-product).
- **BERTopic + mapping** → interpretable **section scores** (your 0.7*emb + 0.3*kw) for labeling clusters.
- Different scores, different roles; both are essential to your stack.
```






```markdown
# Adjusted Reclassification Comparison

This table shows the section counts and percentages before and after reclassification, where 'other' has been reallocated.

| Section                    |   Before Count | Before %   |   After Count | After %   |
|:---------------------------|---------------:|:-----------|--------------:|:----------|
| abstract                   |          14511 | 2.00%      |         45282 | 5.09%     |
| acknowledgments            |           7420 | 1.02%      |         14669 | 1.65%     |
| author_info                |          11447 | 1.58%      |         29982 | 3.37%     |
| conclusion                 |          11679 | 1.61%      |         20296 | 2.28%     |
| conflict_of_interest       |           4202 | 0.58%      |         10867 | 1.22%     |
| data_and_code_availability |          20450 | 2.81%      |         46079 | 5.18%     |
| discussion                 |          53475 | 7.36%      |         65007 | 7.30%     |
| ethics_approval            |           1845 | 0.25%      |          8435 | 0.95%     |
| funding                    |           3425 | 0.47%      |         18152 | 2.04%     |
| introduction               |          24792 | 3.41%      |         34990 | 3.93%     |
| keywords                   |           1211 | 0.17%      |          3093 | 0.35%     |
| methods                    |          56796 | 7.82%      |         73392 | 8.24%     |
| notes                      |           1702 | 0.23%      |          8457 | 0.95%     |
| other                      |         214293 | 29.49%     |             0 | 0.00%     |
| raw_headings               |         163664 | 22.53%     |        163707 | 18.39%    |
| references                 |          42103 | 5.79%      |         67909 | 7.63%     |
| results                    |          36051 | 4.96%      |        140161 | 15.74%    |
| study_participants         |          17569 | 2.42%      |         49684 | 5.58%     |
| supplementary_materials    |          15556 | 2.14%      |         45056 | 5.06%     |
| tables_and_figures         |          20782 | 2.86%      |         45015 | 5.06%     |
| unknown                    |           3596 | 0.49%      |             0 | 0.00%     |
| TOTAL                      |         726569 | 100.00%    |        890233 | 100.00%   |
```






![](https://prod-files-secure.s3.us-west-2.amazonaws.com/7a8a68e8-5855-41f2-ae17-565ecdbd6218/9c7b3b06-a401-48f6-a63d-ccefd9025a5c/before_distribution.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466SZ3LUDMI%2F20251207%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251207T135242Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQDZsoNzzsd22LOslHw5n9OR57oD9IIH%2FaaIQ4VGQv5E9QIhAP12IeZnwvrJPM34xVK5V9n%2BXmsteyP7oyCmCgmnavzEKogECIz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgxRZABJ7fYvzzy4SgYq3APu77dfRkqVWlArXMrlUFseA1MvUSCDmNjSq6LscS1cJGGdtWu4O5sk0fvcHVDXaLN4GkX1DQ5UD0TJ39ZnF%2FppzzBK%2BLaNCW%2Fr2pGWTtvCDLJVwBHXUasG%2BPFbuFPwPwILHUX30tBzmzKRw38K%2Frrsa8WksI4B%2BENmS8lhBCctVgnctrYMeagFt9Qa%2FPx8iGxLuWQH3ga35VSlNtcIowu%2FooB%2BB9acPrjirXgG9g73eXn%2BBrpUEUsiEfAzR1VtWx0567l03IrCGVW4UKSEKI109isTiRHChVp33IIRW6W35Bq9X8YXotV96EopCnGDpvLVWJZbSgw5HFbBZzWvTHRB5%2BqfmtCtebN%2Fv2%2BxUVaX8XMRHb%2FoSnSU9CzCqvT%2BHEauvVMHAxL0Q%2BBJ0aOG%2BEfckHsFAUP3eBTEmrTTKsuDlo2bTNgjZrOUS8FQxB%2F47LNNH1WQYq4NlryVD9jS34BJ83vztflSe%2BSsypS2Rm2aXJmwVUSdGcVwRKCYNIo8%2BT8Sp9UScb2%2F969yCgTUoYnWcfbJIbcLMFMSvCX3OkKtJ8el8A0LbY90qneO04jnDSn%2BU9dco0EW8xIvhO0hz0oenF00qVOO04f2sjbcMoDo4LW1xvR1ZxKVDT126zDvvtXJBjqkAQgjrcGjYe%2FiIpfmA1%2B2hNFKZ7Wpr8HzrVgGfM085D3wN0V6CXst%2BbpAB10tPZ%2FTVr2PdRJhxb7vfomRuGCTP9GNhg%2FdyIQp2Q7ymvsfEKiD62Ug65RjhykGwCDujq8yPqXpWiTlGxo8ZH2Q801C1g5%2F22t1VlOVcp7yB6pSc3USgS21zLRPl8BljriZt15i%2BW3AT95UaXIUW9v7Qozf%2FtntJaRK&X-Amz-Signature=41083c7af0be212d1e67951e262e4f5d93dd91c21ca6ea9f96576f79366cad07&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)






![](https://prod-files-secure.s3.us-west-2.amazonaws.com/7a8a68e8-5855-41f2-ae17-565ecdbd6218/b4f46d8b-ff72-4583-957a-9c4ef93bf427/after_distribution.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466SZ3LUDMI%2F20251207%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251207T135242Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQDZsoNzzsd22LOslHw5n9OR57oD9IIH%2FaaIQ4VGQv5E9QIhAP12IeZnwvrJPM34xVK5V9n%2BXmsteyP7oyCmCgmnavzEKogECIz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgxRZABJ7fYvzzy4SgYq3APu77dfRkqVWlArXMrlUFseA1MvUSCDmNjSq6LscS1cJGGdtWu4O5sk0fvcHVDXaLN4GkX1DQ5UD0TJ39ZnF%2FppzzBK%2BLaNCW%2Fr2pGWTtvCDLJVwBHXUasG%2BPFbuFPwPwILHUX30tBzmzKRw38K%2Frrsa8WksI4B%2BENmS8lhBCctVgnctrYMeagFt9Qa%2FPx8iGxLuWQH3ga35VSlNtcIowu%2FooB%2BB9acPrjirXgG9g73eXn%2BBrpUEUsiEfAzR1VtWx0567l03IrCGVW4UKSEKI109isTiRHChVp33IIRW6W35Bq9X8YXotV96EopCnGDpvLVWJZbSgw5HFbBZzWvTHRB5%2BqfmtCtebN%2Fv2%2BxUVaX8XMRHb%2FoSnSU9CzCqvT%2BHEauvVMHAxL0Q%2BBJ0aOG%2BEfckHsFAUP3eBTEmrTTKsuDlo2bTNgjZrOUS8FQxB%2F47LNNH1WQYq4NlryVD9jS34BJ83vztflSe%2BSsypS2Rm2aXJmwVUSdGcVwRKCYNIo8%2BT8Sp9UScb2%2F969yCgTUoYnWcfbJIbcLMFMSvCX3OkKtJ8el8A0LbY90qneO04jnDSn%2BU9dco0EW8xIvhO0hz0oenF00qVOO04f2sjbcMoDo4LW1xvR1ZxKVDT126zDvvtXJBjqkAQgjrcGjYe%2FiIpfmA1%2B2hNFKZ7Wpr8HzrVgGfM085D3wN0V6CXst%2BbpAB10tPZ%2FTVr2PdRJhxb7vfomRuGCTP9GNhg%2FdyIQp2Q7ymvsfEKiD62Ug65RjhykGwCDujq8yPqXpWiTlGxo8ZH2Q801C1g5%2F22t1VlOVcp7yB6pSc3USgS21zLRPl8BljriZt15i%2BW3AT95UaXIUW9v7Qozf%2FtntJaRK&X-Amz-Signature=a299d2b057c4dc5fa782b7b5abc087b5d3342e0b7294452a4e368af2d77b5367&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)






MASSIVE PROBLEM, check if it took the test out of circulation or tested all topic options against other, … might not be as thats not the purpose of having the model create the cateogires and hten narrowing it down


