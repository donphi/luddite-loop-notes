# PDF Metadata Matching


## Content


### Objective


After downloading all the PDFs we need to ensure that the the actual PDF files were infact the DOIs match that we were looking for from the UK Biobank. We could not accept that what we downloaded was infact the correct PDFs. Looking through the entire corpus of more than 6000 pdfs are n ot feasible. Each document must be accurately identified and enriched with DOI, title, authors, and publication year. This ensures downstream stages operate on verified, properly indexed data. The process is automated, statistically validated to ensure our eventual data is accurate.







---


### Process

1. **Environment**
    - Docker container for reproducibility.
    - GROBID server provides machine-learning based parsing of scientific articles.




1. **Metadata Extraction**
    - `grobid_matcher.py` We created our first script to iterate over all the PDFs. We researched a few libraries that would help with this type of parsing. GeneRation Of BIbliographic Data, GROBID, seemed the most appropriate for the task.
    - This would extract most of the headers: title, authors, DOI.

    


2. **Indexing**
    - `publications_index.json` that was initially created from the downloaded meta file from the UK Biobank was loaded into fast lookup dictionaries for DOIs and titles.

    


3. **Matching Logic**
    - First attempt: DOI → index (high confidence).
    - If DOI missing or fails:
        - Title similarity (BM25 + fuzzy matching).
        - Author list overlap.
        - Publication year proximity.

    


4. **Validation Layer**
    - `scientific_validation.py` recalculates match confidence.
    - Checks author overlap, title similarity, and year deltas.
    - Flags mismatches for manual review.





---


### Validation

- **Sample Size**: 363 docs out of 6,400.
- **Observed Accuracy**: 96.1%.
- **95% CI**: 93.6%–97.7%.
- **Margin of Error**: ±2.0%.
- **Significance**: statistically valid (p < 0.05).

Match Breakdown:


- DOI-based: 96.9% (323 docs).
- Fuzzy title: 88.9% (27 docs).
- Exact title: 100% (4 docs).
- Partial DOI: 88.9% (9 docs).





---


### Reflection

- Overall accuracy excellent at 96.1%.
- Single major dependency: GROBID extraction.
- If GROBID fails (corrupt file, unusual layout, timeout), the PDF becomes unmatchable.
- This creates a bottleneck where PDFs are flagged `unreadable` or `extraction_failed`.
- Lesson learned: relying only on header metadata is fragile — quality of full-text extraction is the real upstream driver.

---


## Next Step


Shift from metadata-only to full-text extraction:


- Adopt `Marker` tool in `1_pdf_to_md_json`.
- Convert the entire PDF into structured Markdown + JSON.
- Marker advantages:
    - Full OCR with Tesseract + Surya.
    - Tables, equations, and images preserved.
    - Hybrid speed/quality modes.
    - GPU acceleration for scale.

This pivot eliminates the single point of failure in metadata-only matching and enables richer downstream tasks (e.g., NER, feature tagging) on the full content.







---


### **References**


Grobid Canonical citation (the main paper):



GitHub / software citation (if allowed):


