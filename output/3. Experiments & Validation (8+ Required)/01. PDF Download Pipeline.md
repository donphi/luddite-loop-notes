# PDF Download Pipeline


## Content


Objective



To design and test an automated pipeline capable of downloading, verifying, and organising full-text PDFs of UK Biobank publications. The goal was to maximise coverage, reduce duplication, and ensure that retrieved documents contained valid, readable scientific content.



Method


- Implemented a **Python-based system** for large-scale PDF retrieval.
- Key features included:
    - **URL extraction**: Parsing publication metadata into structured URL + DOI records.
    - **Download management**: Multi-threaded requests with configurable rate limiting.
    - **Content verification**: File size and text content checks using `PyPDF2`.
    - **Logging and state tracking**: JSON logs, statistics, and resumable states for reproducibility.
    - **Deduplication**: Checksums applied to remove duplicate or corrupted files.
    - **Fallback handling**: If direct URLs failed, DOI-based resolution methods were attempted.
- Containerised the workflow with **Docker** to ensure environment consistency.
- Normalised metadata and filenames using DOI fragments, author names, and publication years.

Code and Environment



The pipeline was modular to maintain clarity and reproducibility:


- `extract_urls.py` – parse metadata and extract URLs/DOIs.
- `download_pdfs.py` – manage downloads, verification, logging, and stats.
- `kill_downloads.py` – process control utility.
- `index/` – stores JSON index of processed publications.
- `logs/` – stores runtime logs, verification results, and summary stats.
- `data/` – stores retrieved PDFs, separated by direct and fallback sources.

Libraries


- Retrieval: `requests`, `tqdm`, `concurrent.futures`
- Verification: `PyPDF2`, `python-magic`
- Parsing: `BeautifulSoup`, `re`, `hashlib`
- Logging/state: `logging`, `json`, `atexit`
- Utilities: `argparse`, `datetime`, `os`

Dockerisation


- All scripts run in a controlled container environment.
- Benefits: reproducibility, consistent Python dependencies, and portability across HPC/cloud.

Results




<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Total publications targeted</td>
      <td>8,528</td>
    </tr>
    <tr>
      <td>PDFs retrieved (initial)</td>
      <td>3,213</td>
    </tr>
    <tr>
      <td>PDFs retrieved (pipeline)</td>
      <td>3,199</td>
    </tr>
    <tr>
      <td>Combined total PDFs</td>
      <td>6,412</td>
    </tr>
    <tr>
      <td>Duplicates removed</td>
      <td>932</td>
    </tr>
    <tr>
      <td>Final unique set</td>
      <td>6,841</td>
    </tr>
    <tr>
      <td>Coverage vs. total (%)</td>
      <td>~80%</td>
    </tr>
  </tbody>
</table>



Reflection



The pipeline demonstrated the importance of combining data science principles with research integrity. Logging, verification, and reproducibility were as crucial as the raw retrieval. Although fallback strategies (e.g. Sci-Hub tests) were trialled during development, the final workflow was limited to institutionally approved and open access sources, following supervisory guidance:contentReference[oaicite:0]{index=0}. This ensured compliance while maintaining the robustness of the dataset.



Next Step



Integrate the verified corpus into the semantic search and embedding pipeline to support hypothesis testing on phenotypes and disease associations.



---


Code (simplified)





~~~python
import requests
from PyPDF2 import PdfReader

def download_and_verify(url, out_path):
    r = requests.get(url, stream=True, timeout=30)
    if r.status_code == 200:
        with open(out_path, "wb") as f:
            for chunk in r.iter_content(8192):
                f.write(chunk)
        reader = PdfReader(out_path)
        return len(reader.pages) > 0
    return False

~~~







