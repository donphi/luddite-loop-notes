# PDF to Markdown/JSON Conversion


## Content


```markdown
# Experiment: PDF to Markdown/JSON Conversion

**Objective**

To design and validate an automated pipeline for converting ~6,600 UK Biobank research PDFs into structured Markdown and JSON. The goal was to preserve full document content (text, tables, metadata), maximise GPU performance, and produce machine-readable outputs for downstream tagging and analysis.

---

**Method**

- Implemented a **GPU-accelerated Python pipeline** for large-scale PDF → MD/JSON conversion.  
- Key components:
  - **Hybrid text extraction**: PyMuPDF for embedded text; OCR fallback via Marker (Tesseract + Surya).  
  - **Table handling**: Tables converted into Markdown with row/col integrity preserved.  
  - **Metadata extraction**: Priority from matched metadata JSON; fallback heuristics (DOI, PMID, font-size title detection, journal heuristics).  
  - **Batch GPU processing**: Parallel workers, CUDA 12.8 tuning, and checkpoint-based recovery.  
  - **File organisation**: Consistent filenames (year + author + DOI/PMID, fallback MD5). Outputs separated into `/md` and `/json`.  
  - **Error handling**: Timeouts, batch GPU resets, and checkpoint restarts via `ProcessingIndex`.  
- Workflow containerised with **Docker** for reproducibility and isolation.

---

**Code and Environment**

- `convert_pdfs.py` – orchestrates conversion (PyMuPDF + Marker + metadata extraction).  
- `final_convert_resume.sh` – batch processing with GPU resets + logging.  
- `final_convert_missing.sh` – isolates and reprocesses failed/missing PDFs.  
- `data/ukb/md/` – Markdown with YAML metadata.  
- `data/ukb/json/` – JSON with structured schema.  

---

**Libraries**

- Parsing: `fitz (PyMuPDF)`, `re`, `hashlib`, `yaml`.  
- Conversion: `Marker v1.8.3`, `torch` (GPU acceleration).  
- Parallelism: `multiprocessing`, `threading`, `subprocess`.  
- Utilities: `tqdm`, `logging`.  

---

**Dockerisation**

- Pipeline runs in containerised environment with GPU passthrough.  
- Benefits: reproducibility, clean isolation of dependencies, restartable at batch level.  

---

**Results**

| Metric | Value |
| --- | --- |
| Total PDFs processed | ~6,600 |
| Avg. speed | ~6.5s per PDF |
| GPU utilization | ~77% |
| VRAM used | ~24GB / 96GB |
| Success rate | >90% |
| Missing PDFs recovered | 296 |

---

**Reflection**

The pipeline proved effective at producing structured outputs from heterogeneous PDFs. Hybrid extraction balanced speed and accuracy, while GPU optimisation cut runtime from days to ~11.5 hours. Error handling and checkpoints were essential for stability across thousands of files.  

Main limitations:  
- Table extraction inconsistent for complex layouts.  
- Metadata extraction weaker when relying solely on PDF content.  
- Processing time still variable for large/poor-quality scans.  

---

**Next Step**

Integrate the validated Markdown/JSON outputs into the **deduplication and synonym-generation pipeline**, ensuring clean metadata before chunking and tagging.  

---

**Code (simplified)**

```python
# placeholder for demonstration
def convert_pdf_to_markdown_json(pdf_file, out_dir):
    pass
```





